<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"marigo1d.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="reinforce learning">
<meta property="og:type" content="article">
<meta property="og:title" content="reinforce-learning-record">
<meta property="og:url" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/index.html">
<meta property="og:site_name" content="Marigold">
<meta property="og:description" content="reinforce learning">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/00e7457c373413b80db70c958566dd84.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610205131367.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610203108806.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610203423677.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610203434567.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610203509585.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610203841177.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610205112917.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610204522539.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610204014373.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610204152914.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213003398.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213050168.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612093758370.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612093948192.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612093958259.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612094048209.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610204627264.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/shiow.PNG">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610205402396.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213050168.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213003398.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213003398.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611150030644.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611191853492.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611152610430.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611152643864.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611153610316.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213050168.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611161858965.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611161157050.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611161942958.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611162513650.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611162624769.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611191221769.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612100452913.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611194333945.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612100509915.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322111503025.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322111831741.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612100629893.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213003398.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612100732404.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612102026769.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612102256431.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611200059062.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612110026419.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612110241521.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612110951796.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612111022161.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612110750284.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250324145210312.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250324141848364.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322150349844.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322150502092.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322151444000.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322151537751.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250611134637079.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322162045699.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250325154631583.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322211023368.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250325200037093.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250323105541258.png">
<meta property="article:published_time" content="2024-06-10T12:29:05.000Z">
<meta property="article:modified_time" content="2025-06-11T07:35:10.277Z">
<meta property="article:author" content="marigo1d">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/00e7457c373413b80db70c958566dd84.png">

<link rel="canonical" href="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>reinforce-learning-record | Marigold</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Marigold</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Salt, Pepper and Birds~</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="marigo1d">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Marigold">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          reinforce-learning-record
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-06-10 20:29:05" itemprop="dateCreated datePublished" datetime="2024-06-10T20:29:05+08:00">2024-06-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-06-11 15:35:10" itemprop="dateModified" datetime="2025-06-11T15:35:10+08:00">2025-06-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforce-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforce Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>23k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>42 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>reinforce learning</p>
<span id="more"></span>
<h1>reinforce learning record</h1>
<h2 id="数学基础">数学基础</h2>
<p>一阶优化与二阶优化</p>
<p>在优化问题中，<strong>一阶优化算法</strong>和<strong>二阶优化算法</strong>是两类重要的优化方法，它们主要区别在于利用了目标函数的哪些信息：</p>
<p><strong>一阶优化算法</strong></p>
<ul>
<li><strong>定义</strong>：一阶优化算法仅利用目标函数的一阶导数（梯度）信息进行优化。</li>
<li><strong>特点</strong>：
<ol>
<li><strong>梯度信息</strong>：依赖梯度来指引优化方向。</li>
<li><strong>计算成本低</strong>：梯度的计算通常比二阶导数（Hessian矩阵）简单，因此更高效。</li>
<li><strong>适用性广</strong>：常用于高维问题或目标函数复杂的情况。</li>
</ol>
</li>
<li><strong>常见算法</strong>：
<ul>
<li><strong>梯度下降法（Gradient Descent, GD）</strong>：$ x_{k+1} = x_k - \eta \nabla f(x_k)$ 其中，$\eta$ 是学习率，$\nabla f(x_k)$ 是目标函数在 $x_k$ 处的梯度。</li>
<li><strong>随机梯度下降（SGD）</strong>：在大规模数据下随机选择小批量样本计算梯度，适合机器学习训练。</li>
<li><strong>动量法（Momentum）</strong>、<strong>Adam优化器</strong>：改进了基础的梯度下降，具有更快的收敛速度。</li>
</ul>
</li>
</ul>
<p><strong>二阶优化算法</strong></p>
<ul>
<li>
<p><strong>定义</strong>：二阶优化算法利用目标函数的一阶导数（梯度）和二阶导数（Hessian矩阵）信息。</p>
</li>
<li>
<p><strong>特点</strong>：</p>
<ol>
<li><strong>曲率信息</strong>：Hessian矩阵包含关于目标函数曲率的信息，因此二阶优化算法能够更准确地找到极值点。</li>
<li><strong>更快收敛</strong>：在理想条件下，具有二次收敛速度。</li>
<li><strong>计算成本高</strong>：Hessian矩阵的计算和存储对于高维问题非常昂贵。</li>
</ol>
</li>
<li>
<p><strong>常见算法</strong>：</p>
<ul>
<li><strong>牛顿法（Newton’s Method）</strong>：使二阶泰勒展开式的导数为0的点，迭代至$\nabla f(x_k)$为极小，$x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)$ ，其中，$[\nabla^2 f(x_k)]^{-1}$ 是Hessian矩阵的逆。</li>
<li><strong>拟牛顿法（Quasi-Newton Methods）</strong>：通过近似Hessian矩阵降低计算复杂度，例如 BFGS 和 L-BFGS。</li>
<li><strong>信赖域方法（Trust Region Methods）</strong>：结合Hessian信息限制更新步长，适合复杂非线性优化。</li>
</ul>
</li>
</ul>
<h2 id="机器学习模型">机器学习模型</h2>
<img src="/2024/06/10/reinforce-learning-record/00e7457c373413b80db70c958566dd84.png" class="" title="ma learning">
<h2 id="MRP">MRP</h2>
<img src="/2024/06/10/reinforce-learning-record/image-20240610205131367.png" class="" title="image-20240610205131367">
<h3 id="回报">回报</h3>
<p>在一个马尔可夫奖励过程中，从第t时刻状态 S_t 开始，直到终止状态时，所有奖励的衰减之和称为<strong>回报</strong>（Return），表示为 G_t</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610203108806.png" class="" title="image-20240610203108806">
<ul>
<li>R_t表示在时刻t获得的奖励</li>
<li><strong>到达状态S_i</strong>，得到奖励r(s)；若在时刻t到达状态S_i，则<strong>R_t=r(S_i)</strong></li>
<li>对于某状态序列<img src="/2024/06/10/reinforce-learning-record/image-20240610203423677.png" class="" title="image-20240610203423677">我们有对应于该状态序列的奖励G<img src="/2024/06/10/reinforce-learning-record/image-20240610203434567.png" class="" title="image-20240610203434567"></li>
</ul>
<h3 id="价值函数">价值函数</h3>
<p>在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的<strong>价值</strong>（value）。所有状态的价值就组成了<strong>价值函数</strong>（value function），价值函数的输入为某个状态，输出为这个状态的价值。</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610203509585.png" class="" title="image-20240610203509585">
<ul>
<li>对于某个状态s，从该状态出发可得到的状态序列有很多条</li>
<li>对于某个状态s，价值函数为<strong>从该状态出发，可能存在的所有状态序列的奖励的均值</strong></li>
</ul>
<p>贝尔曼方程</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610203841177.png" class="" title="image-20240610203841177">
<h2 id="MDP">MDP</h2>
<p>与MRP相比，奖励与状态和动作相关</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610205112917.png" class="" title="image-20240610205112917">
<h3 id="策略">策略</h3>
<p>策略表示在状态s下采取动作a的<strong>概率</strong>，为<strong>概率</strong>，可能为1</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610204522539.png" class="" title="image-20240610204522539">
<h3 id="状态价值函数">状态价值函数</h3>
<p>在 MDP 中基于策略 $\pi$ 的状态价值函数（state-value function），定义为从状态s出发遵循策略Π能获得的期望回报</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610204014373.png" class="" title="image-20240610204014373">
<blockquote>
<p>价值是回报的期望</p>
<p>下标 π 用来<strong>指明这个期望是在什么概率分布下计算的</strong>。</p>
<ul>
<li>在概率论和统计中，期望算子的下标通常用来表示期望是关于哪个随机变量的分布，或者是在哪个参数（定义了分布）的条件下计算的。例如，$E_X[f(X)]$ 表示对随机变量 $X$ 的函数 $f(X)$ 求期望，其中 $X$ 的分布是已知的。</li>
</ul>
</blockquote>
<h3 id="动作价值函数">动作价值函数</h3>
<p>在 MDP 中，由于动作的存在，我们额外定义一个<strong>动作价值函数</strong>（action-value function）。表示在 MDP 遵循策略Π时，对当前状态s执行动作a得到的期望回报</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610204152914.png" class="" title="image-20240610204152914">
<ul>
<li><strong>采取动作a到达状态s</strong>，得到奖励R</li>
</ul>
<h3 id="状态价值函数与动作价值函数关系">状态价值函数与动作价值函数关系</h3>
<img src="/2024/06/10/reinforce-learning-record/image-20240610213003398.png" class="" title="image-20240610213003398">
<ul>
<li>从状态s出发，可能采取动作a1, a2, a3, …;对于每个可能采取的动作，均有动作价值Q，则状态s的状态价值为所有动作价值的期望</li>
</ul>
<img src="/2024/06/10/reinforce-learning-record/image-20240610213050168.png" class="" title="image-20240610213050168">
<ul>
<li>在状态s执行动作a后，得到奖励r(s,a)；由于可能到达多个不同的状态s’，动作价值为即时奖励r(s,a)+可能到达的所有状态的价值的期望</li>
</ul>
<h3 id="最优策略">最优策略</h3>
<p>策略之间的偏序关系</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612093758370.png" class="" title="image-20240612093758370">
<p>最优策略为 对于任意的策略，均有最优策略优于或不差于其他策略</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612093948192.png" class="" title="image-20240612093948192">
<img src="/2024/06/10/reinforce-learning-record/image-20240612093958259.png" class="" title="image-20240612093958259">
<p>最优状态价值与最优动作价值的关系</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612094048209.png" class="" title="image-20240612094048209">
<h3 id="贝尔曼期望方程">贝尔曼期望方程</h3>
<p>采取策略Π</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610204627264.png" class="" title="image-20240610204627264">
<ul>
<li>对于状态s，s的状态价值为 ( 折损的 <strong>所有可达的下一状态的价值的期望</strong> + <strong>到达所有可达的下一状态的r(s’,a)的期望</strong> )</li>
<li>对于状态s和动作a，在状态s进行动作a的价值为 ( <strong>在状态s执行动作a的r(s,a)的期望</strong>(实际上就是r(s,a)) + 折损的 <strong>在执行动作a后所有可达的下一状态的所有可执行动作的价值的期望</strong> )</li>
</ul>
<img src="/2024/06/10/reinforce-learning-record/shiow.PNG" class="" title="shiow">
<p>图中情况为每个动作只会到达一个状态&amp;应该直接将动作视为一个节点，图为树</p>
<h3 id="贝尔曼最优方程">贝尔曼最优方程</h3>
<p>存在策略*，使对于任意的状态s，均有基于策略*状态价值函数大于基于策略Π的状态价值函数</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610205402396.png" class="" title="image-20240610205402396">
<ul>
<li>对于状态价值：非最优形式是求期望，最优形式是直接选取当前状态，所有动作中，未来状态序列中价值最大的</li>
<li>对于动作价值：非最优形式是求期望，最优形式是直接选取下一状态，所有动作中，未来状态序列中动作价值最大的</li>
</ul>
<h2 id="策略迭代算法-基于策略函数的">策略迭代算法/基于策略函数的</h2>
<blockquote>
<p>策略迭代的核心思想是<strong>交替进行策略评估和策略提升</strong>，直到策略收敛到最优。</p>
</blockquote>
<p><strong>策略评估：</strong></p>
<p>基于当前策略Π，在已知状态转移函数的情况下（即我们可以知道采取动作a后有多高的几率到达哪个状态），使用<strong>贝尔曼期望方程</strong>迭代更新状态价值函数</p>
<p>ps: 原文中迭代更新状态价值函数是通过</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610213050168.png" class="" title="image-20240610213050168">
<img src="/2024/06/10/reinforce-learning-record/image-20240610213003398.png" class="" title="image-20240610213003398">
<p>以上两个公式实现的，也即</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">qsa_list = []</span><br><span class="line">	<span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 迷宫环境，四相运动</span></span><br><span class="line">		qsa = <span class="number">0</span>  <span class="comment"># 当前动作的动作价值</span></span><br><span class="line">        <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:  <span class="comment"># 有模型，即状态转移函数已知</span></span><br><span class="line">        	p, next_state, r, done = res</span><br><span class="line">            qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))  <span class="comment"># 第一个公式，计算该动作的价值(开头的p为动作成功概率)</span></span><br><span class="line">         qsa_list.append(self.pi[s][a] * qsa)  <span class="comment"># 乘采取该动作的几率</span></span><br><span class="line">	new_v[s] = <span class="built_in">sum</span>(qsa_list)  <span class="comment"># 第二个公式，根据所有动作的价值计算状态价值</span></span><br></pre></td></tr></table></figure>
<p><strong>策略提升：</strong></p>
<p>由于</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610213003398.png" class="" title="image-20240610213003398">
<p>故修改策略，使得动作价值最大的动作以更高的概率(若最大值唯一，该动作将成为唯一被选取动作；若不唯一，这些动作将均分概率1)被选取，可使新策略下的状态价值函数增大，也即<img src="/2024/06/10/reinforce-learning-record/image-20240611150030644.png" class="" title="image-20240611150030644"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">qsa_list = []</span><br><span class="line">	<span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 迷宫环境，四相运动</span></span><br><span class="line">    	qsa = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:</span><br><span class="line">        	p, next_state, r, done = res</span><br><span class="line">            qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))</span><br><span class="line">        qsa_list.append(qsa)</span><br><span class="line">     maxq = <span class="built_in">max</span>(qsa_list)</span><br><span class="line">     cntq = qsa_list.count(maxq)  <span class="comment"># 计算有几个动作得到了最大的Q值</span></span><br><span class="line">     <span class="comment"># 让这些动作均分概率</span></span><br><span class="line">     self.pi[s] = [<span class="number">1</span> / cntq <span class="keyword">if</span> q == maxq <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> q <span class="keyword">in</span> qsa_list]</span><br></pre></td></tr></table></figure>
<p>target: 修改<strong>策略</strong>，使在新策略下，有基于新策略的状态价值函数大于原策略的状态价值函数</p>
<p>完整策略迭代算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PolicyIteration</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 策略迭代算法 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, env, theta, gamma</span>):</span><br><span class="line">        <span class="comment"># 初始化函数，接受环境对象env、策略评估收敛阈值theta和折扣因子gamma</span></span><br><span class="line">        self.env = env  <span class="comment"># 保存环境对象</span></span><br><span class="line">        self.v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow  <span class="comment"># 初始化价值函数v为0，大小为环境的格子数</span></span><br><span class="line">        self.pi = [[<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>]</span><br><span class="line">                   <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow)]  <span class="comment"># 初始化策略pi为均匀随机策略，每个状态有4个动作，每个动作的概率为0.25</span></span><br><span class="line">        self.theta = theta  <span class="comment"># 策略评估的收敛阈值</span></span><br><span class="line">        self.gamma = gamma  <span class="comment"># 折扣因子，用于计算未来奖励的折扣</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">policy_evaluation</span>(<span class="params">self</span>):  <span class="comment"># 策略评估</span></span><br><span class="line">        cnt = <span class="number">1</span>  <span class="comment"># 计数器，记录策略评估的迭代次数</span></span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            max_diff = <span class="number">0</span>  <span class="comment"># 用于记录价值函数的最大变化量</span></span><br><span class="line">            new_v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow  <span class="comment"># 初始化新的价值函数</span></span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow):  <span class="comment"># 当次地图扫描</span></span><br><span class="line">                qsa_list = []  <span class="comment"># 用于存储状态s下所有动作的Q值</span></span><br><span class="line">                <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 遍历所有动作</span></span><br><span class="line">                    qsa = <span class="number">0</span>  <span class="comment"># 初始化Q(s,a)为0</span></span><br><span class="line">                    <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:  <span class="comment"># 遍历状态s下执行动作a的所有可能结果</span></span><br><span class="line">                        p, next_state, r, done = res  <span class="comment"># 获取转移概率p、下一个状态next_state、奖励r和是否终止done</span></span><br><span class="line">                        qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))  <span class="comment"># 计算Q(s,a)的值，考虑折扣因子gamma</span></span><br><span class="line">                    qsa_list.append(self.pi[s][a] * qsa)  <span class="comment"># 将Q(s,a)乘以策略pi(s,a)并加入列表</span></span><br><span class="line">                new_v[s] = <span class="built_in">sum</span>(qsa_list)  <span class="comment"># 计算状态s的新价值函数值</span></span><br><span class="line">                max_diff = <span class="built_in">max</span>(max_diff, <span class="built_in">abs</span>(new_v[s] - self.v[s]))  <span class="comment"># 更新当次地图扫描中最大变化量</span></span><br><span class="line">            self.v = new_v  <span class="comment"># 更新价值函数</span></span><br><span class="line">            <span class="keyword">if</span> max_diff &lt; self.theta: <span class="keyword">break</span>  <span class="comment"># 如果最大变化量小于阈值theta，则退出循环</span></span><br><span class="line">            cnt += <span class="number">1</span>  <span class="comment"># 增加迭代次数</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;策略评估进行%d轮后完成&quot;</span> % cnt)  <span class="comment"># 打印策略评估的迭代次数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">policy_improvement</span>(<span class="params">self</span>):  <span class="comment"># 策略提升</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.nrow * self.env.ncol):  <span class="comment"># 遍历所有状态</span></span><br><span class="line">            qsa_list = []  <span class="comment"># 用于存储状态s下所有动作的Q值</span></span><br><span class="line">            <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 遍历所有动作</span></span><br><span class="line">                qsa = <span class="number">0</span>  <span class="comment"># 初始化Q(s,a)为0</span></span><br><span class="line">                <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:  <span class="comment"># 遍历状态s下执行动作a的所有可能结果</span></span><br><span class="line">                    p, next_state, r, done = res  <span class="comment"># 获取转移概率p、下一个状态next_state、奖励r和是否终止done</span></span><br><span class="line">                    qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))  <span class="comment"># 计算Q(s,a)的值，考虑折扣因子gamma</span></span><br><span class="line">                qsa_list.append(qsa)  <span class="comment"># 将Q(s,a)加入列表</span></span><br><span class="line">            maxq = <span class="built_in">max</span>(qsa_list)  <span class="comment"># 找到状态s下所有动作的最大Q值</span></span><br><span class="line">            cntq = qsa_list.count(maxq)  <span class="comment"># 计算有多少个动作得到了最大Q值</span></span><br><span class="line">            <span class="comment"># 让这些动作均分概率，其他动作的概率为0</span></span><br><span class="line">            self.pi[s] = [<span class="number">1</span> / cntq <span class="keyword">if</span> q == maxq <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> q <span class="keyword">in</span> qsa_list]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;策略提升完成&quot;</span>)  <span class="comment"># 打印策略提升完成</span></span><br><span class="line">        <span class="keyword">return</span> self.pi  <span class="comment"># 返回更新后的策略</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">policy_iteration</span>(<span class="params">self</span>):  <span class="comment"># 策略迭代</span></span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            self.policy_evaluation()  <span class="comment"># 进行策略评估</span></span><br><span class="line">            old_pi = copy.deepcopy(self.pi)  <span class="comment"># 深拷贝当前策略，用于比较</span></span><br><span class="line">            new_pi = self.policy_improvement()  <span class="comment"># 进行策略提升</span></span><br><span class="line">            <span class="keyword">if</span> old_pi == new_pi: <span class="keyword">break</span>  <span class="comment"># 如果策略不再变化，则退出循环</span></span><br><span class="line">                </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_agent</span>(<span class="params">agent, action_meaning, disaster=[], end=[]</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;状态价值：&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(agent.env.nrow):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(agent.env.ncol):</span><br><span class="line">            <span class="comment"># 为了输出美观,保持输出6个字符</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;%6.6s&#x27;</span> % (<span class="string">&#x27;%.3f&#x27;</span> % agent.v[i * agent.env.ncol + j]), end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;策略：&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(agent.env.nrow):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(agent.env.ncol):</span><br><span class="line">            <span class="comment"># 一些特殊的状态,例如悬崖漫步中的悬崖</span></span><br><span class="line">            <span class="keyword">if</span> (i * agent.env.ncol + j) <span class="keyword">in</span> disaster:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;****&#x27;</span>, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">            <span class="keyword">elif</span> (i * agent.env.ncol + j) <span class="keyword">in</span> end:  <span class="comment"># 目标状态</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;EEEE&#x27;</span>, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                a = agent.pi[i * agent.env.ncol + j]</span><br><span class="line">                pi_str = <span class="string">&#x27;&#x27;</span></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(action_meaning)):</span><br><span class="line">                    pi_str += action_meaning[k] <span class="keyword">if</span> a[k] &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;o&#x27;</span></span><br><span class="line">                <span class="built_in">print</span>(pi_str, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env = CliffWalkingEnv()</span><br><span class="line">action_meaning = [<span class="string">&#x27;^&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;&lt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>]</span><br><span class="line">theta = <span class="number">0.001</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">agent = PolicyIteration(env, theta, gamma)</span><br><span class="line">agent.policy_iteration()</span><br><span class="line">print_agent(agent, action_meaning, <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">37</span>, <span class="number">47</span>)), [<span class="number">47</span>])</span><br></pre></td></tr></table></figure>
<h2 id="价值迭代算法-基于价值函数的">价值迭代算法/基于价值函数的</h2>
<blockquote>
<p>价值迭代的核心思想是<strong>直接通过最大化价值函数来找到最优策略</strong>，不需要显式地维护策略。</p>
</blockquote>
<p>使用贝尔曼最优方程</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611191853492.png" class="" title="image-20240611191853492">
<p>的迭代形式</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611152610430.png" class="" title="image-20240611152610430">
<p>进行价值迭代；</p>
<p>使用</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611152643864.png" class="" title="image-20240611152643864">
<p>从迭代完成的状态价值函数中获取策略，即 从当前状态出发，哪个动作的 (即时奖励+下一状态价值) 最大，策略就为哪个动作</p>
<p>最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611153610316.png" class="" title="image-20240611153610316">
<img src="/2024/06/10/reinforce-learning-record/image-20240610213050168.png" class="" title="image-20240610213050168">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">max_diff = <span class="number">0</span></span><br><span class="line">new_v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow  <span class="comment"># 迷宫环境，初始化状态价值</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow):</span><br><span class="line">    qsa_list = []</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        qsa = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:</span><br><span class="line">            p, next_state, r, done = res</span><br><span class="line">            qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))</span><br><span class="line">        qsa_list.append(qsa)  <span class="comment"># 记录状态s下的所有Q(s,a)价值</span></span><br><span class="line">     new_v[s] = <span class="built_in">max</span>(qsa_list)  <span class="comment"># 取最大作为新状态价值</span></span><br><span class="line">     max_diff = <span class="built_in">max</span>(max_diff, <span class="built_in">abs</span>(new_v[s] - self.v[s]))</span><br><span class="line">self.v = new_v</span><br><span class="line"><span class="keyword">if</span> max_diff &lt; self.theta: <span class="keyword">break</span>  <span class="comment"># 满足收敛条件,退出评估迭代</span></span><br><span class="line">cnt += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>完整价值迭代算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ValueIteration</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 价值迭代算法 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, env, theta, gamma</span>):</span><br><span class="line">        <span class="comment"># 初始化函数，接受环境对象env、价值收敛阈值theta和折扣因子gamma</span></span><br><span class="line">        self.env = env  <span class="comment"># 保存环境对象</span></span><br><span class="line">        self.v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow  <span class="comment"># 初始化价值函数v为0，大小为环境的格子数</span></span><br><span class="line">        self.theta = theta  <span class="comment"># 价值收敛阈值</span></span><br><span class="line">        self.gamma = gamma  <span class="comment"># 折扣因子，用于计算未来奖励的折扣</span></span><br><span class="line">        <span class="comment"># 初始化策略pi为None，表示在价值迭代结束后再计算策略</span></span><br><span class="line">        self.pi = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">value_iteration</span>(<span class="params">self</span>):</span><br><span class="line">        cnt = <span class="number">0</span>  <span class="comment"># 计数器，记录价值迭代的轮数</span></span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            max_diff = <span class="number">0</span>  <span class="comment"># 用于记录价值函数的最大变化量</span></span><br><span class="line">            new_v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow  <span class="comment"># 初始化新的价值函数</span></span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow):  <span class="comment"># 遍历所有状态</span></span><br><span class="line">                qsa_list = []  <span class="comment"># 用于存储状态s下所有动作的Q值</span></span><br><span class="line">                <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 遍历所有动作</span></span><br><span class="line">                    qsa = <span class="number">0</span>  <span class="comment"># 初始化Q(s,a)为0</span></span><br><span class="line">                    <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:  <span class="comment"># 遍历状态s下执行动作a的所有可能结果</span></span><br><span class="line">                        p, next_state, r, done = res  <span class="comment"># 获取转移概率p、下一个状态next_state、奖励r和是否终止done</span></span><br><span class="line">                        qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))  <span class="comment"># 计算Q(s,a)的值，考虑折扣因子gamma</span></span><br><span class="line">                    qsa_list.append(qsa)  <span class="comment"># 将Q(s,a)加入列表</span></span><br><span class="line">                new_v[s] = <span class="built_in">max</span>(qsa_list)  <span class="comment"># 更新状态s的价值函数为所有动作Q值的最大值</span></span><br><span class="line">                max_diff = <span class="built_in">max</span>(max_diff, <span class="built_in">abs</span>(new_v[s] - self.v[s]))  <span class="comment"># 更新最大变化量</span></span><br><span class="line">            self.v = new_v  <span class="comment"># 更新价值函数</span></span><br><span class="line">            <span class="keyword">if</span> max_diff &lt; self.theta: <span class="keyword">break</span>  <span class="comment"># 如果最大变化量小于阈值theta，则退出循环</span></span><br><span class="line">            cnt += <span class="number">1</span>  <span class="comment"># 增加迭代次数</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;价值迭代一共进行%d轮&quot;</span> % cnt)  <span class="comment"># 打印价值迭代的轮数</span></span><br><span class="line">        self.get_policy()  <span class="comment"># 根据最终的价值函数导出策略</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_policy</span>(<span class="params">self</span>):  <span class="comment"># 根据价值函数导出一个贪婪策略</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.nrow * self.env.ncol):  <span class="comment"># 遍历所有状态</span></span><br><span class="line">            qsa_list = []  <span class="comment"># 用于存储状态s下所有动作的Q值</span></span><br><span class="line">            <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 遍历所有动作</span></span><br><span class="line">                qsa = <span class="number">0</span>  <span class="comment"># 初始化Q(s,a)为0</span></span><br><span class="line">                <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:  <span class="comment"># 遍历状态s下执行动作a的所有可能结果</span></span><br><span class="line">                    p, next_state, r, done = res  <span class="comment"># 获取转移概率p、下一个状态next_state、奖励r和是否终止done</span></span><br><span class="line">                    qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))  <span class="comment"># 计算Q(s,a)的值，考虑折扣因子gamma</span></span><br><span class="line">                qsa_list.append(qsa)  <span class="comment"># 将Q(s,a)加入列表</span></span><br><span class="line">            maxq = <span class="built_in">max</span>(qsa_list)  <span class="comment"># 找到状态s下所有动作的最大Q值</span></span><br><span class="line">            cntq = qsa_list.count(maxq)  <span class="comment"># 计算有多少个动作得到了最大Q值</span></span><br><span class="line">            <span class="comment"># 让这些动作均分概率，其他动作的概率为0</span></span><br><span class="line">            self.pi[s] = [<span class="number">1</span> / cntq <span class="keyword">if</span> q == maxq <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> q <span class="keyword">in</span> qsa_list]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 环境初始化</span></span><br><span class="line">env = CliffWalkingEnv()  <span class="comment"># 创建一个悬崖漫步环境</span></span><br><span class="line">action_meaning = [<span class="string">&#x27;^&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;&lt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>]  <span class="comment"># 动作的含义，分别表示上、下、左、右</span></span><br><span class="line">theta = <span class="number">0.001</span>  <span class="comment"># 价值收敛阈值</span></span><br><span class="line">gamma = <span class="number">0.9</span>  <span class="comment"># 折扣因子</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建价值迭代算法对象</span></span><br><span class="line">agent = ValueIteration(env, theta, gamma)</span><br><span class="line">agent.value_iteration()  <span class="comment"># 执行价值迭代</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印策略和路径</span></span><br><span class="line">print_agent(agent, action_meaning, <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">37</span>, <span class="number">47</span>)), [<span class="number">47</span>])</span><br></pre></td></tr></table></figure>
<h2 id="策略迭代-价值迭代对比">策略迭代&amp;价值迭代对比</h2>
<ul>
<li>策略迭代的核心思想是<strong>交替进行策略评估和策略提升</strong>，直到策略收敛到最优。</li>
<li>价值迭代的核心思想是<strong>直接通过最大化价值函数来找到最优策略</strong>，不需要显式地维护策略。</li>
</ul>
<p>因此，价值迭代函数 <code>value_iteration</code> 和策略评估函数 <code>policy_evaluation</code> 有着一样的实现</p>
<p><strong>动作价值更新</strong>和<strong>状态价值更新</strong>是价值函数更新的两种方式，可以用于<strong>策略迭代</strong>、<strong>价值迭代</strong>或其他算法。</p>
<ul>
<li>状态价值迭代和动作价值迭代是等价的，只是更新的对象不同。</li>
<li>在价值迭代中，通常使用状态价值迭代，因为它的计算量较小。</li>
</ul>
<h2 id="有模型-无模型-在线策略-离线策略">有模型&amp;无模型+在线策略&amp;离线策略</h2>
<p><strong>有模型强化学习：</strong></p>
<p>智能体学习环境的状态转移函数</p>
<p>环境的状态转移函数已知</p>
<p>智能体可以直接根据状态转移函数得到在对环境进行动作a后环境的下一状态</p>
<p># 状态转移函数P[state][action] = [(p, next_state, reward, done)]包含转移成功概率，下一个状态，奖励和是否完成</p>
<ul>
<li>Dyna-Q</li>
<li>Monte Carlo Tree Search (MCTS)</li>
<li>PILCO (Probabilistic Inference for Learning Control)</li>
</ul>
<p><strong>无模型强化学习：</strong></p>
<p>智能体通过与环境交互学习状态和奖励之间的映射关系</p>
<p>环境的状态转移函数未知</p>
<p>智能体必须通过与环境的交互才能得到环境的下一状态</p>
<ul>
<li>Q-learning</li>
<li>SARSA</li>
<li>Deep Q-Network (DQN)</li>
<li>Policy Gradient methods (e.g., REINFORCE, A2C, PPO)</li>
</ul>
<p>样本：当前状态，下一状态，采取动作，奖励</p>
<p><strong>在线策略学习：</strong></p>
<p>不保存样本</p>
<p><strong>离线策略学习：</strong></p>
<p>使用经验回放池保存样本</p>
<h2 id="算法">算法</h2>
<h3 id="蒙特卡洛方法">蒙特卡洛方法</h3>
<p>Value-based + online</p>
<p>从某状态s出发，基于策略Π，获得一条状态序列，该状态序列对应一个回报G；该过程为一次采样。</p>
<p>反复采样，得到N个状态序列+回报和M，由大数定律，可得该状态的状态价值</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611161858965.png" class="" title="image-20240611161858965">
<p>使用增量更新方法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611161157050.png" class="" title="image-20240611161157050">
<p>补：增量更新原理</p>
<p>新均值 = 旧均值 + 1/总量 * (新值 - 旧均值)</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611161942958.png" class="" title="image-20240611161942958">
<h3 id="时序差分算法">时序差分算法</h3>
<p>Value-based + online</p>
<p>类似于蒙特卡洛，更新状态s的状态价值时，不使用完整的状态序列，在得到下一状态时立即对状态s的状态价值进行更新</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611162513650.png" class="" title="image-20240611162513650">
<p>蒙特卡洛使用第三行对状态价值进行更新，时序差分使用第四行对状态价值进行更新</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611162624769.png" class="" title="image-20240611162624769">
<ul>
<li>只向前走了一步的蒙特卡洛</li>
<li>使用下一状态的状态价值代替了很长的状态序列的回报(状态价值本身就是所有未来时刻的回报期望和)</li>
<li>增量更新体现在 减号前部分为 (新 状态s的状态价值)，减号后部分为 (旧 状态s的状态价值)</li>
</ul>
<h3 id="Sarsa算法">Sarsa算法</h3>
<p>Value-based + online</p>
<p>类似于时序差分算法，对动作价值进行更新，目标是估计 <strong>ε-greedy策略的动作价值函数</strong></p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611191221769.png" class="" title="image-20240611191221769">
<p>算法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612100452913.png" class="" title="image-20240612100452913">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]</span><br><span class="line">self.Q_table[s0, a0] += self.alpha * td_error</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Q表的变化其实是从后向前蔓延的</p>
<p>一个全0的Q表，当某一步产生奖励后，该步对应的Q值出现了值，然后在每一轮的更新中，值向前扩散</p>
<p>我们可以把这个过程形象地比喻成：</p>
<ul>
<li>
<p><strong>“宝藏”被发现：</strong> 当智能体在某一步 <code>(s, a)</code> 执行后，到达 <code>s'</code> 并获得了奖励 <code>r</code> (尤其是正奖励，可以看作找到了“宝藏”的第一丝线索)，那么 <code>Q(s, a)</code> 会首先直接受到这个 <code>r</code> 的影响而更新。如果这是第一次获得奖励，那么这个 <code>Q(s, a)</code> 就从0变成了非0值。</p>
</li>
<li>
<p><strong>“消息”向前传递：</strong></p>
<ul>
<li>在<strong>下一轮</strong>的训练中，如果智能体再次访问到状态 <code>s_prev</code>，并且它采取的动作 <code>a_prev</code> 能够引导它到达之前那个状态 <code>s</code>（也就是宝藏线索更近一步的状态），那么在更新 <code>Q(s_prev, a_prev)</code> 时，会使用到 <code>Q(s, a_selected_in_s)</code>（对于SARSA）或 <code>max_a' Q(s, a')</code>（对于Q-learning）。</li>
<li>因为 <code>Q(s, ...)</code> 已经因为上一轮的奖励而有了一个非零值，这个值就会通过折扣因子 <code>γ</code> “打折”后，贡献给 <code>Q(s_prev, a_prev)</code> 的更新。</li>
<li>这就好比，状态 <code>s</code> 告诉了状态 <code>s_prev</code>：“嘿，从你这里走某条路到我这里，我这里有点好东西（或者预示着好东西）！”</li>
</ul>
</li>
<li>
<p><strong>逐层蔓延/扩散：</strong> 这个过程会不断重复。状态 <code>s_prev</code> 的更新值又会影响到更早访问它的状态 <code>s_prev_prev</code>，以此类推。就像水波一样，从奖励产生的地方（或目标状态附近）开始，价值信息逐渐向起始状态或远离目标的状态扩散。</p>
</li>
</ul>
</blockquote>
<br>
<h3 id="Q-learning算法">Q-learning算法</h3>
<p>Value-based + offline</p>
<p>对动作价值进行更新，目标是估计 <strong>最优策略的动作价值函数</strong></p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611194333945.png" class="" title="image-20240611194333945">
<p>算法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612100509915.png" class="" title="image-20240612100509915">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">td_error = r + self.gamma * self.Q_table[s1].<span class="built_in">max</span>() - self.Q_table[s0, a0]</span><br><span class="line">self.Q_table[s0, a0] += self.alpha * td_error</span><br></pre></td></tr></table></figure>
<blockquote>
<p>对比Sarsa和Q-learning</p>
<p>实际上，$r_t$和$R_t$在各自的算法中都表示在时间步$t$时环境反馈的即时奖励，但在更新公式中的使用方式略有不同。Sarsa算法使用下一个动作$a_{t+1}$的动作价值$Q(s_{t+1}, a_{t+1})$，而Q-learning算法使用下一状态$s_{t+1}$下最优动作的动作价值$\max_{a’} Q(s’, a’)$。</p>
<p>Sarsa 是<strong>在线策略</strong>（on-policy）算法， Q-learning 是<strong>离线策略</strong>（off-policy）算法，因为目标策略和行为策略分离，Q-learning 可以采用非常具有探索性的行为策略（甚至是完全随机的策略），同时仍然学习最优的Q值。这使得它在探索未知环境时可能更有效率。SARSA 学习的价值函数直接受到其探索行为的影响。如果探索策略包含了一些“愚蠢”的动作，SARSA计算出的Q值会反映这些“愚蠢”动作的后果。</p>
<p>说人话就是，Q-learning在实际行动中“大胆尝试”，但在学习和评估时则“以最优为目标”。</p>
</blockquote>
<blockquote>
<p>Q: 为什么Q-leanring中的Q表的值是从：一个全0的Q表，当某一步产生奖励后，该步对应的Q值出现了值，然后在每一轮的更新中，值向前扩散；为什么我们不直接从终点出发（从有reward的地方出发），反向向前推进，直接一次遍历得到Q表呢？这不是更快吗？</p>
<p>A: 你提出的想法非常好，也非常有洞察力！你所描述的“从终点出发，反向向前推进，直接一次遍历得到Q表”的方法，其实非常接近甚至就是一种经典的**动态规划（Dynamic Programming, DP）<strong>方法，比如</strong>价值迭代（Value Iteration）<strong>或</strong>策略迭代（Policy Iteration）**中的一部分。</p>
<p>那么，为什么Q-learning不直接这么做呢？主要原因在于它们解决问题的<strong>前提假设和适用场景</strong>不同：</p>
<ol>
<li>
<p><strong>模型未知 vs. 模型已知 (Model-Free vs. Model-Based)</strong></p>
<ul>
<li><strong>你描述的方法 (类似DP)：</strong> 这种方法通常需要我们<strong>已知环境的模型 (Model-Based)</strong>。也就是说，你需要知道：
<ul>
<li><strong>状态转移概率 P(s’ | s, a):</strong> 在状态s执行动作a后，转移到状态s’的概率。</li>
<li><strong>奖励函数 R(s, a, s’):</strong> 在状态s执行动作a并转移到状态s’后能获得的奖励。</li>
<li>有了这些信息，你才能从终点（或有奖励的地方）准确地反向推算出前一个状态采取什么动作能达到这个奖励状态，以及这个动作的价值。例如，在价值迭代中，我们会用贝尔曼最优方程 <code>V*(s) = max_a Σ_&#123;s'&#125; P(s'|s,a) [R(s,a,s') + γV*(s')]</code> 来更新所有状态的价值，这确实是从“结果”倒推。</li>
</ul>
</li>
<li><strong>Q-learning：</strong> Q-learning是一种<strong>模型无关 (Model-Free)</strong> 的强化学习算法。它<strong>不需要事先知道环境的模型</strong>。智能体就像一个在未知迷宫中探索的人，它只能通过与环境交互（尝试动作、观察结果和奖励）来学习。它不知道“墙的另一边是什么”，也不知道“走这条路一定能拿到奶酪”。</li>
</ul>
</li>
<li>
<p><strong>学习方式：通过经验学习 vs. 通过规划学习</strong></p>
<ul>
<li><strong>你描述的方法 (类似DP)：</strong> 这是在<strong>规划 (Planning)</strong>。你拥有了完整的地图（模型），然后你可以在脑海中或纸上进行推演，计算出最优路径。</li>
<li><strong>Q-learning：</strong> 这是在<strong>学习 (Learning)</strong>。智能体通过一次次的<strong>试错 (trial-and-error)</strong> 来积累经验。
<ul>
<li>初始Q表为0，表示对环境一无所知。</li>
<li>当某一步产生奖励时，Q-learning会更新该<code>(状态, 动作)</code>对的Q值。这个Q值代表了“在那个状态下执行那个动作，期望未来能获得的总回报（包括即时奖励）”。</li>
<li>“值向前扩散”（实际上是奖励信息通过贝尔曼方程反向传播）的过程，是智能体逐渐认识到“哦，原来这个动作最终能导向那个有奖励的状态，所以这个动作的价值也应该高一些”。这个“扩散”是基于实际探索到的路径和奖励。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong></p>
<ul>
<li><strong>你描述的方法 (类似DP)：</strong> 非常适合于模型已知且状态空间不至于过大的问题，例如棋盘游戏（规则已知）、最短路径问题（地图已知）。它的计算效率在模型已知时通常更高。</li>
<li><strong>Q-learning：</strong> 非常适合于模型未知或模型非常复杂难以精确描述的环境，例如机器人控制（物理模型复杂）、很多现实世界的交互问题。它可以通过与真实环境或模拟环境交互来学习。</li>
</ul>
</li>
</ol>
<p><strong>为什么Q-learning的“扩散”看起来慢？</strong></p>
<ul>
<li><strong>探索的必要性：</strong> 因为模型未知，Q-learning需要通过探索（比如ε-greedy策略）来发现新的路径和潜在的奖励。如果它只走已知的“好路”，可能永远发现不了更好的路径。</li>
<li><strong>采样效率：</strong> Q-learning是基于采样的，它只更新实际经历过的<code>(状态, 动作)</code>对。如果一个状态很少被访问，或者某个通往奖励的路径很长，那么奖励信息就需要很多轮迭代才能慢慢“传播”回去。</li>
<li><strong>在线学习：</strong> Q-learning可以进行在线学习，即在与环境交互的过程中不断更新Q表，适应环境的变化。而DP通常是离线进行的，假设环境模型固定。</li>
</ul>
<p><strong>总结一下：</strong></p>
<ul>
<li>如果你<strong>有完整的环境模型</strong>（知道所有状态转移和奖励），那么你描述的从终点反向传播的方法（如价值迭代）确实是更直接和高效的，能够一次性（或经过几次迭代）计算出Q表。</li>
<li>Q-learning的设计初衷是为了解决<strong>模型未知</strong>的问题。它通过与环境交互、逐步探索和利用贝尔曼方程来学习Q值，这个过程看起来像是“奖励向前扩散”，但本质上是奖励信息通过经验逐步反向传播更新先前状态的价值。</li>
</ul>
<p>所以，不是Q-learning“傻”，而是它被设计用来解决一类更普遍（模型未知）的问题，为此它采用了通过经验学习的方式，这种方式的特性就是逐步更新和信息传播。</p>
</blockquote>
<h3 id="DQN-DDQN">DQN/DDQN</h3>
<p>Value-based + offline</p>
<p><strong>DQN</strong></p>
<p>将Q-learning的Q表换成net</p>
<p>网络的损失函数</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250322111503025.png" class="" title="image-20250322111503025">
<blockquote>
<p>DQN网络旨在让Q(s,a)更新到逼近最优动作的动作价值$r + \max_{a’} Q(s’, a’)$</p>
<p>单网络存在问题：</p>
<p>在 DQN 中，损失函数TD本身依赖于神经网络的输出 <em>Q</em>(<em>s</em>′,<em>a</em>′)。这意味着：</p>
<ul>
<li>当我们更新神经网络的参数时，<em>Q</em>(<em>s</em>′,<em>a</em>′) 也会发生变化。</li>
<li>这种变化会导致 TD 目标不断改变，从而使得训练过程变得不稳定。</li>
</ul>
</blockquote>
<p>训练网络每步更新，目标网络每隔C步进行一次更新；</p>
<p>使用目标网络计算最优动作价值（最优动作选取使用目标网络），最小化损失函数更新训练网络；</p>
<p><strong>Double DQN</strong></p>
<p>DQN在<strong>选择和评估动作使用的是同一套神经网络</strong>，如果神经网络在估算Q值时存在误差（尤其是正向误差），这种误差会在选择和评估动作的过程中被放大。具体来说：</p>
<ul>
<li>当神经网络高估某个动作的Q值时，该动作更可能被选为最优动作。</li>
<li>被选中的动作的Q值又被用于计算目标Q值，进一步放大了高估的影响。</li>
<li>这种正向误差会在更新过程中逐步累积，导致Q值严重偏离真实值。</li>
</ul>
<p>Double DQN通过两套独立网络分别选择和评估动作，有效降低了过估计问题，使目标Q值更接近真实值。</p>
<p>这种改进在动作空间较大的任务中尤为重要，能够显著提升算法的稳定性和性能。</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250322111831741.png" class="" title="image-20250322111831741">
<h3 id="策略梯度算法-REINFORCE">策略梯度算法(REINFORCE)</h3>
<p>Policy-based + online</p>
<p>定义策略学习的目标函数为，s0为初始状态</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612100629893.png" class="" title="image-20240612100629893">
<p>目标是修改参数θ，使J(θ)取最大；即对θ求导</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610213003398.png" class="" title="image-20240610213003398">
<img src="/2024/06/10/reinforce-learning-record/image-20240612100732404.png" class="" title="image-20240612100732404">
<p>where 第一行第一个求和符号后的对象为 策略的<strong>状态访问分布</strong></p>
<blockquote>
<p>下图中的 (1-γ) 为归一化参数，原因为几何级数的求和公式为这个</p>
</blockquote>
<img src="/2024/06/10/reinforce-learning-record/image-20240612102026769.png" class="" title="image-20240612102026769">
<p>策略的状态访问分布表示该策略和在环境中会访问到的状态的分布情况(即到达每个状态的概率)</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612102256431.png" class="" title="image-20240612102256431">
<p>故该求导表示 <strong>对于每个状态，我们有一定的概率在该状态，从该状态出发，策略决定了我们能获得的价值的期望</strong></p>
<p>目标：</p>
<p>​	最大化函数J(θ)</p>
<p>算法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611200059062.png" class="" title="image-20240611200059062">
<p>时刻t向后的回报即为(动作)价值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">reward_list = transition_dict[<span class="string">&#x27;rewards&#x27;</span>]</span><br><span class="line">state_list = transition_dict[<span class="string">&#x27;states&#x27;</span>]</span><br><span class="line">action_list = transition_dict[<span class="string">&#x27;actions&#x27;</span>]</span><br><span class="line"></span><br><span class="line">G = <span class="number">0</span></span><br><span class="line">self.optimizer.zero_grad()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(reward_list))):  <span class="comment"># 从最后一步算起</span></span><br><span class="line">    reward = reward_list[i]  <span class="comment"># 取时间步i获得的奖励</span></span><br><span class="line">    state = torch.tensor([state_list[i]],</span><br><span class="line">                         dtype=torch.<span class="built_in">float</span>).to(self.device)  <span class="comment"># 取时间步i的状态</span></span><br><span class="line">    action = torch.tensor([action_list[i]]).view(-<span class="number">1</span>, <span class="number">1</span>).to(self.device)  <span class="comment"># 取时间步i的动作</span></span><br><span class="line">    log_prob = torch.log(self.policy_net(state).gather(<span class="number">1</span>, action))</span><br><span class="line">    <span class="comment"># self.policy_net(state)： 将状态输入策略网络，得到所有可能动作的概率分布。</span></span><br><span class="line">	<span class="comment"># .gather(1, action): 从概率分布中选取实际执行动作对应的概率值。</span></span><br><span class="line">	<span class="comment"># torch.log(...): 对选取的概率值取对数，得到对数概率</span></span><br><span class="line">    G = self.gamma * G + reward  <span class="comment"># G累计了从当前时间步i到episode结束的所有奖励</span></span><br><span class="line">    loss = -log_prob * G  <span class="comment"># loss对应公式中符号α后的部分</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">self.optimizer.step()  <span class="comment"># 梯度下降，实际上是对策略进行求导</span></span><br></pre></td></tr></table></figure>
<h3 id="Actor-Critic">Actor-Critic</h3>
<p>(Value + Policy)-based + offline</p>
<p><strong>Actor采用策略迭代算法更新，Critic采用价值(状态价值)迭代算法更新</strong>，Actor的更新依赖于于Critic计算的优势函数；</p>
<p>策略函数梯度</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612110026419.png" class="" title="image-20240612110026419">
<p>其中，ψ_t可为不同值，表示不同的方法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612110241521.png" class="" title="image-20240612110241521">
<blockquote>
<p>注意，理论上来说，在收敛模型上上述6种方法得到的值是一致的，这六种方法都是基于策略梯度定理（Policy Gradient Theorem）的不同实现形式</p>
</blockquote>
<p>价值函数的损失函数与梯度</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612110951796.png" class="" title="image-20240612110951796">
<img src="/2024/06/10/reinforce-learning-record/image-20240612111022161.png" class="" title="image-20240612111022161">
<p>Actor基于策略函数，使用上图中的第6种的 <code>td_delta</code> 作为动作价值，使用Critic的价值函数得到动作价值的值；</p>
<p>Critic基于价值函数，用于最小化当前状态的价值估计与时序差分目标之间的差异，反向传播时对 <code>td_delta</code> 做了 <code>.detach()</code></p>
<p>基于<strong>时序差分(TD)<strong>方法的</strong>Actor-Critic</strong>算法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612110750284.png" class="" title="image-20240612110750284">
<p>价值网络使用时序差分(TD)更新，策略网络使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">states = torch.tensor(transition_dict[<span class="string">&#x27;states&#x27;</span>],</span><br><span class="line">                              dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line">actions = torch.tensor(transition_dict[<span class="string">&#x27;actions&#x27;</span>]).view(-<span class="number">1</span>, <span class="number">1</span>).to(</span><br><span class="line">            self.device)</span><br><span class="line">rewards = torch.tensor(transition_dict[<span class="string">&#x27;rewards&#x27;</span>],</span><br><span class="line">                               dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>).to(self.device)</span><br><span class="line">next_states = torch.tensor(transition_dict[<span class="string">&#x27;next_states&#x27;</span>],</span><br><span class="line">                                   dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line">dones = torch.tensor(transition_dict[<span class="string">&#x27;dones&#x27;</span>],</span><br><span class="line">                             dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>).to(self.device)</span><br><span class="line"><span class="comment"># 当前状态，动作、奖励、下一个状态和结束标志</span></span><br><span class="line"></span><br><span class="line">td_target = rewards + self.gamma * self.critic(next_states) * (<span class="number">1</span> - dones)</span><br><span class="line"><span class="comment"># 价值网络(critic)</span></span><br><span class="line"><span class="comment"># self.critic(next_states) 使用价值网络(critic)预测下一个状态的价值。</span></span><br><span class="line"><span class="comment"># (1 - dones) 用于处理 episode 结束的情况，如果 dones 为 1 (True)，则表示 episode 结束，此时不需要考虑未来的奖励。</span></span><br><span class="line">td_delta = td_target - self.critic(states)  </span><br><span class="line"><span class="comment"># 计算时序差分误差，即目标值与当前状态价值的差。</span></span><br><span class="line"></span><br><span class="line">log_probs = torch.log(self.actor(states).gather(<span class="number">1</span>, actions))</span><br><span class="line"><span class="comment"># 策略网络(actor)</span></span><br><span class="line"><span class="comment"># self.actor(states) 使用策略网络预测每个动作的概率。</span></span><br><span class="line"><span class="comment"># .gather(1, actions) 从预测的概率分布中选择实际采取的动作对应的概率。torch.log 计算对数概率。</span></span><br><span class="line"></span><br><span class="line">critic_loss = torch.mean(</span><br><span class="line">	F.mse_loss(self.critic(states), td_target.detach()))</span><br><span class="line"><span class="comment"># 计算价值网络的损失函数。(价值网络给出的当前状态价值)和(r+价值网络给出的下一状态的状态价值)的均方误差，即((a-b)^2)/2</span></span><br><span class="line"><span class="comment"># 使用 td_target.detach() 阻止梯度通过 td_target 向策略网络回传。</span></span><br><span class="line">actor_loss = torch.mean(-log_probs * td_delta.detach())</span><br><span class="line"><span class="comment"># 计算策略网络的损失函数。策略函数梯度</span></span><br><span class="line"><span class="comment"># 使用 td_delta.detach() 阻止梯度通过 td_delta 向价值网络回传。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">self.actor_optimizer.zero_grad()</span><br><span class="line">self.critic_optimizer.zero_grad()</span><br><span class="line">actor_loss.backward()  <span class="comment"># 计算策略网络的梯度</span></span><br><span class="line">critic_loss.backward()  <span class="comment"># 计算价值网络的梯度</span></span><br><span class="line">self.actor_optimizer.step()  <span class="comment"># 更新策略网络的参数</span></span><br><span class="line">self.critic_optimizer.step()  <span class="comment"># 更新价值网络的参数</span></span><br></pre></td></tr></table></figure>
<h3 id="TRPO">TRPO</h3>
<p>策略函数</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250324145210312.png" class="" title="image-20250324145210312">
<blockquote>
<p>E 的下标 $s_0$ 表示期望是“关于”初始状态 s_0 的分布来计算的, 这个公式中的 $E_{s_0}$ <strong>明确地表示初始状态 $s_0$ 是一个随机变量，它遵循某个初始状态分布 (initial state distribution)。</strong></p>
</blockquote>
<p>在AC的，优化目标为最大化 <code>J(θ)</code>，通过梯度更新 ；</p>
<p>在TRPO中，优化目标为最大化 新策略与旧策略的价值差 <code>J(θ')- J(θ) </code> ，通过近似方法（相似策略下状态分布函数不变假设与<strong>重要性采样</strong>）将原目标  <code>J(θ')- J(θ) </code>  近似为下图，且满足<strong>KL散度</strong>约束（<strong>约束</strong>策略为相似策略）</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250324141848364.png" class="" title="image-20250324141848364">
<p>TRPO与AC的Critic相同；</p>
<h4 id="整体流程">整体流程</h4>
<img src="/2024/06/10/reinforce-learning-record/image-20250322150349844.png" class="" title="image-20250322150349844">
<img src="/2024/06/10/reinforce-learning-record/image-20250322150502092.png" class="" title="image-20250322150502092">
<br>
<h4 id="近似处理">近似处理</h4>
<p>在策略优化中，我们希望找到新策略，使其期望回报最大化。但是，由于计算新策略的状态部分困难，有如下假设：</p>
<p>TRPO假设：<strong>当新旧策略非常接近时，状态分布的变化可以忽略</strong>，即<strong>新策略的状态分布和旧策略的状态分布一致</strong></p>
<blockquote>
<p>合理性：如果策略更新步幅很小（通过后续的KL散度或信任域约束），新旧策略生成的轨迹相似，因此状态分布的差异是“高阶小量”，可以被忽略。这类似于泰勒展开中的一阶近似。</p>
</blockquote>
<p>状态分布被近似为旧策略，但是动作选择使用新策略，为了处理动作分布的差异，TRPO使用<strong>重要性采样</strong>（Importance Sampling），即图中的新旧策略概率比值；</p>
<p>最后，状态分布使用旧策略的，动作选择使用旧策略的，加权计算新策略的改进量。</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250322151444000.png" class="" title="image-20250322151444000">
<img src="/2024/06/10/reinforce-learning-record/image-20250322151537751.png" class="" title="image-20250322151537751">
<blockquote>
<p>重要性采样是什么?</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250611134637079.png" class="" title="image-20250611134637079">
</blockquote>
<br>
<h4 id="近似约束与求解">近似约束与求解</h4>
<p>由于这里使用了近似，所以需要引入约束，使近似可行。因此，为了保证新旧策略足够接近，TRPO 使用了<strong>库尔贝克-莱布勒</strong>（Kullback-Leibler，KL）散度来衡量策略之间的距离。</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250322162045699.png" class="" title="image-20250322162045699">
<br>
<h4 id="广义优势">广义优势</h4>
<p>多步时序差分的指数加权平均</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250325154631583.png" class="" title="image-20250325154631583">
<blockquote>
<p>广义优势是干嘛的？</p>
<p>优势是优化目标里的那个 $A^{\pi_{\theta}}(s_{t}, a_{t})$，定义是： $A^{\pi_{\theta}}(s_t, a_t) = Q^{\pi_{\theta}}(s_t, a_t) - V^{\pi_{\theta}}(s_t) = \left( r(s,a) + \gamma E_{s’ \sim P(\cdot|s,a)}[V^{\pi}(s’)] \right) - V^{\pi}(s)$，优势函数告诉我们：在状态 <code>s</code> 下，选择动作 <code>a</code> <strong>相比于遵循当前策略 <code>π</code> 的平均表现</strong>，是更好还是更差。</p>
<p>广义优势其实就是将单步优势变成了多步优势，使得优势更加平均；</p>
<p>单步TD误差（$\lambda=0$）：方差相对较低，但如果值函数估计 $V^{\pi_{\theta}}$ 不准，会有较高的偏差。<br>
蒙特卡洛（MC）优势估计（相当于GAE中 $\lambda=1$）：使用从当前时刻到回合结束的完整回报来估计 $Q^{\pi_{\theta}}(s_t, a_t)$，然后减去 $V^{\pi_{\theta}}(s_t)$。这种方法偏差低（因为它不依赖于后续的值函数估计，只依赖于当前的 $V^{\pi_{\theta}}(s_t)$），但方差非常高，因为它累积了多个时间步的随机性。</p>
</blockquote>
<blockquote>
<p>Critic网络和Actor网络在哪里？</p>
<p><strong>广义优势的计算依赖于 Critic</strong>，Critic网络的核心任务是学习一个值函数 <code>V(s)</code>，用于评估在某个状态 <code>s</code> 下的期望累积回报。在GAE的计算中，<code>V(s_t)</code> 和 <code>V(s_&#123;t+1&#125;)</code> 这两个关键的组成部分<strong>直接由Critic网络输出</strong>。因此，GAE的准确性直接受到Critic网络估计准确性的影响。如果Critic估计不准，那么计算出的GAE也会不准确。</p>
<ul>
<li><strong>Critic网络</strong>：直接参与GAE的计算，提供状态值 <code>V(s)</code>，是计算TD误差 <code>δ_t</code> 和进而计算 <code>A_t^GAE</code> 的基础。</li>
<li><strong>Actor网络</strong>：
<ul>
<li>生成用于计算GAE的经验数据（状态、动作、奖励），Actor网络根据当前策略与环境进行交互，生成一系列的轨迹 (episodes)，即 <code>(s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T)</code>。这些收集到的状态 <code>s_t</code>、<code>s_&#123;t+1&#125;</code> 和奖励 <code>r_t</code> 是计算GAE所必需的原始数据。</li>
<li>是GAE最终服务的对象。GAE计算出来后，用于指导Actor网络参数的更新，使其学习到更好的策略。GAE计算出来的优势 <code>A_t^GAE</code> 用于评估在状态 <code>s_t</code> 下采取动作 <code>a_t</code> 相对于平均动作的好坏程度。这个优势值随后被用于构建TRPO的目标函数（或其替代目标函数）的梯度，以更新Actor网络的参数 <code>θ</code>。</li>
</ul>
</li>
</ul>
</blockquote>
<br>
<h4 id="算法-2">算法</h4>
<img src="/2024/06/10/reinforce-learning-record/image-20250322211023368.png" class="" title="image-20250322211023368">
<h3 id="PPO-clip">PPO-clip</h3>
<p>基于AC和TRPO的，替换<strong>KL散度约束</strong>实现近似为<strong>惩罚优化/裁剪目标优化</strong>实现近似</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250325200037093.png" class="" title="image-20250325200037093">
<blockquote>
<p>说白了就是把约束去掉变成无约束问题，更加易于求解</p>
</blockquote>
<h3 id="DDPG">DDPG</h3>
<p>确定性策略</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250323105541258.png" class="" title="image-20250323105541258">
<p>4个网络，价值目标+训练网络，策略目标+训练网络</p>
<p>动作选择引入噪声</p>
<p>价值网络更新使用类似于DDQN的思路更新，残差计算时使用目标网络值-训练网络值</p>
<p>策略网络使用？（证明好复杂</p>
<p>价值目标网络和策略目标网络使用软更新</p>
<h2 id="部分心得">部分心得</h2>
<h3 id="AC分离">AC分离</h3>
<p><strong>关于DQN算法中网络的构成</strong></p>
<ul>
<li><strong>Deep Q-Network (DQN):</strong> 当状态空间或动作空间变得非常大时，我们使用神经网络来近似Q函数，这就是DQN。
<ul>
<li>DQN的核心是一个<strong>Q网络 (Q-Network)</strong>。这个网络输入状态 <code>s</code>，输出对应状态下所有可能动作的Q值。例如，如果环境有3个离散动作，输入状态 <code>s</code>，网络会输出 <code>[Q(s, a1), Q(s, a2), Q(s, a3)]</code>。</li>
<li><strong>动作选择 (Policy Derivation):</strong> 在DQN中，策略（如何选择动作）是根据Q网络输出的Q值<strong>派生</strong>出来的，通常使用ε-greedy策略：
<ul>
<li>以 <code>1-ε</code> 的概率选择具有最大Q值的动作：<code>a = argmax_a' Q(s, a')</code></li>
<li>以 <code>ε</code> 的概率随机选择一个动作（用于探索）。</li>
</ul>
</li>
<li><strong>所以，DQN中并没有一个独立的、参数化的“Actor网络”来直接输出策略 <code>π(a|s)</code> 或确定性动作 <code>a = μ(s)</code>。</strong> 它的“行为” (acting) 是由Q网络的结果驱动的。Q网络本身是一个<strong>价值网络 (Value Network)</strong>。</li>
</ul>
</li>
</ul>
<p>因此，Q-learning及其深度版本DQN本质上是<strong>基于值 (Value-based)</strong> 的方法，它们学习一个值函数（Q函数），然后从这个值函数中隐式地或显式地导出策略。它们没有像Actor-Critic方法那样显式地学习一个独立的策略网络（Actor）。</p>
<br>
<p><strong>为什么会出现Actor-Critic网络的分离？</strong></p>
<p>Actor-Critic (AC) 架构将策略学习和价值学习分离开来，由两个独立的（或部分共享参数的）网络（或函数逼近器）来承担：</p>
<ol>
<li>
<p><strong>Actor (策略网络 <code>π_θ(a|s)</code>)</strong>:</p>
<ul>
<li>负责学习策略，即在给定状态 <code>s</code> 下应该采取什么动作 <code>a</code>（或动作的概率分布）。</li>
<li>它的目标是最大化期望累积回报。</li>
<li>参数为 <code>θ</code>。</li>
</ul>
</li>
<li>
<p><strong>Critic (价值网络 <code>V_φ(s)</code> 或 <code>Q_φ(s,a)</code>)</strong>:</p>
<ul>
<li>负责评估Actor所选择的策略的好坏。它学习一个值函数，这个值函数可以是对当前策略下状态 <code>s</code> 的价值 <code>V(s)</code>（状态值函数），或者是状态-动作对 <code>(s,a)</code> 的价值 <code>Q(s,a)</code>（动作值函数）。</li>
<li>它的目标是尽可能准确地估计价值。</li>
<li>参数为 <code>φ</code>。</li>
</ul>
</li>
</ol>
<p><strong>分离的主要原因和优势如下：</strong></p>
<ol>
<li>
<p><strong>降低策略梯度的方差 (Variance Reduction) - 这是最核心的原因：</strong></p>
<ul>
<li>纯粹的策略梯度方法（如REINFORCE）直接使用蒙特卡洛回报 <code>G_t</code> 来估计策略梯度。这种回报的方差通常非常大，因为一个好的动作可能因为后续一系列坏的随机事件而导致低回报，反之亦然。高方差使得学习过程不稳定且收敛缓慢。</li>
<li>Critic通过学习一个值函数（如 <code>V(s)</code>）提供了一个<strong>基线 (baseline)</strong>。策略梯度可以使用<strong>优势函数 (Advantage Function)</strong> <code>A(s, a) = Q(s, a) - V(s)</code> 或者 <code>A(s, a) = r_t + γV(s_&#123;t+1&#125;) - V(s_t)</code> (TD误差) 来更新。</li>
<li>优势函数衡量了在状态 <code>s</code> 下采取动作 <code>a</code> 相对于在该状态下采取平均动作的好坏程度。使用优势函数代替原始回报 <code>G_t</code> 可以显著降低梯度的方差，因为我们减去了一个与当前状态相关的期望值，使得梯度信号更集中于动作本身的好坏，而不是状态本身的好坏或后续轨迹的随机性。</li>
<li><strong>更稳定的学习信号：</strong> Critic为Actor提供了一个更稳定、方差更低的学习信号，指导Actor如何调整其策略。</li>
</ul>
</li>
<li>
<p><strong>处理连续动作空间的能力：</strong></p>
<ul>
<li><strong>Value-based方法 (如DQN)</strong> 在处理高维或连续动作空间时会遇到困难。如果动作是连续的，<code>argmax_a Q(s, a)</code> 变成了一个在每个时间步都需要解决的复杂优化问题。</li>
<li><strong>Actor-Critic方法</strong> 中的Actor可以直接输出连续动作（例如，输出高斯分布的均值和方差），或者直接输出确定性动作，这使得它们非常适合连续动作空间。</li>
</ul>
</li>
<li>
<p><strong>学习随机策略的能力：</strong></p>
<ul>
<li>Actor可以直接参数化一个随机策略 <code>π(a|s)</code>，这在某些情况下是必要的（例如，部分可观察环境，或者为了更好的探索）。</li>
<li>虽然可以通过ε-greedy从Q值导出随机性，但Actor可以直接学习更复杂的随机策略。</li>
</ul>
</li>
<li>
<p><strong>更快的收敛（有时）：</strong></p>
<ul>
<li>由于方差降低和更有效的信用分配，AC方法有时可以比纯策略梯度方法或某些纯价值方法收敛更快。Critic的引导可以帮助Actor更快地找到好的策略方向。</li>
</ul>
</li>
<li>
<p><strong>模块化和灵活性：</strong></p>
<ul>
<li>Actor和Critic可以有不同的网络结构、学习率，甚至可以使用不同的优化算法。这种模块化设计提供了更大的灵活性。</li>
<li>例如，在A2C/A3C中，Actor和Critic可以共享底层的特征提取层，然后在顶部分别有各自的输出头。</li>
</ul>
</li>
</ol>
<p><strong>总结一下Actor-Critic分离的动机：</strong></p>
<ul>
<li><strong>核心：</strong> Critic帮助Actor进行更有效的学习，主要是通过<strong>降低策略梯度的方差</strong>。</li>
<li><strong>适用性：</strong> 使强化学习算法能更好地应用于<strong>连续动作空间</strong>。</li>
<li><strong>策略表达：</strong> 方便学习<strong>随机策略</strong>。</li>
</ul>
<p>所以，Actor-Critic的分离是一种权衡和结合：Actor负责“做什么”，Critic负责“做得怎么样”，两者协同工作，使得算法在具有挑战性的环境中表现更好，学习更稳定。</p>
<br>
<p><strong>说明案例</strong></p>
<p>好的，我们来举一个具体的例子来说明蒙特卡洛回报 <code>G_t</code> 的高方差问题，以及为什么它会导致学习不稳定。</p>
<p><strong>场景设定：</strong></p>
<p>假设我们有一个简单的环境，智能体从一个起始状态 <code>S_0</code> 开始，可以采取两个动作：<code>A_left</code> 或 <code>A_right</code>。</p>
<ul>
<li><strong>动作效果：</strong>
<ul>
<li><code>A_left</code>：有 70% 的概率立即获得 +10 的奖励并结束回合 (win)；有 30% 的概率立即获得 -5 的奖励并结束回合 (lose)。</li>
<li><code>A_right</code>：进入一个中间状态 <code>S_intermediate</code>。</li>
</ul>
</li>
<li><strong>中间状态 <code>S_intermediate</code>：</strong>
<ul>
<li>从 <code>S_intermediate</code> 开始，无论采取什么后续动作（假设只有一个默认动作），有 50% 的概率获得 +100 的巨大奖励并结束回合 (big win)；有 50% 的概率获得 -100 的巨大惩罚并结束回合 (big loss)。</li>
</ul>
</li>
</ul>
<p><strong>智能体的策略：</strong></p>
<p>假设智能体的策略网络 <code>π_θ(a|S_0)</code> 在起始状态 <code>S_0</code> 对两个动作的输出概率是可调整的。我们的目标是学习到在 <code>S_0</code> 时应该选择哪个动作。</p>
<p><strong>纯粹的策略梯度 (REINFORCE) 如何工作：</strong></p>
<p>REINFORCE算法的策略梯度更新规则大致如下：<br>
<code>∇_θ J(θ) ≈ E [ Σ_t ∇_θ log π_θ(a_t|s_t) * G_t ]</code></p>
<p>其中 <code>G_t</code> 是从时间步 <code>t</code> 开始到回合结束的累积折扣回报。为了简化，我们假设折扣因子 <code>γ = 1</code>，并且我们只关注在 <code>S_0</code> 处做的第一个决策。所以，<code>G_0</code> 就是整个回合的总回报。</p>
<p><strong>分析高方差问题：</strong></p>
<p>我们来分析在 <code>S_0</code> 采取不同动作时 <code>G_0</code> 的情况：</p>
<ol>
<li>
<p><strong>如果智能体在 <code>S_0</code> 选择 <code>A_left</code>：</strong></p>
<ul>
<li>有 70% 的概率，<code>G_0 = +10</code>。</li>
<li>有 30% 的概率，<code>G_0 = -5</code>。</li>
<li><code>A_left</code> 的期望回报 = <code>0.7 * 10 + 0.3 * (-5) = 7 - 1.5 = 5.5</code>。这是一个相当不错的稳定正回报。</li>
</ul>
</li>
<li>
<p><strong>如果智能体在 <code>S_0</code> 选择 <code>A_right</code>：</strong></p>
<ul>
<li>智能体进入 <code>S_intermediate</code>。</li>
<li>然后，有 50% 的概率，<code>G_0 = +100</code>。</li>
<li>有 50% 的概率，<code>G_0 = -100</code>。</li>
<li><code>A_right</code> 的期望回报 = <code>0.5 * 100 + 0.5 * (-100) = 50 - 50 = 0</code>。从期望上看，这个动作并不好。</li>
</ul>
</li>
</ol>
<p><strong>高方差如何影响学习：</strong></p>
<p>假设智能体通过多次试验来学习。</p>
<ul>
<li>
<p><strong>考虑 <code>A_left</code>：</strong></p>
<ul>
<li>大部分情况下，智能体采取 <code>A_left</code> 会得到 <code>+10</code> 的回报。<code>log π_θ(A_left|S_0)</code> 这一项会乘以一个正数，使得策略更倾向于 <code>A_left</code>。</li>
<li>少数情况下，得到 <code>-5</code>。<code>log π_θ(A_left|S_0)</code> 会乘以一个负数，略微抑制 <code>A_left</code>。</li>
<li>回报的方差相对较小（值在 -5 到 10 之间波动）。</li>
</ul>
</li>
<li>
<p><strong>考虑 <code>A_right</code>：</strong></p>
<ul>
<li>一半情况下，智能体采取 <code>A_right</code> 会得到 <code>+100</code> 的巨大回报。<code>log π_θ(A_right|S_0)</code> 会乘以一个非常大的正数。这会<strong>极大地</strong>增强选择 <code>A_right</code> 的倾向。</li>
<li>另一半情况下，智能体采取 <code>A_right</code> 会得到 <code>-100</code> 的巨大惩罚。<code>log π_θ(A_right|S_0)</code> 会乘以一个非常大的负数。这会<strong>极大地</strong>抑制选择 <code>A_right</code> 的倾向。</li>
<li>回报的方差非常大（在 -100 和 +100 之间剧烈波动）。</li>
</ul>
</li>
</ul>
<p><strong>学习过程中的问题：</strong></p>
<ol>
<li>
<p><strong>不稳定的梯度估计：</strong> 假设智能体当前策略对 <code>A_left</code> 和 <code>A_right</code> 的概率相近。</p>
<ul>
<li>如果某一批次的经验中，恰好多次采取 <code>A_right</code> 都碰上了 <code>+100</code> 的好运气，那么梯度更新会强烈地让智能体未来更倾向于选择 <code>A_right</code>，尽管 <code>A_right</code> 的期望回报是0，不如 <code>A_left</code> 的期望回报5.5。</li>
<li>反之，如果多次采取 <code>A_right</code> 都碰上了 <code>-100</code> 的坏运气，梯度更新会强烈地让智能体避开 <code>A_right</code>。</li>
<li>这种基于少数样本的剧烈波动使得梯度的方向非常不稳定。</li>
</ul>
</li>
<li>
<p><strong>“好的动作可能因为后续一系列坏的随机事件而导致低回报”：</strong><br>
在这个例子中，<code>A_left</code> 本身是一个期望回报为正 (5.5) 的好动作。但如果智能体尝试 <code>A_left</code>，有 30% 的几率得到 <code>-5</code>。如果此时它也尝试了 <code>A_right</code> 并且碰巧得到了 <code>+100</code>，那么算法可能会错误地认为 <code>A_right</code> 比 <code>A_left</code> 好得多，尽管只是因为 <code>A_right</code> 后续的随机性导致了一个偶然的高回报。</p>
</li>
<li>
<p><strong>“坏的动作可能因为后续一系列好的随机事件而导致高回报”：</strong><br>
<code>A_right</code> 本身是一个期望回报为零的动作（不如 <code>A_left</code>）。但是，如果智能体尝试 <code>A_right</code>，有 50% 的概率得到 <code>+100</code> 的高回报。如果它恰好在少数几个样本中都经历了这种情况，算法会错误地强化这个“坏”动作。</p>
</li>
<li>
<p><strong>缓慢收敛：</strong> 为了克服这种高方差，算法需要收集大量的样本，才能让 <code>G_t</code> 的平均效果逐渐接近其真实期望。如果方差很大，就需要更多的样本来获得一个可靠的梯度估计，这导致收敛缓慢。智能体的策略可能会在好坏之间来回摆动很长时间。</p>
</li>
</ol>
<p><strong>Actor-Critic如何帮助：</strong></p>
<p>如果使用Actor-Critic：</p>
<ul>
<li>
<p>Critic会学习状态值函数 <code>V(s)</code>。</p>
<ul>
<li>理想情况下，<code>V(S_0)</code> 会学习到接近 <code>5.5</code>（如果最优策略是选 <code>A_left</code>）。</li>
<li><code>V(S_intermediate)</code> 会学习到接近 <code>0</code>。</li>
</ul>
</li>
<li>
<p>当计算优势 <code>A(s, a)</code> 时：</p>
<ul>
<li>对于 <code>A_left</code> 得到 <code>+10</code>：<code>A(S_0, A_left) = r + γV(end) - V(S_0) = 10 + 0 - V(S_0)</code>。如果 <code>V(S_0)</code> 接近 <code>5.5</code>，优势就是 <code>4.5</code>。</li>
<li>对于 <code>A_left</code> 得到 <code>-5</code>：<code>A(S_0, A_left) = -5 + 0 - V(S_0)</code>。如果 <code>V(S_0)</code> 接近 <code>5.5</code>，优势就是 <code>-10.5</code>。</li>
<li>对于 <code>A_right</code> 得到 <code>+100</code>：智能体先到达 <code>S_intermediate</code>（假设这里没有立即奖励），然后得到 <code>+100</code>。<br>
优势的计算会更像是 <code>δ_0 = r_0 + γV(S_intermediate) - V(S_0)</code>。如果 <code>r_0=0</code>（立即奖励为0），<code>δ_0 = γV(S_intermediate) - V(S_0)</code>。如果 <code>V(S_intermediate)</code> 接近0，<code>V(S_0)</code> 接近5.5，那么这个初始TD误差是负的。后续的TD误差 <code>δ_1 = 100 + γV(end) - V(S_intermediate) = 100 - V(S_intermediate)</code> 会是大的正值。GAE会将这些TD误差组合起来。<br>
更简单地看，如果Critic已经比较准确，<code>A(S_0, A_right) = Q(S_0, A_right) - V(S_0)</code>。<code>Q(S_0, A_right)</code> 的期望是0。如果 <code>V(S_0)</code> 是基于当前混合策略的平均值，那么优势值会更好地反映 <code>A_right</code> 相对于当前平均表现的好坏，而不是其绝对回报的剧烈波动。</li>
</ul>
</li>
</ul>
<p>通过引入基线 <code>V(S_0)</code>，优势函数 <code>A(S_0, a)</code> 的波动幅度会减小。例如，即使 <code>A_right</code> 得到了 <code>+100</code>，如果 <code>V(S_0)</code> 已经学到了一个比较高的值（比如因为当前策略有时会选 <code>A_left</code>），那么 <code>+100 - V(S_0)</code> 相比于直接使用 <code>+100</code> 作为权重，其绝对值可能更小，或者其相对其他动作的优势更合理。</p>
<p>这个例子清晰地展示了：</p>
<ul>
<li>蒙特卡洛回报 <code>G_t</code> 的值可以有很大的波动范围。</li>
<li>这种波动不是由动作本身的好坏唯一决定的，而是受到了后续大量随机事件的强烈影响。</li>
<li>使用这种高方差的回报来指导策略更新，会导致梯度估计非常嘈杂，使得学习不稳定且效率低下。</li>
</ul>
<p>而引入Critic和优势函数，就是为了从回报中剥离掉那些与当前状态价值相关但与当前动作选择无关的部分，从而得到一个更纯粹、更低方差的信号来指导Actor的学习。</p>
<br>
<h3 id="心得">心得</h3>
<p>控制状态空间大小，尽量选择有限状态空间</p>
<p>eg: 将state中的一些值int化</p>
<br>
<p>状态空间的表示值尽量接近</p>
<p>采用标准化技术将state内的值均一化</p>
<p>eg: [1, 200, 9999] -&gt; [0.001, 0.99, 0.9999]</p>
<br>
<p>更好的初始化</p>
<br>
<p>reward设置</p>
<p>惩罚和奖励设置插值不要过大, 防止惩罚拟合过快</p>
<p>+2, -200 -&gt; +2, 0</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/04/25/Algo-Learning-Record/" rel="prev" title="Algo Learning Record">
      <i class="fa fa-chevron-left"></i> Algo Learning Record
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/06/22/common-commands/" rel="next" title="common-commands">
      common-commands <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">reinforce learning record</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80"><span class="nav-text">数学基础</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B"><span class="nav-text">机器学习模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MRP"><span class="nav-text">MRP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E6%8A%A5"><span class="nav-text">回报</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">价值函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MDP"><span class="nav-text">MDP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5"><span class="nav-text">策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">状态价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">动作价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E4%B8%8E%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E5%85%B3%E7%B3%BB"><span class="nav-text">状态价值函数与动作价值函数关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5"><span class="nav-text">最优策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%9F%E6%9C%9B%E6%96%B9%E7%A8%8B"><span class="nav-text">贝尔曼期望方程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E6%96%B9%E7%A8%8B"><span class="nav-text">贝尔曼最优方程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95-%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E5%87%BD%E6%95%B0%E7%9A%84"><span class="nav-text">策略迭代算法&#x2F;基于策略函数的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95-%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84"><span class="nav-text">价值迭代算法&#x2F;基于价值函数的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3-%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E5%AF%B9%E6%AF%94"><span class="nav-text">策略迭代&amp;价值迭代对比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%89%E6%A8%A1%E5%9E%8B-%E6%97%A0%E6%A8%A1%E5%9E%8B-%E5%9C%A8%E7%BA%BF%E7%AD%96%E7%95%A5-%E7%A6%BB%E7%BA%BF%E7%AD%96%E7%95%A5"><span class="nav-text">有模型&amp;无模型+在线策略&amp;离线策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95"><span class="nav-text">算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95"><span class="nav-text">蒙特卡洛方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95"><span class="nav-text">时序差分算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sarsa%E7%AE%97%E6%B3%95"><span class="nav-text">Sarsa算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q-learning%E7%AE%97%E6%B3%95"><span class="nav-text">Q-learning算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DQN-DDQN"><span class="nav-text">DQN&#x2F;DDQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95-REINFORCE"><span class="nav-text">策略梯度算法(REINFORCE)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Actor-Critic"><span class="nav-text">Actor-Critic</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TRPO"><span class="nav-text">TRPO</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B"><span class="nav-text">整体流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%91%E4%BC%BC%E5%A4%84%E7%90%86"><span class="nav-text">近似处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%91%E4%BC%BC%E7%BA%A6%E6%9D%9F%E4%B8%8E%E6%B1%82%E8%A7%A3"><span class="nav-text">近似约束与求解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89%E4%BC%98%E5%8A%BF"><span class="nav-text">广义优势</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95-2"><span class="nav-text">算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PPO-clip"><span class="nav-text">PPO-clip</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DDPG"><span class="nav-text">DDPG</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E5%88%86%E5%BF%83%E5%BE%97"><span class="nav-text">部分心得</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AC%E5%88%86%E7%A6%BB"><span class="nav-text">AC分离</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BF%83%E5%BE%97"><span class="nav-text">心得</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="marigo1d"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">marigo1d</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/marigo1d" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;marigo1d" rel="noopener" target="_blank">GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">marigo1d</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">511k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">15:30</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
