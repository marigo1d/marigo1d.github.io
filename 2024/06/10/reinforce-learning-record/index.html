<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"marigo1d.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="reinforce learning">
<meta property="og:type" content="article">
<meta property="og:title" content="reinforce-learning-record">
<meta property="og:url" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/index.html">
<meta property="og:site_name" content="Marigold">
<meta property="og:description" content="reinforce learning">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/00e7457c373413b80db70c958566dd84.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610205131367.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610203108806.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610203423677.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610203434567.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610203509585.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610203841177.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610205112917.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610204522539.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610204014373.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610204152914.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213003398.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213050168.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612093758370.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612093948192.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612093958259.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612094048209.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610204627264.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/shiow.PNG">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610205402396.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213050168.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213003398.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213003398.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611150030644.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611191853492.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611152610430.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611152643864.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611153610316.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213050168.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611161858965.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611161157050.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611161942958.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611162513650.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611162624769.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611191221769.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612100452913.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611194333945.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612100509915.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322111503025.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322111831741.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612100629893.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240610213003398.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612100732404.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612102026769.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612102256431.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240611200059062.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612110026419.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612110241521.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612110951796.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612111022161.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20240612110750284.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250324145210312.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250324141848364.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322150349844.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322150502092.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322151444000.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322151537751.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322162045699.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250325154631583.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250322211023368.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250325200037093.png">
<meta property="og:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/image-20250323105541258.png">
<meta property="article:published_time" content="2024-06-10T12:29:05.000Z">
<meta property="article:modified_time" content="2025-03-28T02:25:54.588Z">
<meta property="article:author" content="marigo1d">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/00e7457c373413b80db70c958566dd84.png">

<link rel="canonical" href="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>reinforce-learning-record | Marigold</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Marigold</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Salt, Pepper and Birds~</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://marigo1d.github.io/2024/06/10/reinforce-learning-record/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="marigo1d">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Marigold">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          reinforce-learning-record
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-06-10 20:29:05" itemprop="dateCreated datePublished" datetime="2024-06-10T20:29:05+08:00">2024-06-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-03-28 10:25:54" itemprop="dateModified" datetime="2025-03-28T10:25:54+08:00">2025-03-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforce-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforce Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>14k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>26 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>reinforce learning</p>
<span id="more"></span>
<h1 id="reinforce-learning-record"><a href="#reinforce-learning-record" class="headerlink" title="reinforce learning record"></a>reinforce learning record</h1><h2 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h2><p>一阶优化与二阶优化</p>
<p>在优化问题中，<strong>一阶优化算法</strong>和<strong>二阶优化算法</strong>是两类重要的优化方法，它们主要区别在于利用了目标函数的哪些信息：</p>
<p><strong>一阶优化算法</strong></p>
<ul>
<li><strong>定义</strong>：一阶优化算法仅利用目标函数的一阶导数（梯度）信息进行优化。</li>
<li><strong>特点</strong>：<ol>
<li><strong>梯度信息</strong>：依赖梯度来指引优化方向。</li>
<li><strong>计算成本低</strong>：梯度的计算通常比二阶导数（Hessian矩阵）简单，因此更高效。</li>
<li><strong>适用性广</strong>：常用于高维问题或目标函数复杂的情况。</li>
</ol>
</li>
<li><strong>常见算法</strong>：<ul>
<li><strong>梯度下降法（Gradient Descent, GD）</strong>：$ x_{k+1} = x_k - \eta \nabla f(x_k)$ 其中，$\eta$ 是学习率，$\nabla f(x_k)$ 是目标函数在 $x_k$ 处的梯度。</li>
<li><strong>随机梯度下降（SGD）</strong>：在大规模数据下随机选择小批量样本计算梯度，适合机器学习训练。</li>
<li><strong>动量法（Momentum）</strong>、<strong>Adam优化器</strong>：改进了基础的梯度下降，具有更快的收敛速度。</li>
</ul>
</li>
</ul>
<p><strong>二阶优化算法</strong></p>
<ul>
<li><p><strong>定义</strong>：二阶优化算法利用目标函数的一阶导数（梯度）和二阶导数（Hessian矩阵）信息。</p>
</li>
<li><p><strong>特点</strong>：</p>
<ol>
<li><strong>曲率信息</strong>：Hessian矩阵包含关于目标函数曲率的信息，因此二阶优化算法能够更准确地找到极值点。</li>
<li><strong>更快收敛</strong>：在理想条件下，具有二次收敛速度。</li>
<li><strong>计算成本高</strong>：Hessian矩阵的计算和存储对于高维问题非常昂贵。</li>
</ol>
</li>
<li><p><strong>常见算法</strong>：</p>
<ul>
<li><strong>牛顿法（Newton’s Method）</strong>：使二阶泰勒展开式的导数为0的点，迭代至$\nabla f(x_k)$为极小，$x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)$ ，其中，$[\nabla^2 f(x_k)]^{-1}$ 是Hessian矩阵的逆。</li>
<li><strong>拟牛顿法（Quasi-Newton Methods）</strong>：通过近似Hessian矩阵降低计算复杂度，例如 BFGS 和 L-BFGS。</li>
<li><strong>信赖域方法（Trust Region Methods）</strong>：结合Hessian信息限制更新步长，适合复杂非线性优化。</li>
</ul>
</li>
</ul>
<h2 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h2><img src="/2024/06/10/reinforce-learning-record/00e7457c373413b80db70c958566dd84.png" class="" title="ma learning">
<h2 id="MRP"><a href="#MRP" class="headerlink" title="MRP"></a>MRP</h2><img src="/2024/06/10/reinforce-learning-record/image-20240610205131367.png" class="" title="image-20240610205131367">
<h3 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h3><p>在一个马尔可夫奖励过程中，从第t时刻状态 S_t 开始，直到终止状态时，所有奖励的衰减之和称为<strong>回报</strong>（Return），表示为 G_t</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610203108806.png" class="" title="image-20240610203108806">
<ul>
<li>R_t表示在时刻t获得的奖励</li>
<li><strong>到达状态S_i</strong>，得到奖励r(s)；若在时刻t到达状态S_i，则<strong>R_t=r(S_i)</strong></li>
<li>对于某状态序列<img src="/2024/06/10/reinforce-learning-record/image-20240610203423677.png" class="" title="image-20240610203423677">我们有对应于该状态序列的奖励G<img src="/2024/06/10/reinforce-learning-record/image-20240610203434567.png" class="" title="image-20240610203434567"></li>
</ul>
<h3 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h3><p>在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的<strong>价值</strong>（value）。所有状态的价值就组成了<strong>价值函数</strong>（value function），价值函数的输入为某个状态，输出为这个状态的价值。</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610203509585.png" class="" title="image-20240610203509585">
<ul>
<li>对于某个状态s，从该状态出发可得到的状态序列有很多条</li>
<li>对于某个状态s，价值函数为<strong>从该状态出发，可能存在的所有状态序列的奖励的均值</strong></li>
</ul>
<p>贝尔曼方程</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610203841177.png" class="" title="image-20240610203841177">
<h2 id="MDP"><a href="#MDP" class="headerlink" title="MDP"></a>MDP</h2><p>与MRP相比，奖励与状态和动作相关</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610205112917.png" class="" title="image-20240610205112917">
<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>策略表示在状态s下采取动作a的<strong>概率</strong>，为<strong>概率</strong>，可能为1</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610204522539.png" class="" title="image-20240610204522539">
<h3 id="状态价值函数"><a href="#状态价值函数" class="headerlink" title="状态价值函数"></a>状态价值函数</h3><p>在 MDP 中基于策略Π的状态价值函数（state-value function），定义为从状态s出发遵循策略Π能获得的期望回报</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610204014373.png" class="" title="image-20240610204014373">
<blockquote>
<p>价值是回报的期望</p>
</blockquote>
<h3 id="动作价值函数"><a href="#动作价值函数" class="headerlink" title="动作价值函数"></a>动作价值函数</h3><p>在 MDP 中，由于动作的存在，我们额外定义一个<strong>动作价值函数</strong>（action-value function）。表示在 MDP 遵循策略Π时，对当前状态s执行动作a得到的期望回报</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610204152914.png" class="" title="image-20240610204152914">
<ul>
<li><strong>采取动作a到达状态s</strong>，得到奖励R</li>
</ul>
<h3 id="状态价值函数与动作价值函数关系"><a href="#状态价值函数与动作价值函数关系" class="headerlink" title="状态价值函数与动作价值函数关系"></a>状态价值函数与动作价值函数关系</h3><img src="/2024/06/10/reinforce-learning-record/image-20240610213003398.png" class="" title="image-20240610213003398">
<ul>
<li>从状态s出发，可能采取动作a1, a2, a3, …;对于每个可能采取的动作，均有动作价值Q，则状态s的状态价值为所有动作价值的期望</li>
</ul>
<img src="/2024/06/10/reinforce-learning-record/image-20240610213050168.png" class="" title="image-20240610213050168">
<ul>
<li>在状态s执行动作a后，得到奖励r(s,a)；由于可能到达多个不同的状态s’，动作价值为即时奖励r(s,a)+可能到达的所有状态的价值的期望</li>
</ul>
<h3 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h3><p>策略之间的偏序关系</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612093758370.png" class="" title="image-20240612093758370">
<p>最优策略为 对于任意的策略，均有最优策略优于或不差于其他策略</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612093948192.png" class="" title="image-20240612093948192">
<img src="/2024/06/10/reinforce-learning-record/image-20240612093958259.png" class="" title="image-20240612093958259">
<p>最优状态价值与最优动作价值的关系</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612094048209.png" class="" title="image-20240612094048209">
<h3 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h3><p>采取策略Π</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610204627264.png" class="" title="image-20240610204627264">
<ul>
<li>对于状态s，s的状态价值为 ( 折损的 <strong>所有可达的下一状态的价值的期望</strong> + <strong>到达所有可达的下一状态的r(s’,a)的期望</strong> )</li>
<li>对于状态s和动作a，在状态s进行动作a的价值为 ( <strong>在状态s执行动作a的r(s,a)的期望</strong>(实际上就是r(s,a)) + 折损的 <strong>在执行动作a后所有可达的下一状态的所有可执行动作的价值的期望</strong> )</li>
</ul>
<img src="/2024/06/10/reinforce-learning-record/shiow.PNG" class="" title="shiow">
<p>图中情况为每个动作只会到达一个状态&amp;应该直接将动作视为一个节点，图为树</p>
<h3 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h3><p>存在策略*，使对于任意的状态s，均有基于策略*状态价值函数大于基于策略Π的状态价值函数</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610205402396.png" class="" title="image-20240610205402396">
<ul>
<li>对于状态价值：非最优形式是求期望，最优形式是直接选取当前状态，所有动作中，未来状态序列中价值最大的</li>
<li>对于动作价值：非最优形式是求期望，最优形式是直接选取下一状态，所有动作中，未来状态序列中动作价值最大的</li>
</ul>
<h2 id="策略迭代算法-基于策略函数的"><a href="#策略迭代算法-基于策略函数的" class="headerlink" title="策略迭代算法/基于策略函数的"></a>策略迭代算法/基于策略函数的</h2><blockquote>
<p>策略迭代的核心思想是<strong>交替进行策略评估和策略提升</strong>，直到策略收敛到最优。</p>
</blockquote>
<p><strong>策略评估：</strong></p>
<p>基于当前策略Π，在已知状态转移函数的情况下（即我们可以知道采取动作a后有多高的几率到达哪个状态），使用<strong>贝尔曼期望方程</strong>迭代更新状态价值函数</p>
<p>ps: 原文中迭代更新状态价值函数是通过</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610213050168.png" class="" title="image-20240610213050168">
<img src="/2024/06/10/reinforce-learning-record/image-20240610213003398.png" class="" title="image-20240610213003398">
<p>以上两个公式实现的，也即</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">qsa_list = []</span><br><span class="line">	<span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 迷宫环境，四相运动</span></span><br><span class="line">		qsa = <span class="number">0</span>  <span class="comment"># 当前动作的动作价值</span></span><br><span class="line">        <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:  <span class="comment"># 有模型，即状态转移函数已知</span></span><br><span class="line">        	p, next_state, r, done = res</span><br><span class="line">            qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))  <span class="comment"># 第一个公式，计算该动作的价值(开头的p为动作成功概率)</span></span><br><span class="line">         qsa_list.append(self.pi[s][a] * qsa)  <span class="comment"># 乘采取该动作的几率</span></span><br><span class="line">	new_v[s] = <span class="built_in">sum</span>(qsa_list)  <span class="comment"># 第二个公式，根据所有动作的价值计算状态价值</span></span><br></pre></td></tr></table></figure>
<p><strong>策略提升：</strong></p>
<p>由于</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610213003398.png" class="" title="image-20240610213003398">
<p>故修改策略，使得动作价值最大的动作以更高的概率(若最大值唯一，该动作将成为唯一被选取动作；若不唯一，这些动作将均分概率1)被选取，可使新策略下的状态价值函数增大，也即<img src="/2024/06/10/reinforce-learning-record/image-20240611150030644.png" class="" title="image-20240611150030644"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">qsa_list = []</span><br><span class="line">	<span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 迷宫环境，四相运动</span></span><br><span class="line">    	qsa = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:</span><br><span class="line">        	p, next_state, r, done = res</span><br><span class="line">            qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))</span><br><span class="line">        qsa_list.append(qsa)</span><br><span class="line">     maxq = <span class="built_in">max</span>(qsa_list)</span><br><span class="line">     cntq = qsa_list.count(maxq)  <span class="comment"># 计算有几个动作得到了最大的Q值</span></span><br><span class="line">     <span class="comment"># 让这些动作均分概率</span></span><br><span class="line">     self.pi[s] = [<span class="number">1</span> / cntq <span class="keyword">if</span> q == maxq <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> q <span class="keyword">in</span> qsa_list]</span><br></pre></td></tr></table></figure>
<p>target: 修改<strong>策略</strong>，使在新策略下，有基于新策略的状态价值函数大于原策略的状态价值函数</p>
<p>完整策略迭代算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PolicyIteration</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 策略迭代算法 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, env, theta, gamma</span>):</span><br><span class="line">        <span class="comment"># 初始化函数，接受环境对象env、策略评估收敛阈值theta和折扣因子gamma</span></span><br><span class="line">        self.env = env  <span class="comment"># 保存环境对象</span></span><br><span class="line">        self.v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow  <span class="comment"># 初始化价值函数v为0，大小为环境的格子数</span></span><br><span class="line">        self.pi = [[<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>]</span><br><span class="line">                   <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow)]  <span class="comment"># 初始化策略pi为均匀随机策略，每个状态有4个动作，每个动作的概率为0.25</span></span><br><span class="line">        self.theta = theta  <span class="comment"># 策略评估的收敛阈值</span></span><br><span class="line">        self.gamma = gamma  <span class="comment"># 折扣因子，用于计算未来奖励的折扣</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">policy_evaluation</span>(<span class="params">self</span>):  <span class="comment"># 策略评估</span></span><br><span class="line">        cnt = <span class="number">1</span>  <span class="comment"># 计数器，记录策略评估的迭代次数</span></span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            max_diff = <span class="number">0</span>  <span class="comment"># 用于记录价值函数的最大变化量</span></span><br><span class="line">            new_v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow  <span class="comment"># 初始化新的价值函数</span></span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow):  <span class="comment"># 当次地图扫描</span></span><br><span class="line">                qsa_list = []  <span class="comment"># 用于存储状态s下所有动作的Q值</span></span><br><span class="line">                <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 遍历所有动作</span></span><br><span class="line">                    qsa = <span class="number">0</span>  <span class="comment"># 初始化Q(s,a)为0</span></span><br><span class="line">                    <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:  <span class="comment"># 遍历状态s下执行动作a的所有可能结果</span></span><br><span class="line">                        p, next_state, r, done = res  <span class="comment"># 获取转移概率p、下一个状态next_state、奖励r和是否终止done</span></span><br><span class="line">                        qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))  <span class="comment"># 计算Q(s,a)的值，考虑折扣因子gamma</span></span><br><span class="line">                    qsa_list.append(self.pi[s][a] * qsa)  <span class="comment"># 将Q(s,a)乘以策略pi(s,a)并加入列表</span></span><br><span class="line">                new_v[s] = <span class="built_in">sum</span>(qsa_list)  <span class="comment"># 计算状态s的新价值函数值</span></span><br><span class="line">                max_diff = <span class="built_in">max</span>(max_diff, <span class="built_in">abs</span>(new_v[s] - self.v[s]))  <span class="comment"># 更新当次地图扫描中最大变化量</span></span><br><span class="line">            self.v = new_v  <span class="comment"># 更新价值函数</span></span><br><span class="line">            <span class="keyword">if</span> max_diff &lt; self.theta: <span class="keyword">break</span>  <span class="comment"># 如果最大变化量小于阈值theta，则退出循环</span></span><br><span class="line">            cnt += <span class="number">1</span>  <span class="comment"># 增加迭代次数</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;策略评估进行%d轮后完成&quot;</span> % cnt)  <span class="comment"># 打印策略评估的迭代次数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">policy_improvement</span>(<span class="params">self</span>):  <span class="comment"># 策略提升</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.nrow * self.env.ncol):  <span class="comment"># 遍历所有状态</span></span><br><span class="line">            qsa_list = []  <span class="comment"># 用于存储状态s下所有动作的Q值</span></span><br><span class="line">            <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 遍历所有动作</span></span><br><span class="line">                qsa = <span class="number">0</span>  <span class="comment"># 初始化Q(s,a)为0</span></span><br><span class="line">                <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:  <span class="comment"># 遍历状态s下执行动作a的所有可能结果</span></span><br><span class="line">                    p, next_state, r, done = res  <span class="comment"># 获取转移概率p、下一个状态next_state、奖励r和是否终止done</span></span><br><span class="line">                    qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))  <span class="comment"># 计算Q(s,a)的值，考虑折扣因子gamma</span></span><br><span class="line">                qsa_list.append(qsa)  <span class="comment"># 将Q(s,a)加入列表</span></span><br><span class="line">            maxq = <span class="built_in">max</span>(qsa_list)  <span class="comment"># 找到状态s下所有动作的最大Q值</span></span><br><span class="line">            cntq = qsa_list.count(maxq)  <span class="comment"># 计算有多少个动作得到了最大Q值</span></span><br><span class="line">            <span class="comment"># 让这些动作均分概率，其他动作的概率为0</span></span><br><span class="line">            self.pi[s] = [<span class="number">1</span> / cntq <span class="keyword">if</span> q == maxq <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> q <span class="keyword">in</span> qsa_list]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;策略提升完成&quot;</span>)  <span class="comment"># 打印策略提升完成</span></span><br><span class="line">        <span class="keyword">return</span> self.pi  <span class="comment"># 返回更新后的策略</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">policy_iteration</span>(<span class="params">self</span>):  <span class="comment"># 策略迭代</span></span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            self.policy_evaluation()  <span class="comment"># 进行策略评估</span></span><br><span class="line">            old_pi = copy.deepcopy(self.pi)  <span class="comment"># 深拷贝当前策略，用于比较</span></span><br><span class="line">            new_pi = self.policy_improvement()  <span class="comment"># 进行策略提升</span></span><br><span class="line">            <span class="keyword">if</span> old_pi == new_pi: <span class="keyword">break</span>  <span class="comment"># 如果策略不再变化，则退出循环</span></span><br><span class="line">                </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">print_agent</span>(<span class="params">agent, action_meaning, disaster=[], end=[]</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;状态价值：&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(agent.env.nrow):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(agent.env.ncol):</span><br><span class="line">            <span class="comment"># 为了输出美观,保持输出6个字符</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;%6.6s&#x27;</span> % (<span class="string">&#x27;%.3f&#x27;</span> % agent.v[i * agent.env.ncol + j]), end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;策略：&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(agent.env.nrow):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(agent.env.ncol):</span><br><span class="line">            <span class="comment"># 一些特殊的状态,例如悬崖漫步中的悬崖</span></span><br><span class="line">            <span class="keyword">if</span> (i * agent.env.ncol + j) <span class="keyword">in</span> disaster:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;****&#x27;</span>, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">            <span class="keyword">elif</span> (i * agent.env.ncol + j) <span class="keyword">in</span> end:  <span class="comment"># 目标状态</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;EEEE&#x27;</span>, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                a = agent.pi[i * agent.env.ncol + j]</span><br><span class="line">                pi_str = <span class="string">&#x27;&#x27;</span></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(action_meaning)):</span><br><span class="line">                    pi_str += action_meaning[k] <span class="keyword">if</span> a[k] &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;o&#x27;</span></span><br><span class="line">                <span class="built_in">print</span>(pi_str, end=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">env = CliffWalkingEnv()</span><br><span class="line">action_meaning = [<span class="string">&#x27;^&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;&lt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>]</span><br><span class="line">theta = <span class="number">0.001</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">agent = PolicyIteration(env, theta, gamma)</span><br><span class="line">agent.policy_iteration()</span><br><span class="line">print_agent(agent, action_meaning, <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">37</span>, <span class="number">47</span>)), [<span class="number">47</span>])</span><br></pre></td></tr></table></figure>
<h2 id="价值迭代算法-基于价值函数的"><a href="#价值迭代算法-基于价值函数的" class="headerlink" title="价值迭代算法/基于价值函数的"></a>价值迭代算法/基于价值函数的</h2><blockquote>
<p>价值迭代的核心思想是<strong>直接通过最大化价值函数来找到最优策略</strong>，不需要显式地维护策略。</p>
</blockquote>
<p>使用贝尔曼最优方程</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611191853492.png" class="" title="image-20240611191853492">
<p>的迭代形式</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611152610430.png" class="" title="image-20240611152610430">
<p>进行价值迭代；</p>
<p>使用</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611152643864.png" class="" title="image-20240611152643864">
<p>从迭代完成的状态价值函数中获取策略，即 从当前状态出发，哪个动作的 (即时奖励+下一状态价值) 最大，策略就为哪个动作</p>
<p>最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611153610316.png" class="" title="image-20240611153610316">
<img src="/2024/06/10/reinforce-learning-record/image-20240610213050168.png" class="" title="image-20240610213050168">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">max_diff = <span class="number">0</span></span><br><span class="line">new_v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow  <span class="comment"># 迷宫环境，初始化状态价值</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow):</span><br><span class="line">    qsa_list = []</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        qsa = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:</span><br><span class="line">            p, next_state, r, done = res</span><br><span class="line">            qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))</span><br><span class="line">        qsa_list.append(qsa)  <span class="comment"># 记录状态s下的所有Q(s,a)价值</span></span><br><span class="line">     new_v[s] = <span class="built_in">max</span>(qsa_list)  <span class="comment"># 取最大作为新状态价值</span></span><br><span class="line">     max_diff = <span class="built_in">max</span>(max_diff, <span class="built_in">abs</span>(new_v[s] - self.v[s]))</span><br><span class="line">self.v = new_v</span><br><span class="line"><span class="keyword">if</span> max_diff &lt; self.theta: <span class="keyword">break</span>  <span class="comment"># 满足收敛条件,退出评估迭代</span></span><br><span class="line">cnt += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>完整价值迭代算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ValueIteration</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 价值迭代算法 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, env, theta, gamma</span>):</span><br><span class="line">        <span class="comment"># 初始化函数，接受环境对象env、价值收敛阈值theta和折扣因子gamma</span></span><br><span class="line">        self.env = env  <span class="comment"># 保存环境对象</span></span><br><span class="line">        self.v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow  <span class="comment"># 初始化价值函数v为0，大小为环境的格子数</span></span><br><span class="line">        self.theta = theta  <span class="comment"># 价值收敛阈值</span></span><br><span class="line">        self.gamma = gamma  <span class="comment"># 折扣因子，用于计算未来奖励的折扣</span></span><br><span class="line">        <span class="comment"># 初始化策略pi为None，表示在价值迭代结束后再计算策略</span></span><br><span class="line">        self.pi = [<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">value_iteration</span>(<span class="params">self</span>):</span><br><span class="line">        cnt = <span class="number">0</span>  <span class="comment"># 计数器，记录价值迭代的轮数</span></span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">            max_diff = <span class="number">0</span>  <span class="comment"># 用于记录价值函数的最大变化量</span></span><br><span class="line">            new_v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow  <span class="comment"># 初始化新的价值函数</span></span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow):  <span class="comment"># 遍历所有状态</span></span><br><span class="line">                qsa_list = []  <span class="comment"># 用于存储状态s下所有动作的Q值</span></span><br><span class="line">                <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 遍历所有动作</span></span><br><span class="line">                    qsa = <span class="number">0</span>  <span class="comment"># 初始化Q(s,a)为0</span></span><br><span class="line">                    <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:  <span class="comment"># 遍历状态s下执行动作a的所有可能结果</span></span><br><span class="line">                        p, next_state, r, done = res  <span class="comment"># 获取转移概率p、下一个状态next_state、奖励r和是否终止done</span></span><br><span class="line">                        qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))  <span class="comment"># 计算Q(s,a)的值，考虑折扣因子gamma</span></span><br><span class="line">                    qsa_list.append(qsa)  <span class="comment"># 将Q(s,a)加入列表</span></span><br><span class="line">                new_v[s] = <span class="built_in">max</span>(qsa_list)  <span class="comment"># 更新状态s的价值函数为所有动作Q值的最大值</span></span><br><span class="line">                max_diff = <span class="built_in">max</span>(max_diff, <span class="built_in">abs</span>(new_v[s] - self.v[s]))  <span class="comment"># 更新最大变化量</span></span><br><span class="line">            self.v = new_v  <span class="comment"># 更新价值函数</span></span><br><span class="line">            <span class="keyword">if</span> max_diff &lt; self.theta: <span class="keyword">break</span>  <span class="comment"># 如果最大变化量小于阈值theta，则退出循环</span></span><br><span class="line">            cnt += <span class="number">1</span>  <span class="comment"># 增加迭代次数</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;价值迭代一共进行%d轮&quot;</span> % cnt)  <span class="comment"># 打印价值迭代的轮数</span></span><br><span class="line">        self.get_policy()  <span class="comment"># 根据最终的价值函数导出策略</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_policy</span>(<span class="params">self</span>):  <span class="comment"># 根据价值函数导出一个贪婪策略</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.nrow * self.env.ncol):  <span class="comment"># 遍历所有状态</span></span><br><span class="line">            qsa_list = []  <span class="comment"># 用于存储状态s下所有动作的Q值</span></span><br><span class="line">            <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 遍历所有动作</span></span><br><span class="line">                qsa = <span class="number">0</span>  <span class="comment"># 初始化Q(s,a)为0</span></span><br><span class="line">                <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:  <span class="comment"># 遍历状态s下执行动作a的所有可能结果</span></span><br><span class="line">                    p, next_state, r, done = res  <span class="comment"># 获取转移概率p、下一个状态next_state、奖励r和是否终止done</span></span><br><span class="line">                    qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))  <span class="comment"># 计算Q(s,a)的值，考虑折扣因子gamma</span></span><br><span class="line">                qsa_list.append(qsa)  <span class="comment"># 将Q(s,a)加入列表</span></span><br><span class="line">            maxq = <span class="built_in">max</span>(qsa_list)  <span class="comment"># 找到状态s下所有动作的最大Q值</span></span><br><span class="line">            cntq = qsa_list.count(maxq)  <span class="comment"># 计算有多少个动作得到了最大Q值</span></span><br><span class="line">            <span class="comment"># 让这些动作均分概率，其他动作的概率为0</span></span><br><span class="line">            self.pi[s] = [<span class="number">1</span> / cntq <span class="keyword">if</span> q == maxq <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> q <span class="keyword">in</span> qsa_list]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 环境初始化</span></span><br><span class="line">env = CliffWalkingEnv()  <span class="comment"># 创建一个悬崖漫步环境</span></span><br><span class="line">action_meaning = [<span class="string">&#x27;^&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;&lt;&#x27;</span>, <span class="string">&#x27;&gt;&#x27;</span>]  <span class="comment"># 动作的含义，分别表示上、下、左、右</span></span><br><span class="line">theta = <span class="number">0.001</span>  <span class="comment"># 价值收敛阈值</span></span><br><span class="line">gamma = <span class="number">0.9</span>  <span class="comment"># 折扣因子</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建价值迭代算法对象</span></span><br><span class="line">agent = ValueIteration(env, theta, gamma)</span><br><span class="line">agent.value_iteration()  <span class="comment"># 执行价值迭代</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印策略和路径</span></span><br><span class="line">print_agent(agent, action_meaning, <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">37</span>, <span class="number">47</span>)), [<span class="number">47</span>])</span><br></pre></td></tr></table></figure>
<h2 id="策略迭代-amp-价值迭代对比"><a href="#策略迭代-amp-价值迭代对比" class="headerlink" title="策略迭代&amp;价值迭代对比"></a>策略迭代&amp;价值迭代对比</h2><ul>
<li>策略迭代的核心思想是<strong>交替进行策略评估和策略提升</strong>，直到策略收敛到最优。</li>
<li>价值迭代的核心思想是<strong>直接通过最大化价值函数来找到最优策略</strong>，不需要显式地维护策略。</li>
</ul>
<p>因此，价值迭代函数 <code>value_iteration</code> 和策略评估函数 <code>policy_evaluation</code> 有着一样的实现</p>
<p><strong>动作价值更新</strong>和<strong>状态价值更新</strong>是价值函数更新的两种方式，可以用于<strong>策略迭代</strong>、<strong>价值迭代</strong>或其他算法。</p>
<ul>
<li>状态价值迭代和动作价值迭代是等价的，只是更新的对象不同。</li>
<li>在价值迭代中，通常使用状态价值迭代，因为它的计算量较小。</li>
</ul>
<h2 id="有模型-amp-无模型-在线策略-amp-离线策略"><a href="#有模型-amp-无模型-在线策略-amp-离线策略" class="headerlink" title="有模型&amp;无模型+在线策略&amp;离线策略"></a>有模型&amp;无模型+在线策略&amp;离线策略</h2><p><strong>有模型强化学习：</strong></p>
<p>智能体学习环境的状态转移函数</p>
<p>环境的状态转移函数已知</p>
<p>智能体可以直接根据状态转移函数得到在对环境进行动作a后环境的下一状态</p>
<p># 状态转移函数P\[state][action] = [(p, next_state, reward, done)]包含转移成功概率，下一个状态，奖励和是否完成</p>
<ul>
<li>Dyna-Q</li>
<li>Monte Carlo Tree Search (MCTS)</li>
<li>PILCO (Probabilistic Inference for Learning Control)</li>
</ul>
<p><strong>无模型强化学习：</strong></p>
<p>智能体通过与环境交互学习状态和奖励之间的映射关系</p>
<p>环境的状态转移函数未知</p>
<p>智能体必须通过与环境的交互才能得到环境的下一状态</p>
<ul>
<li>Q-learning</li>
<li>SARSA</li>
<li>Deep Q-Network (DQN)</li>
<li>Policy Gradient methods (e.g., REINFORCE, A2C, PPO)</li>
</ul>
<p>样本：当前状态，下一状态，采取动作，奖励</p>
<p><strong>在线策略学习：</strong></p>
<p>不保存样本</p>
<p><strong>离线策略学习：</strong></p>
<p>使用经验回放池保存样本</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h3><p>Value-based + online</p>
<p>从某状态s出发，基于策略Π，获得一条状态序列，该状态序列对应一个回报G；该过程为一次采样。</p>
<p>反复采样，得到N个状态序列+回报和M，由大数定律，可得该状态的状态价值</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611161858965.png" class="" title="image-20240611161858965">
<p>使用增量更新方法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611161157050.png" class="" title="image-20240611161157050">
<p>补：增量更新原理</p>
<p>新均值 = 旧均值 + 1/总量 * (新值 - 旧均值) </p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611161942958.png" class="" title="image-20240611161942958">
<h3 id="时序差分算法"><a href="#时序差分算法" class="headerlink" title="时序差分算法"></a>时序差分算法</h3><p>Value-based + online</p>
<p>类似于蒙特卡洛，更新状态s的状态价值时，不使用完整的状态序列，在得到下一状态时立即对状态s的状态价值进行更新</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611162513650.png" class="" title="image-20240611162513650">
<p>蒙特卡洛使用第三行对状态价值进行更新，时序差分使用第四行对状态价值进行更新</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611162624769.png" class="" title="image-20240611162624769">
<ul>
<li>只向前走了一步的蒙特卡洛</li>
<li>使用下一状态的状态价值代替了很长的状态序列的回报(状态价值本身就是所有未来时刻的回报期望和)</li>
<li>增量更新体现在 减号前部分为 (新 状态s的状态价值)，减号后部分为 (旧 状态s的状态价值)</li>
</ul>
<h3 id="Sarsa算法"><a href="#Sarsa算法" class="headerlink" title="Sarsa算法"></a>Sarsa算法</h3><p>Value-based + online</p>
<p>类似于时序差分算法，对动作价值进行更新，目标是估计 <strong>ε-greedy策略的动作价值函数</strong></p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611191221769.png" class="" title="image-20240611191221769">
<p>算法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612100452913.png" class="" title="image-20240612100452913">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]</span><br><span class="line">self.Q_table[s0, a0] += self.alpha * td_error</span><br></pre></td></tr></table></figure>
<h3 id="Q-learning算法"><a href="#Q-learning算法" class="headerlink" title="Q-learning算法"></a>Q-learning算法</h3><p>Value-based + offline</p>
<p>对动作价值进行更新，目标是估计 <strong>最优策略的动作价值函数</strong></p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611194333945.png" class="" title="image-20240611194333945">
<p>算法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612100509915.png" class="" title="image-20240612100509915">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">td_error = r + self.gamma * self.Q_table[s1].<span class="built_in">max</span>() - self.Q_table[s0, a0]</span><br><span class="line">self.Q_table[s0, a0] += self.alpha * td_error</span><br></pre></td></tr></table></figure>
<blockquote>
<p>对比Sarsa和Q-learning</p>
<p>实际上，$r_t$和$R_t$在各自的算法中都表示在时间步$t$时环境反馈的即时奖励，但在更新公式中的使用方式略有不同。Sarsa算法使用下一个动作$a_{t+1}$的动作价值$Q(s_{t+1}, a_{t+1})$，而Q-learning算法使用下一状态$s_{t+1}$下最优动作的动作价值$\max_{a’} Q(s’, a’)$。</p>
<p>Sarsa 是<strong>在线策略</strong>（on-policy）算法， Q-learning 是<strong>离线策略</strong>（off-policy）算法</p>
</blockquote>
<h3 id="DQN-DDQN"><a href="#DQN-DDQN" class="headerlink" title="DQN/DDQN"></a>DQN/DDQN</h3><p>Value-based + offline</p>
<p><strong>DQN</strong></p>
<p>将Q-learning的Q表换成net</p>
<p>网络的损失函数</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250322111503025.png" class="" title="image-20250322111503025">
<blockquote>
<p>DQN网络旨在让Q(s,a)更新到逼近最优动作的动作价值$r + \max_{a’} Q(s’, a’)$</p>
<p>单网络存在问题：</p>
<p>在 DQN 中，损失函数TD本身依赖于神经网络的输出 <em>Q</em>(<em>s</em>′,<em>a</em>′)。这意味着：</p>
<ul>
<li>当我们更新神经网络的参数时，<em>Q</em>(<em>s</em>′,<em>a</em>′) 也会发生变化。</li>
<li>这种变化会导致 TD 目标不断改变，从而使得训练过程变得不稳定。</li>
</ul>
</blockquote>
<p>训练网络每步更新，目标网络每隔C步进行一次更新；</p>
<p>使用目标网络计算最优动作价值（最优动作选取使用目标网络），最小化损失函数更新训练网络；</p>
<p><strong>Double DQN</strong></p>
<p>DQN在<strong>选择和评估动作使用的是同一套神经网络</strong>，如果神经网络在估算Q值时存在误差（尤其是正向误差），这种误差会在选择和评估动作的过程中被放大。具体来说：</p>
<ul>
<li>当神经网络高估某个动作的Q值时，该动作更可能被选为最优动作。</li>
<li>被选中的动作的Q值又被用于计算目标Q值，进一步放大了高估的影响。</li>
<li>这种正向误差会在更新过程中逐步累积，导致Q值严重偏离真实值。</li>
</ul>
<p>Double DQN通过两套独立网络分别选择和评估动作，有效降低了过估计问题，使目标Q值更接近真实值。</p>
<p>这种改进在动作空间较大的任务中尤为重要，能够显著提升算法的稳定性和性能。</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250322111831741.png" class="" title="image-20250322111831741">
<h3 id="策略梯度算法-REINFORCE"><a href="#策略梯度算法-REINFORCE" class="headerlink" title="策略梯度算法(REINFORCE)"></a>策略梯度算法(REINFORCE)</h3><p>Policy-based + online</p>
<p>定义策略学习的目标函数为，s0为初始状态</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612100629893.png" class="" title="image-20240612100629893">
<p>目标是修改参数θ，使J(θ)取最大；即对θ求导</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610213003398.png" class="" title="image-20240610213003398">
<img src="/2024/06/10/reinforce-learning-record/image-20240612100732404.png" class="" title="image-20240612100732404">
<p>where 第一行第一个求和符号后的对象为 策略的<strong>状态访问分布</strong></p>
<blockquote>
<p>下图中的 (1-γ) 为归一化参数，原因为几何级数的求和公式为这个</p>
</blockquote>
<img src="/2024/06/10/reinforce-learning-record/image-20240612102026769.png" class="" title="image-20240612102026769">
<p>策略的状态访问分布表示该策略和在环境中会访问到的状态的分布情况(即到达每个状态的概率)</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612102256431.png" class="" title="image-20240612102256431">
<p>故该求导表示 <strong>对于每个状态，我们有一定的概率在该状态，从该状态出发，策略决定了我们能获得的价值的期望</strong></p>
<p>目标：</p>
<p>​    最大化函数J(θ)</p>
<p>算法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611200059062.png" class="" title="image-20240611200059062">
<p>时刻t向后的回报即为(动作)价值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">reward_list = transition_dict[<span class="string">&#x27;rewards&#x27;</span>]</span><br><span class="line">state_list = transition_dict[<span class="string">&#x27;states&#x27;</span>]</span><br><span class="line">action_list = transition_dict[<span class="string">&#x27;actions&#x27;</span>]</span><br><span class="line"></span><br><span class="line">G = <span class="number">0</span></span><br><span class="line">self.optimizer.zero_grad()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(reward_list))):  <span class="comment"># 从最后一步算起</span></span><br><span class="line">    reward = reward_list[i]  <span class="comment"># 取时间步i获得的奖励</span></span><br><span class="line">    state = torch.tensor([state_list[i]],</span><br><span class="line">                         dtype=torch.<span class="built_in">float</span>).to(self.device)  <span class="comment"># 取时间步i的状态</span></span><br><span class="line">    action = torch.tensor([action_list[i]]).view(-<span class="number">1</span>, <span class="number">1</span>).to(self.device)  <span class="comment"># 取时间步i的动作</span></span><br><span class="line">    log_prob = torch.log(self.policy_net(state).gather(<span class="number">1</span>, action))</span><br><span class="line">    <span class="comment"># self.policy_net(state)： 将状态输入策略网络，得到所有可能动作的概率分布。</span></span><br><span class="line">	<span class="comment"># .gather(1, action): 从概率分布中选取实际执行动作对应的概率值。</span></span><br><span class="line">	<span class="comment"># torch.log(...): 对选取的概率值取对数，得到对数概率</span></span><br><span class="line">    G = self.gamma * G + reward  <span class="comment"># G累计了从当前时间步i到episode结束的所有奖励</span></span><br><span class="line">    loss = -log_prob * G  <span class="comment"># loss对应公式中符号α后的部分</span></span><br><span class="line">    loss.backward()  <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">self.optimizer.step()  <span class="comment"># 梯度下降，实际上是对策略进行求导</span></span><br></pre></td></tr></table></figure>
<h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h3><p>(Value + Policy)-based + offline</p>
<p><strong>Actor采用策略迭代算法更新，Critic采用价值(状态价值)迭代算法更新</strong>，Actor的更新依赖于于Critic计算的优势函数；</p>
<p>策略函数梯度</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612110026419.png" class="" title="image-20240612110026419">
<p>其中，ψ_t可为不同值，表示不同的方法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612110241521.png" class="" title="image-20240612110241521">
<blockquote>
<p>注意，理论上来说，在收敛模型上上述6种方法得到的值是一致的，这六种方法都是基于策略梯度定理（Policy Gradient Theorem）的不同实现形式</p>
</blockquote>
<p>价值函数的损失函数与梯度</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612110951796.png" class="" title="image-20240612110951796">
<img src="/2024/06/10/reinforce-learning-record/image-20240612111022161.png" class="" title="image-20240612111022161">
<p>Actor基于策略函数，使用上图中的第6种的 <code>td_delta</code> 作为动作价值，使用Critic的价值函数得到动作价值的值；</p>
<p>Critic基于价值函数，用于最小化当前状态的价值估计与时序差分目标之间的差异，反向传播时对 <code>td_delta</code> 做了 <code>.detach()</code></p>
<p>基于<strong>时序差分(TD)</strong>方法的<strong>Actor-Critic</strong>算法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240612110750284.png" class="" title="image-20240612110750284">
<p>价值网络使用时序差分(TD)更新，策略网络使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">states = torch.tensor(transition_dict[<span class="string">&#x27;states&#x27;</span>],</span><br><span class="line">                              dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line">actions = torch.tensor(transition_dict[<span class="string">&#x27;actions&#x27;</span>]).view(-<span class="number">1</span>, <span class="number">1</span>).to(</span><br><span class="line">            self.device)</span><br><span class="line">rewards = torch.tensor(transition_dict[<span class="string">&#x27;rewards&#x27;</span>],</span><br><span class="line">                               dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>).to(self.device)</span><br><span class="line">next_states = torch.tensor(transition_dict[<span class="string">&#x27;next_states&#x27;</span>],</span><br><span class="line">                                   dtype=torch.<span class="built_in">float</span>).to(self.device)</span><br><span class="line">dones = torch.tensor(transition_dict[<span class="string">&#x27;dones&#x27;</span>],</span><br><span class="line">                             dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>).to(self.device)</span><br><span class="line"><span class="comment"># 当前状态，动作、奖励、下一个状态和结束标志</span></span><br><span class="line"></span><br><span class="line">td_target = rewards + self.gamma * self.critic(next_states) * (<span class="number">1</span> - dones)</span><br><span class="line"><span class="comment"># 价值网络(critic)</span></span><br><span class="line"><span class="comment"># self.critic(next_states) 使用价值网络(critic)预测下一个状态的价值。</span></span><br><span class="line"><span class="comment"># (1 - dones) 用于处理 episode 结束的情况，如果 dones 为 1 (True)，则表示 episode 结束，此时不需要考虑未来的奖励。</span></span><br><span class="line">td_delta = td_target - self.critic(states)  </span><br><span class="line"><span class="comment"># 计算时序差分误差，即目标值与当前状态价值的差。</span></span><br><span class="line"></span><br><span class="line">log_probs = torch.log(self.actor(states).gather(<span class="number">1</span>, actions))</span><br><span class="line"><span class="comment"># 策略网络(actor)</span></span><br><span class="line"><span class="comment"># self.actor(states) 使用策略网络预测每个动作的概率。</span></span><br><span class="line"><span class="comment"># .gather(1, actions) 从预测的概率分布中选择实际采取的动作对应的概率。torch.log 计算对数概率。</span></span><br><span class="line"></span><br><span class="line">critic_loss = torch.mean(</span><br><span class="line">	F.mse_loss(self.critic(states), td_target.detach()))</span><br><span class="line"><span class="comment"># 计算价值网络的损失函数。(价值网络给出的当前状态价值)和(r+价值网络给出的下一状态的状态价值)的均方误差，即((a-b)^2)/2</span></span><br><span class="line"><span class="comment"># 使用 td_target.detach() 阻止梯度通过 td_target 向策略网络回传。</span></span><br><span class="line">actor_loss = torch.mean(-log_probs * td_delta.detach())</span><br><span class="line"><span class="comment"># 计算策略网络的损失函数。策略函数梯度</span></span><br><span class="line"><span class="comment"># 使用 td_delta.detach() 阻止梯度通过 td_delta 向价值网络回传。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">self.actor_optimizer.zero_grad()</span><br><span class="line">self.critic_optimizer.zero_grad()</span><br><span class="line">actor_loss.backward()  <span class="comment"># 计算策略网络的梯度</span></span><br><span class="line">critic_loss.backward()  <span class="comment"># 计算价值网络的梯度</span></span><br><span class="line">self.actor_optimizer.step()  <span class="comment"># 更新策略网络的参数</span></span><br><span class="line">self.critic_optimizer.step()  <span class="comment"># 更新价值网络的参数</span></span><br></pre></td></tr></table></figure>
<h3 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h3><p>策略函数</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250324145210312.png" class="" title="image-20250324145210312">
<p>在AC的，优化目标为最大化 <code>J(θ)</code>，通过梯度更新 ；</p>
<p>在TRPO中，优化目标为最大化 新策略与旧策略的价值差 <code>J(θ&#39;)- J(θ)</code> ，通过近似方法（相似策略下状态分布函数不变假设与<strong>重要性采样</strong>）将原目标  <code>J(θ&#39;)- J(θ)</code>  近似为下图，且满足<strong>KL散度</strong>约束（<strong>约束</strong>策略为相似策略）</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250324141848364.png" class="" title="image-20250324141848364">
<p>TRPO与AC的Critic相同；</p>
<h4 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h4><img src="/2024/06/10/reinforce-learning-record/image-20250322150349844.png" class="" title="image-20250322150349844">
<img src="/2024/06/10/reinforce-learning-record/image-20250322150502092.png" class="" title="image-20250322150502092">
<h4 id="近似"><a href="#近似" class="headerlink" title="近似"></a>近似</h4><p>在策略优化中，我们希望找到新策略，使其期望回报最大化。但是，由于计算新策略的状态部分困难，有如下假设：</p>
<p>TRPO假设：<strong>当新旧策略非常接近时，状态分布的变化可以忽略</strong>，即<strong>新策略的状态分布和旧策略的状态分布一致</strong></p>
<blockquote>
<p>合理性：如果策略更新步幅很小（通过后续的KL散度或信任域约束），新旧策略生成的轨迹相似，因此状态分布的差异是“高阶小量”，可以被忽略。这类似于泰勒展开中的一阶近似。</p>
</blockquote>
<p>状态分布被近似为旧策略，但是动作选择使用新策略，为了处理动作分布的差异，TRPO使用<strong>重要性采样</strong>（Importance Sampling），即图中的新旧策略概率比值；</p>
<p>最后，状态分布使用旧策略的，动作选择使用旧策略的，加权计算新策略的改进量。</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250322151444000.png" class="" title="image-20250322151444000">
<img src="/2024/06/10/reinforce-learning-record/image-20250322151537751.png" class="" title="image-20250322151537751">
<h4 id="近似约束与求解"><a href="#近似约束与求解" class="headerlink" title="近似约束与求解"></a>近似约束与求解</h4><p>由于这里使用了近似，所以需要引入约束，使近似可行。因此，为了保证新旧策略足够接近，TRPO 使用了<strong>库尔贝克-莱布勒</strong>（Kullback-Leibler，KL）散度来衡量策略之间的距离。</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250322162045699.png" class="" title="image-20250322162045699">
<h4 id="广义优势"><a href="#广义优势" class="headerlink" title="广义优势"></a>广义优势</h4><p>多步时序差分的指数加权平均</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250325154631583.png" class="" title="image-20250325154631583">
<h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><img src="/2024/06/10/reinforce-learning-record/image-20250322211023368.png" class="" title="image-20250322211023368">
<h3 id="PPO-clip"><a href="#PPO-clip" class="headerlink" title="PPO-clip"></a>PPO-clip</h3><p>基于AC和TRPO的，替换<strong>KL散度约束</strong>实现近似为<strong>惩罚优化/裁剪目标优化</strong>实现近似</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250325200037093.png" class="" title="image-20250325200037093">
<h3 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h3><p>确定性策略</p>
<img src="/2024/06/10/reinforce-learning-record/image-20250323105541258.png" class="" title="image-20250323105541258">
<p>4个网络，价值目标+训练网络，策略目标+训练网络</p>
<p>动作选择引入噪声</p>
<p>价值网络更新使用类似于DDQN的思路更新，残差计算时使用目标网络值-训练网络值</p>
<p>策略网络使用？（证明好复杂</p>
<p>价值目标网络和策略目标网络使用软更新</p>
<h2 id="部分心得"><a href="#部分心得" class="headerlink" title="部分心得"></a>部分心得</h2><p>控制状态空间大小，尽量选择有限状态空间</p>
<p>eg: 将state中的一些值int化</p>
<p>状态空间的表示值尽量接近</p>
<p>采用标准化技术将state内的值均一化</p>
<p>eg: [1, 200, 9999] -&gt; [0.001, 0.99, 0.9999]</p>
<p>更好的初始化</p>
<p>reward设置</p>
<p>惩罚和奖励设置插值不要过大, 防止惩罚拟合过快</p>
<p>+2, -200 -&gt; +2, 0</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/05/08/pytorch-learning-record/" rel="prev" title="pytorch_learning_record">
      <i class="fa fa-chevron-left"></i> pytorch_learning_record
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/06/22/common-commands/" rel="next" title="common-commands">
      common-commands <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#reinforce-learning-record"><span class="nav-text">reinforce learning record</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80"><span class="nav-text">数学基础</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B"><span class="nav-text">机器学习模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MRP"><span class="nav-text">MRP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E6%8A%A5"><span class="nav-text">回报</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">价值函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MDP"><span class="nav-text">MDP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5"><span class="nav-text">策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">状态价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-text">动作价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E4%B8%8E%E5%8A%A8%E4%BD%9C%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E5%85%B3%E7%B3%BB"><span class="nav-text">状态价值函数与动作价值函数关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5"><span class="nav-text">最优策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%9F%E6%9C%9B%E6%96%B9%E7%A8%8B"><span class="nav-text">贝尔曼期望方程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E6%96%B9%E7%A8%8B"><span class="nav-text">贝尔曼最优方程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95-%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E5%87%BD%E6%95%B0%E7%9A%84"><span class="nav-text">策略迭代算法&#x2F;基于策略函数的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95-%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84"><span class="nav-text">价值迭代算法&#x2F;基于价值函数的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3-amp-%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E5%AF%B9%E6%AF%94"><span class="nav-text">策略迭代&amp;价值迭代对比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%89%E6%A8%A1%E5%9E%8B-amp-%E6%97%A0%E6%A8%A1%E5%9E%8B-%E5%9C%A8%E7%BA%BF%E7%AD%96%E7%95%A5-amp-%E7%A6%BB%E7%BA%BF%E7%AD%96%E7%95%A5"><span class="nav-text">有模型&amp;无模型+在线策略&amp;离线策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95"><span class="nav-text">算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95"><span class="nav-text">蒙特卡洛方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95"><span class="nav-text">时序差分算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sarsa%E7%AE%97%E6%B3%95"><span class="nav-text">Sarsa算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q-learning%E7%AE%97%E6%B3%95"><span class="nav-text">Q-learning算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DQN-DDQN"><span class="nav-text">DQN&#x2F;DDQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95-REINFORCE"><span class="nav-text">策略梯度算法(REINFORCE)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Actor-Critic"><span class="nav-text">Actor-Critic</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TRPO"><span class="nav-text">TRPO</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B"><span class="nav-text">整体流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%91%E4%BC%BC"><span class="nav-text">近似</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%91%E4%BC%BC%E7%BA%A6%E6%9D%9F%E4%B8%8E%E6%B1%82%E8%A7%A3"><span class="nav-text">近似约束与求解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89%E4%BC%98%E5%8A%BF"><span class="nav-text">广义优势</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95-1"><span class="nav-text">算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PPO-clip"><span class="nav-text">PPO-clip</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DDPG"><span class="nav-text">DDPG</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E5%88%86%E5%BF%83%E5%BE%97"><span class="nav-text">部分心得</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="marigo1d"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">marigo1d</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/marigo1d" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;marigo1d" rel="noopener" target="_blank">GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">marigo1d</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">386k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">11:43</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
