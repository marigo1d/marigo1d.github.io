<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Algo Learning Record</title>
    <url>/2024/04/25/Algo-Learning-Record/</url>
    <content><![CDATA[<p>算法相关学习记录</p>
<span id="more"></span>



<h1 id="らくがき-about-Algorithm"><a href="#らくがき-about-Algorithm" class="headerlink" title="らくがき about Algorithm"></a>らくがき about Algorithm</h1><h2 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming"></a>Dynamic Programming</h2><h3 id="DP问题分析"><a href="#DP问题分析" class="headerlink" title="DP问题分析"></a>DP问题分析</h3><p>递归&amp;动态规划</p>
<img src="/2024/04/25/Algo-Learning-Record/IMG_0091.PNG" class="" title="IMG_0091(20240425-192828)">

<p>递归会重复进入状态，DP不会</p>
<p>递归一个起点，多个终点；</p>
<p>DP多个起点，一个终点；</p>
<p>DP核心：</p>
<p>1.确定状态和动作</p>
<p>2.确定起点和终点</p>
<h3 id="无权图两点间路径数"><a href="#无权图两点间路径数" class="headerlink" title="无权图两点间路径数"></a>无权图两点间<strong>路径数</strong></h3><p>递归思路：</p>
<p>1.从终点出发，存在多个选择，每个选择为分支，<strong>传入递归函数</strong>并进入；取得递归结果相加。</p>
<p>2.对于每一个中间态，存在多个选择，<strong>传入递归函数</strong>并进入；</p>
<p>3.到达起点，当前递归结束，返回；</p>
<p>DP思路：</p>
<p>1.从起点出发，到达所有<strong>邻接点</strong>的走法已知；</p>
<p>2.到达<strong>次邻接点</strong>的走法为从<strong>邻接点</strong>到<strong>次邻接点</strong>的走法之和；</p>
<p>3.到达终点的走法为从<strong>终点邻接点</strong>到终点的走法之和；</p>
<p>实例：</p>
<img src="/2024/04/25/Algo-Learning-Record/image-20240426101614103.png" class="" title="image-20240426101614103">

<p>递归</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">pathsCount</span><span class="params">(<span class="type">int</span> m, <span class="type">int</span> n)</span> &#123;</span><br><span class="line">    <span class="comment">//到达起点</span></span><br><span class="line">    <span class="keyword">if</span>(m == <span class="number">1</span> || n == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">//从终点前进</span></span><br><span class="line">    <span class="keyword">return</span> pathsCount(m - <span class="number">1</span>, n) + pathsCount(m , n -<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>DP</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">pathsCounts</span><span class="params">(<span class="type">int</span> m, <span class="type">int</span> n)</span> &#123;</span><br><span class="line">    <span class="type">int</span>[][] dp = <span class="keyword">new</span> <span class="title class_">int</span>[m][n];</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//初始化起点</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; n; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(i == <span class="number">0</span> || j == <span class="number">0</span>) &#123;</span><br><span class="line">                dp[i][j] = <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">        </span><br><span class="line">    <span class="comment">//从起点前进</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; n; j++) &#123;</span><br><span class="line">            dp[i][j] = dp[i-<span class="number">1</span>][j] + dp[i][j-<span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//到达终点</span></span><br><span class="line">    <span class="keyword">return</span> dp[m-<span class="number">1</span>][n-<span class="number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="有权图两点间最大路径"><a href="#有权图两点间最大路径" class="headerlink" title="有权图两点间最大路径"></a>有权图两点间<strong>最大路径</strong></h3><p>递归思路：</p>
<p>1.从终点出发，存在多个选择，每个选择为分支，<strong>传入递归函数</strong>并进入；取得递归结果取最大。</p>
<p>2.对于每一个中间态，存在多个选择，<strong>传入递归函数</strong>并进入；</p>
<p>3.到达起点，当前递归结束，返回当前节点值；</p>
<p>DP思路：</p>
<p>1.从起点出发，到达所有<strong>邻接点</strong>的距离可知；</p>
<p>2.到达<strong>次邻接点</strong>的最大距离为从<strong>邻接点</strong>到<strong>次邻接点</strong>的最大距离；</p>
<p>3.到达终点的走法为从<strong>终点邻接点</strong>到终点的最大距离；</p>
<p>实例：</p>
<img src="/2024/04/25/Algo-Learning-Record/image-20240426102031155.png" class="" title="image-20240426102031155">

<p>递归</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">minPathSum</span><span class="params">(<span class="type">int</span>[][] grid)</span> &#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">m</span> <span class="operator">=</span> grid.length;</span><br><span class="line">    <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> grid[<span class="number">0</span>].length;</span><br><span class="line">    <span class="comment">//从终点前进</span></span><br><span class="line">    <span class="keyword">return</span> getMin(grid, m-<span class="number">1</span>, n-<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="type">int</span> <span class="title function_">getMin</span><span class="params">(<span class="type">int</span>[][] grid, <span class="type">int</span> i, <span class="type">int</span> j)</span> &#123;</span><br><span class="line">    <span class="comment">//到达起点</span></span><br><span class="line">    <span class="keyword">if</span>(i == <span class="number">0</span> &amp;&amp; j == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> grid[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="keyword">if</span>(i == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> grid[i][j] + getMin(grid, i, j-<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(j == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> grid[i][j] + getMin(grid, i-<span class="number">1</span>, j);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">	<span class="comment">//</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">minRow</span> <span class="operator">=</span> getMin(grid, i-<span class="number">1</span>, j);</span><br><span class="line">    <span class="type">int</span> <span class="variable">minColumn</span> <span class="operator">=</span> getMin(grid, i, j-<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grid[i][j] + Math.min(minRow, minColumn);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>DP</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">minPathSum</span><span class="params">(<span class="type">int</span>[][] grid)</span> &#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">m</span> <span class="operator">=</span> grid.length;</span><br><span class="line">    <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> grid[<span class="number">0</span>].length;</span><br><span class="line">    <span class="type">int</span>[][] dp = <span class="keyword">new</span> <span class="title class_">int</span>[m][n];</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//初始化起点</span></span><br><span class="line">    dp[<span class="number">0</span>][<span class="number">0</span>] = grid[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt; m ;i++) &#123;</span><br><span class="line">        dp[i][<span class="number">0</span>] = dp[i-<span class="number">1</span>][<span class="number">0</span>] + grid[i][<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt; n;i++) &#123;</span><br><span class="line">        dp[<span class="number">0</span>][i] = dp[<span class="number">0</span>][i-<span class="number">1</span>] + grid[<span class="number">0</span>][i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//从起点前进</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt; m; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">1</span>; j &lt; n; j++) &#123;</span><br><span class="line">            dp[i][j] = grid[i][j] + Math.min(dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//到达终点</span></span><br><span class="line">    <span class="keyword">return</span> dp[m-<span class="number">1</span>][n-<span class="number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title>Op_sys Learning Record</title>
    <url>/2024/04/12/Op-sys-Learning-Record/</url>
    <content><![CDATA[<p>操作系统相关学习记录</p>
<span id="more"></span>



<h1 id="Opsys-learning-record"><a href="#Opsys-learning-record" class="headerlink" title="Opsys_learning_record"></a>Opsys_learning_record</h1><h3 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h3><p>QEMU：近似模拟硬件SiFiveFU540-C000主板(RISC-V,64 bit register, 56 bit physical memory)</p>
<p>intel 8086：20根地址线，16位寄存器</p>
<h3 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h3><p>XV6：操作系统</p>
<p><a href="https://github.com/seaswalker/tiny-os">tiny-os</a></p>
<h2 id="系统启动"><a href="#系统启动" class="headerlink" title="系统启动"></a>系统启动</h2><h3 id="基础信息"><a href="#基础信息" class="headerlink" title="基础信息"></a>基础信息</h3><h4 id="地址空间"><a href="#地址空间" class="headerlink" title="?地址空间"></a>?地址空间</h4><p>20根地址线，16位寄存器，可访问1MB的内存空间</p>
<p>0x00000-0x9FFFF: DRAM</p>
<p>0xF0000-0xFFFFF: ROM，BIOS代码位置</p>
<p>通过段寄存器左移4位+偏移地址</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240413150024273.png" class="" title="image-20240413150024273">

<img src="/2024/04/12/Op-sys-Learning-Record/image-20240417110651232.png" class="" title="image-20240417110651232">





<h4 id="SiFive-FU540地址空间"><a href="#SiFive-FU540地址空间" class="headerlink" title="SiFive FU540地址空间"></a>SiFive FU540地址空间</h4><p>物理地址0x80000000以前的部分由主板上非DRAM组成（例如：boot ROM，PLIC等）</p>
<p>物理地址对应的硬件（图例为SiFive主板，仅截取部分，物理地址0x80000000以前的部分非DRAM）</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240413150852809.png" class="" title="image-20240413150852809">

<img src="/2024/04/12/Op-sys-Learning-Record/image-20240412184947166.png" class="" title="image-20240412184947166">



<h4 id="显存地址空间"><a href="#显存地址空间" class="headerlink" title="显存地址空间"></a>显存地址空间</h4><p>0xB8000-0xBFFFF为显存中字符显示部分，默认模式为80个字符*25行，一个字符2字节表示</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240420184138573.png" class="" title="image-20240420184138573">





<h4 id="寄存器"><a href="#寄存器" class="headerlink" title="寄存器"></a>寄存器</h4><p>不可见寄存器</p>
<p>GDTR	全局描述符表寄存器</p>
<p>IDTR	中断描述符表寄存器</p>
<p>LDTR	局部描述符表寄存器</p>
<p>TR	任务寄存器</p>
<p>CR0~3	控制寄存器</p>
<p>IP	指令指针寄存器</p>
<p>flags	标志寄存器</p>
<p>DR0~3	调试寄存器</p>
<p>可见寄存器</p>
<p>通用寄存器，可直接访问</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240420144227413.png" class="" title="image-20240420144227413">

<img src="/2024/04/12/Op-sys-Learning-Record/image-20240420144655049.png" class="" title="image-20240420144655049">

<p>段寄存器，段基址*16+偏移</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240420144356905.png" class="" title="image-20240420144356905">



<p>32位与16位</p>
<p>32位地址总线，32位寄存器</p>
<p>寄存器组</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240416185859023.png" class="" title="image-20240416185859023">

<p>通用寄存器组，Eflag和EIP变为32位</p>
<p>段寄存器仍为16位宽！！内容为段选择子</p>
<h4 id="Intel汇编语法"><a href="#Intel汇编语法" class="headerlink" title="Intel汇编语法"></a>Intel汇编语法</h4><p>实模式</p>
<p>段内偏移默认基于DS进行偏移</p>
<p>基址寄存器：bx寄存器默认基于DS进行偏移，bp寄存器默认基于SS进行偏移</p>
<p>变址寄存器：si寄存器和di寄存器默认基于ds进行偏移</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mov ax, [0x1234]	;取DS*16+0x1234处的值赋给ax</span><br><span class="line"></span><br><span class="line">add [bx], 0x1234	;将0x1234赋给内存中DS*16+bx处</span><br><span class="line"></span><br><span class="line">mov ax, [fs:0x5678]	;显式指定使用gs寄存器作为段基址，将gs*16+0x5678的值赋给ax</span><br><span class="line"></span><br><span class="line">mov byte [gs:0x01], &#x27;M&#x27;	;显式指定使用gs段寄存器作为段基址，指定操作数为1字节，向gs*16+0x01的位置写入1字节的数据&#x27;M&#x27;</span><br><span class="line"></span><br><span class="line">mov ax, [sp]	;将sp指向的值赋给ax</span><br><span class="line"></span><br><span class="line">mov ax, [bp+4]	;将SS*16+bp+4处的值赋给ax</span><br><span class="line"></span><br><span class="line">mov [di], ax	;将ax的值写入DS*16+di处</span><br><span class="line"></span><br><span class="line">mov [si+0x1234], ax	;将ax的值写入DS*16+si+0x1234处</span><br><span class="line"></span><br><span class="line">mov [bx+di], ax	;将ax的值写入DS*16+bx+di</span><br><span class="line"></span><br><span class="line">jump $	; $符号表示当前行，该语句为跳转至当前行；$$为当前section</span><br><span class="line"></span><br><span class="line">;-----------------------stdcall调用约定------------------------</span><br><span class="line">push 2</span><br><span class="line">push 3	;从右往左将传入参数压栈</span><br><span class="line">call subtract	;将当前eip压栈，跳转至被调函数地址开始执行</span><br><span class="line"></span><br><span class="line">push ebp	;压栈保存原ebp，</span><br><span class="line">mov ebp, esp	;更新栈帧基址，进入新栈帧</span><br><span class="line"></span><br><span class="line">mov eax, [ebp+0x8]</span><br><span class="line">mov eax, [ebp+0xc]</span><br><span class="line"></span><br><span class="line">mov esp, ebp</span><br><span class="line"></span><br><span class="line">pop ebp	;弹出栈顶保存的原ebp值到ebp</span><br><span class="line">ret 8	;弹出栈顶地址到eip(弹栈使得esp自加4)，esp+8(移除压栈的传入参数)</span><br><span class="line">;-------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">;-----------------------cdecl调用约定--------------------------</span><br><span class="line">push 2</span><br><span class="line">push 3	;从右往左将传入参数压栈</span><br><span class="line">call subtract	;将当前eip压栈，跳转至被调函数地址开始执行</span><br><span class="line">add esp, 8	;清栈，移除传入参数</span><br><span class="line"></span><br><span class="line">push ebp	;压栈保存原ebp，进入新栈帧</span><br><span class="line">mov ebp, esp</span><br><span class="line"></span><br><span class="line">mov eax, [ebp+0x8]</span><br><span class="line">mov eax, [ebp+0xc]</span><br><span class="line"></span><br><span class="line">mov esp, ebp</span><br><span class="line"></span><br><span class="line">pop ebp	;弹出栈顶保存的原ebp值到ebp</span><br><span class="line">ret	;弹出栈顶地址到eip(弹栈使得esp自加4)，esp+8(移除压栈的传入参数)</span><br><span class="line">;-------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">section .data	;section为伪指令，面向程序员的跳转记号</span><br><span class="line">	var dd 0</span><br><span class="line">	str: dd &quot;hello&quot;, 0xa, 0</span><br><span class="line"></span><br><span class="line">section .text</span><br><span class="line">extern c_print	;导入外部函数 c_print</span><br><span class="line">global _start	;将函数_start导出为全局符号，供外部文件调用</span><br><span class="line"></span><br><span class="line">_start:</span><br><span class="line">	push str</span><br><span class="line">	jmp $$</span><br></pre></td></tr></table></figure>

<p>SS为栈底，bp为当前帧底</p>
<p>保护模式</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240420193121630.png" class="" title="image-20240420193121630">

<img src="/2024/04/12/Op-sys-Learning-Record/image-20240420195137241.png" class="" title="image-20240420195137241">



<p>segment与section</p>
<p>section称为节，是指在汇编源码中经由关键字section或segment修饰、逻辑划分的指令或数据区域，汇编器会将这两个关键字修饰的区域在目标文件中编译成节，也就是说“节”最初诞生于目标文件中。</p>
<p>segment称为段，是链接器根据目标文件中属性相同的多个section合并后的section集合，这个集合称为segment，也就是段，链接器把目标文件链接成可执行文件，因此段最终诞生于可执行文件中。我们平时所说的可执行程序内存空间中的代码段和数据段就是指的segment。</p>
<h4 id="AT-T汇编语法"><a href="#AT-T汇编语法" class="headerlink" title="AT&amp;T汇编语法"></a>AT&amp;T汇编语法</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">movl %eax, base_value+(%ebx, %esi,2)	</span><br><span class="line">	//将eax的值写入内存base_value+ebx+esi*2处</span><br><span class="line">	//()括号表示取内的值作为内存地址</span><br><span class="line">	//寄存器前必须有%</span><br><span class="line"></span><br><span class="line">movl $123, %eax	//将立即数123写入寄存器eax</span><br></pre></td></tr></table></figure>



<p>内联汇编</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">asm</span> [<span class="keyword">volatile</span>] (<span class="string">&quot;&quot;</span>)	<span class="comment">//volatile为可选项，表示此处代码不得修改</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">asm</span> (<span class="string">&quot;movl $9, %eax;&quot;</span> <span class="string">&quot;pushl %eax&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">asm</span> [<span class="keyword">volatile</span>] (<span class="string">&quot;&quot;</span>:output :input :clobber/modify)</span><br><span class="line">	<span class="comment">//output和input为c语言变量，clobber/modify可能存在寄存器破坏</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">asm</span> (<span class="string">&quot;addl %%ebx, %%eax;&quot;</span>:<span class="string">&quot;=a&quot;</span>(out_sum):<span class="string">&quot;a&quot;</span>(in_a),<span class="string">&quot;b&quot;</span>(in_b))</span><br><span class="line">	<span class="comment">//&quot;a&quot;为约束名，约束c语言变量in_a使用寄存器eax</span></span><br><span class="line">	<span class="comment">//&quot;b&quot;为约束名，约束c语言变量in_b使用寄存器ebx</span></span><br><span class="line">	<span class="comment">//&quot;=a&quot;为约束名，将寄存器eax的值写入c语言变量out_sum</span></span><br><span class="line">	<span class="comment">//=为只写，+为读写，&amp;为独占所约束的寄存器</span></span><br><span class="line">	<span class="comment">//由于扩展内联汇编中，%0为占位符符号，故修改寄存器表示为%%eax</span></span><br><span class="line">	</span><br><span class="line"><span class="keyword">asm</span> (<span class="string">&quot;movb %0 %1;&quot;</span>::<span class="string">&quot;a&quot;</span>(in_a),<span class="string">&quot;m&quot;</span>(in_b))</span><br><span class="line">	<span class="comment">//&quot;a&quot;为约束名，约束c语言变量in_a使用寄存器eax</span></span><br><span class="line">	<span class="comment">//&quot;m&quot;为约束名，使用c语言变量in_b的内存地址</span></span><br><span class="line">	<span class="comment">//%0,%1为序号占位符，所有input和output从左至右被标序号，代表对应的		input/output</span></span><br><span class="line">	</span><br><span class="line"><span class="keyword">asm</span> (<span class="string">&quot;divb %[divisor]; movb %%al, %[result]&quot;</span>\</span><br><span class="line">		:[result]<span class="string">&quot;=m&quot;</span>(out)\</span><br><span class="line">		:<span class="string">&quot;a&quot;</span>(in_a),[divisor]<span class="string">&quot;m&quot;</span>(in_b)\</span><br><span class="line">		);</span><br><span class="line">	<span class="comment">//[divisor]为in_b的内存地址，通过%[divisor]获取内存的值</span></span><br><span class="line">	<span class="comment">//[result]为out的内存地址，</span></span><br></pre></td></tr></table></figure>



<h4 id="GDT-LDT与TSS"><a href="#GDT-LDT与TSS" class="headerlink" title="GDT, LDT与TSS"></a>GDT, LDT与TSS</h4><img src="/2024/04/12/Op-sys-Learning-Record/image-20240609174716552.png" class="" title="image-20240609174716552">

<p>硬件厂商推荐通过每个任务拥有一个LDT和一个TSS的方式实现多任务</p>
<p>LDT保存任务的代码和数据，TSS保存任务的上下文（各寄存器的值）和不同权限下的栈指针</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240506191638362.png" class="" title="image-20240506191638362">



<p>实际使用中Linux，每个CPU上的所有任务共享一个TSS，并由该CPU的TR保存；进程切换时，改变TSS中SS0和esp0的值，任务的上下文（即部分寄存器值）压入该任务的<strong>内核中断栈</strong>保存 —– 出自操作系统真相还原</p>
<p>所有进程的代码段，数据段等描述符均保存在GDT中</p>
<p>eg: call 0x0018:0x1234 使用在GDT中索引0x0018的选择子获得段基址，偏移0x1234</p>
<h4 id="段描述符与门描述符区别"><a href="#段描述符与门描述符区别" class="headerlink" title="段描述符与门描述符区别"></a>段描述符与门描述符区别</h4><img src="/2024/04/12/Op-sys-Learning-Record/image-20240416192952163.png" class="" title="image-20240416192952163">

<img src="/2024/04/12/Op-sys-Learning-Record/image-20240417104649804.png" class="" title="image-20240417104649804">

<p>门描述符中包含了段选择子，指向段描述符</p>
<p>门描述符包含了偏移量！！！</p>
<h4 id="部分地址空间"><a href="#部分地址空间" class="headerlink" title="部分地址空间"></a>部分地址空间</h4><p>tiny-os kernel地址空间</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240421151156773.png" class="" title="image-20240421151156773">

<p>地址空间：</p>
<p>0x0009a000为位图起始地址，占4页</p>
<p>0x0009e000为内核PCB起始，占1页</p>
<p>​	0x0009f000为PCB中内核栈顶(低1M地址中所使用到的最高地址)</p>
<p>0x0009f000~0x00100000为空</p>
<p>0x00100000为内核堆起始，</p>
<p>​	0x00100000~0x00101fff为页目录表和页表</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240427095137775.png" class="" title="image-20240427095137775">



<p>SiFive FU540</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240412184838896.png" class="" title="image-20240412184838896.png">



<p>用户程序地址空间</p>
<p>虚拟地址：</p>
<p>KERNBASE &#x3D; 0x8000 0000</p>
<p>KERNLINK &#x3D; 0x8010 0000</p>
<p>KERNBASE+PHYSTOP &#x3D; 0x8E00 0000</p>
<p>DEVSPACE &#x3D; 0xFE00 0000</p>
<p>物理地址：</p>
<p>低1M部分：0 - 0x10 0000</p>
<p>部分内核数据+空闲空间（用户程序所分配的页表）：0x10 0000 - 0xE00 0000(PHYSTOP)</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240412200020614.png" class="" title="image-20240412200020614">

<p>用户程序虚拟内存内容</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240413095215393.png" class="" title="image-20240413095215393">



<h4 id="会话session"><a href="#会话session" class="headerlink" title="会话session"></a>会话session</h4><p>进程通过fork创建新的子进程，进程与创建出的子进程同属一个进程组</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> main.c | grep <span class="keyword">for</span> | more</span><br></pre></td></tr></table></figure>

<p>以上三个命令对应三个进程，同属于一个进程组，CTRL+C将中止该进程组的所有进程</p>
<p>Session为进程组的集合，用户通过tty登录后所有的命令(执行的程序)属于同一个session</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240609190505627.png" class="" title="image-20240609190505627">





<h3 id="引导启动程序"><a href="#引导启动程序" class="headerlink" title="引导启动程序"></a>引导启动程序</h3><h4 id="启动流程图"><a href="#启动流程图" class="headerlink" title="启动流程图"></a>启动流程图</h4><p>Linux</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240608151929426.png" class="" title="image-20240608151929426">



<h4 id="BIOS"><a href="#BIOS" class="headerlink" title="BIOS"></a>BIOS</h4><p>计算机启动时为实模式运行，20位地址线，仅能访问1MB空间</p>
<p>机器上电，cs:ip 初始化为 0xf000:0x1110；</p>
<p>CPU从0xffff0开始执行，执行至0xfffff处指令jump后跳转至 f000:e05b，BIOS主体代码所在处；</p>
<p>BIOS内容代码进行硬件检测（内存检测，显卡检测…），</p>
<p>在0x000-0x3ff处建立数据结构，中断向量表IVT，填写中断例程，</p>
<p>进行启动盘检测(0盘0道1扇区最后2个字节:55AA)，从硬盘0盘0道1扇区加载MBR至0x7c00后，跳转执行MBR。</p>
<h4 id="MBR"><a href="#MBR" class="headerlink" title="MBR"></a>MBR</h4><p>MBR占512字节（一个扇区），结尾内容为55AA。</p>
<p>MBR程序位于0x7c00</p>
<p>Linux: boot&#x2F;bootsect.s</p>
<p>bootsect程序(MBR)位于0x7c00，然后bootsect将自身复制到0x90000处，执行跳转至0x90000，使用0x9ff00为栈顶</p>
<p>bootsect将setup程序写入0x90200处，将system加载至0x10000处</p>
<p>​	ps:未直接加载内核至0x00000的原因是0x00000有中断向量表</p>
<p>tiny_os</p>
<p>MBR程序将loader程序写入0x90000处</p>
<h4 id="setup-loader"><a href="#setup-loader" class="headerlink" title="setup&amp;loader"></a>setup&amp;loader</h4><p>Linux: boot&#x2F;setup.s</p>
<p>使用实模式下的中断向量表读取机器数据，写入0x90000处（覆盖bootsect）</p>
<p>整体下移内核至0x00000处，</p>
<p>设置临时ldt和gdt，进入保护模式，跳转至地址0x00000</p>
<p>tiny_os</p>
<p>loader程序位于0x90000，使用0x90000以下部分作为程序栈</p>
<p>loader设置gdt，进入保护模式，并在保护模式加载内核</p>
<h5 id="保护模式"><a href="#保护模式" class="headerlink" title="保护模式"></a>保护模式</h5><p>保护模式使机器从16位进入到32位，寻址方式变更为 基址+偏移</p>
<h6 id="段描述符与全局描述符表GDT"><a href="#段描述符与全局描述符表GDT" class="headerlink" title="段描述符与全局描述符表GDT"></a>段描述符与全局描述符表GDT</h6><p>段寄存器: cs, ds, …(16位寄存器)，保存选择子，即gdt&#x2F;ldt的index</p>
<p>选择子</p>
<p>RPL：特权等级；TI：选择GDT&#x2F;LDT；索引值：GDT&#x2F;LDT表中的第x个段描述符</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240416193845119.png" class="" title="image-20240416193845119">

<p>GDTR</p>
<p>保存GDT的物理地址，GDT的段描述符记录了各内存段的相关信息</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240416193617238.png" class="" title="image-20240416193617238">

<p>LDTR</p>
<p>保存LDT的内存地址，<strong>LDT必须在GDT中声明</strong>，通过GDT获得LDT内存地址后写入LDTR后使用LDT</p>
<p>GDT</p>
<p>item: 段描述符</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240416192952163.png" class="" title="image-20240416192952163">

<p>段基址：32位段基址；</p>
<p>段界限：对于数据段和代码段，段界限向高地址延申；对于栈，段界限向低地址延申；若偏移量超界错；段大小由 段界限*G 确定，</p>
<p>DPL：段特权级</p>
<p>S：是否系统段</p>
<p>TYPE：段类型</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240427102403905.png" class="" title="image-20240427102403905">



<p>段描述符缓冲寄存器</p>
<p>保存处理后的段描述符</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240420192623948.png" class="" title="image-20240420192623948">

<p>段描述符与内存段</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240427102456078.png" class="" title="image-20240427102456078">

<p>地址计算方式&#x3D;32位段基址+32段偏移</p>
<h5 id="保护模式的启动"><a href="#保护模式的启动" class="headerlink" title="保护模式的启动"></a>保护模式的启动</h5><p>Linux: boot&#x2F;setup.s</p>
<p>使用实模式下的中断向量表读取机器数据，写入0x90000处（覆盖bootsect）</p>
<p>整体下移内核至0x00000处，</p>
<p>设置临时ldt和gdt，加载ldtr和gdtr，开启A20地址线；</p>
<p>重新设备中断控制芯片8259A，设置硬件中断号为0x20 ~ 0x2f，修改CR0寄存器</p>
<p>进入保护模式，跳转至地址0x00000</p>
<p>tiny_os</p>
<p>loader程序位于0x90000，使用0x90000以下部分作为程序栈</p>
<p>1.构建GDT，<strong>GDT内容直接定义在loader内</strong></p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240427102121106.png" class="" title="image-20240427102121106">

<p>2.开启A20地址线</p>
<p>A20地址线：20位地址线的CPU自动发生地址回绕（20位段基址+16位段偏移所产生的超出1MB的部分）</p>
<p>32位地址线为了兼容20位地址线产生的地址回绕，在关闭A20地址线时，CPU将进行地址回绕；在开启A20地址线时，正确访问；</p>
<p>3.加载GDT地址到GDTR</p>
<p>4.置位CR0寄存器的PE位</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240416195112069.png" class="" title="image-20240416195112069">

<p>5.将段寄存器内容更新为段选择子内容</p>
<p>Linux:[类似]</p>
<p>临时gdt包含了两个段描述符，数据段描述符和代码段描述符，段基址均为0x00000</p>
<p>跳转0x00000处</p>
<h4 id="head-loader"><a href="#head-loader" class="headerlink" title="head&amp;loader"></a>head&amp;loader</h4><h5 id="页表组成与构建"><a href="#页表组成与构建" class="headerlink" title="页表组成与构建"></a>页表组成与构建</h5><p>虚拟地址与物理地址映射关系</p>
<p>虚实转换由MMU(Memory Management Unit)硬件实现，</p>
<p>CPU中的SATP寄存器保存Page Table Entry，指向最高一级的页目录page directory的物理内存地址</p>
<p>CPU中的TLB(Translation Lookside Buffer)保存近期访问的PTE内容</p>
<p>XV6</p>
<p>页表中的每一项为PTE(Page Table Entry)，一个PTE 54bit，一级页表为3.375KB</p>
<p>通过SATP寄存器获得最高级页表的物理地址PPN，使用L2查最高级页表获得中间级页表的物理地址；使用L1查中间级页表获得最低级页表的物理地址；使用L0查最低级页表，获得物理地址；</p>
<p>通过TLB获取物理地址PPN；</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240412190738605.png" class="" title="image-20240412190738605">

<img src="/2024/04/12/Op-sys-Learning-Record/image-20240412192043741.png" class="" title="image-20240412192043741">



<p>Linux 2.6</p>
<p>虚拟地址高10位，在页目录表中查找对应索引，获得页表的物理地址（20位，余下12位补0）；</p>
<p>虚拟地址中10位，在页表中查找对应索引，获得页的物理地址（20位，余下12位补0）；</p>
<p>虚拟地址后12位，页内偏移；</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240420140907729.png" class="" title="image-20240420140907729">





<p>tiny-os</p>
<p>用户使用低3GB，内核使用高1GB，每页4KB</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240427102727532.png" class="" title="image-20240427102727532">

<p>建立页目录表–页目录表的项即指向一个页表</p>
<p>在物理地址0x100000建立页目录表，一个页目录表含1024个页表项PDE，每个页目录表项4B，占4KB，</p>
<p>第0项（保证loader程序在启动页表后能够正常运行）指向第一个页表，物理地址0x101000</p>
<p>第768项（虚拟地址3GB，用户程序访问内核）指向第一个页表，物理地址0x101000</p>
<p>第1023项指向页目录表，物理地址0x100000</p>
<p>建立页表</p>
<p>在物理地址0x101000建立，一个页表含1024个页表项，每个页表项4B，占4KB，可表示4KB*1024&#x3D;4MB的地址空间</p>
<p>该页表指向的物理地址为0-0x3fffff</p>
<h5 id="页表构建与进入分页模式"><a href="#页表构建与进入分页模式" class="headerlink" title="页表构建与进入分页模式"></a>页表构建与进入分页模式</h5><p>Linux: boot&#x2F;head.s</p>
<p><img src="file://D:/github/blog/source/_posts/Op-sys-Learning-Record/image-20240609161606117.png?lastModify=1717920885" alt="image-20240609161606117"></p>
<p>位于物理地址0x00000处</p>
<p>加载数据段寄存器，</p>
<p>重新设置中断描述符表idt，256项全部指向哑中断处理程序</p>
<p>重新设置gdt</p>
<p>检查A20地址线是否开启</p>
<p>检查机器是否含有数学协处理器，修改CR0寄存器标志位</p>
<p>设置页目录表（为全体进程使用）与页表，4个页表（内核专用页表，新的进程的页表会在主内存区申请）位于页目录表之后，寻址16Mb空间（4KB&#x2F;页表大小，4B&#x2F;页表项，1024*4KB&#x3D;4MB每项寻址空间）</p>
<p>将main代码的地址压入位于0x10处的<strong>内核的栈</strong>，使用iret跳转main代码执行</p>
<p>tiny_os</p>
<p>loader中实现页表的构建与进入分页模式</p>
<p>loader程序</p>
<p>6.清空页目录占用空间0x100000+4096</p>
<p>7.创建页目录项并写入页目录表第0项(指向第1个页表)，第768项(指向第1个页表)和第1023项(指向页目录表，该项用于修改页目录表)</p>
<p>8.创建页表项写入第1个页表(位于0x101000)，分配物理地址0~0x3fffff(实验用mini内核在此处)</p>
<p>9.创建页目录项并写入页目录表第769项~第1022项(指向第2,3,4…个页表,第2,3,4…个页表位于0x102000,0x103000…)</p>
<p>10.修改gdt中段描述符的地址值，上移至0xc0000000以上</p>
<p>11.修改gdt基址，修改esp，上移至0xc0000000以上</p>
<p>12.将页目录地址赋值给cr3，打开cr3的pg位，新gdt值写入gdtr</p>
<p>分页模式启动，</p>
<p>程序<strong>此时</strong>已进入虚拟地址运行，</p>
<p>当前虚拟地址0x0000xxxx，仍处于低1M地址空间中(页目录表第一项指向第一个页表的原因)；</p>
<p>从gdt中获得的段描述符的值为虚拟地址；</p>
<p>​	</p>
<h4 id="初始化程序-loader"><a href="#初始化程序-loader" class="headerlink" title="初始化程序&amp;loader"></a>初始化程序&amp;loader</h4><img src="/2024/04/12/Op-sys-Learning-Record/image-20240609163346462.png" class="" title="image-20240609163346462">

<p>进入保护模式时，<strong>内核的栈</strong>位于0x10</p>
<p>初始化时，main.c被切换至任务0执行，此时main代码获得了PCB，main.c的用户态栈仍为起始于0x10的栈，内核栈为PCB中的栈</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240609180508264.png" class="" title="image-20240609180508264">

<p>Linux: main.c &amp; 任务0</p>
<p>初始化设备与内存信息：</p>
<p>​	根设备号，高速缓存末端地址，机器内存数，主内存开始地址；</p>
<p>调用内核中初始化函数初始化各功能部分；</p>
<ul>
<li>硬件中断向量初始化</li>
<li>块设备初始化</li>
<li>字符设备初始化</li>
<li>tty初始化</li>
<li>时间初始化，访问CMOS获取</li>
<li>调度程序初始化</li>
<li>缓存初始化</li>
<li>硬盘初始化</li>
<li>软驱初始化</li>
</ul>
<p>进入用户模式，fork创建出任务1(init())</p>
<p>循环pause状态—</p>
<p>​	ps: 程序进入interruptible状态（闲置进程），无任务可调度时调度任务0</p>
<p>​	ps:任务0为所有进程的父进程，故不使用堆栈</p>
<p>Linux: main.c&#x2F;init() 任务1</p>
<p>读取硬盘参数，包含分区表信息，加载虚拟盘（若存在），安装根文件系统设备</p>
<p>以读写方式打开tty0，得到文件句柄0stdin，复制两次得到stdout和stderr</p>
<p>fork产生任务2，等待任务2退出</p>
<p>循环，产生更多任务</p>
<p>Linux: main.c&#x2F;init() 任务2</p>
<p>关闭从任务1所继承的stdin，stdout和stderr；创建新的会话期</p>
<p>tiny_os</p>
<p>loader中实现kernel写入内核</p>
<p>loader程序位于0x90000，使用0x90000以下部分作为程序栈</p>
<p>13.读磁盘指定扇区，写kernel.bin到虚拟地址0xc0070000~0xc009fbff</p>
<p>14.初始化kernel，将kernel.bin中的segment复制到被编译的虚拟地址处，第一个segment起始于0xc0001500</p>
<p>15.修改栈指针，跳转至kernel，虚拟地址0xc0001500</p>
<h2 id="中断"><a href="#中断" class="headerlink" title="中断"></a>中断</h2><h3 id="外部中断"><a href="#外部中断" class="headerlink" title="外部中断"></a>外部中断</h3><img src="/2024/04/12/Op-sys-Learning-Record/image-20240421152049031.png" class="" title="image-20240421152049031">

<p>中断引脚：</p>
<p>INTR: Interupter</p>
<p>NMI: Non Maskable Interupter	向CPU传入中断向量号 2</p>
<p>外部设备中断由中断代理芯片8259A接收后发送中断向量号到CPU中断引脚INTR</p>
<p>Linux： int32 ~ int47 -&gt; IRQ0 ~ IRQ15</p>
<p>IRQ0</p>
<p>可编程芯片 8253 定时发出IRQ0信号</p>
<h3 id="内部中断"><a href="#内部中断" class="headerlink" title="内部中断"></a>内部中断</h3><p>软中断：系统调用</p>
<p>Linux：int128</p>
<p>异常：指针越界，栈溢出</p>
<p>Linux：int0 ~ int31</p>
<h4 id="实模式下的中断向量表"><a href="#实模式下的中断向量表" class="headerlink" title="实模式下的中断向量表"></a>实模式下的中断向量表</h4><p>仅在实模式使用，由BIOS进行创建，位于0x00000~0x003ff</p>
<h4 id="中断门与保护模式下的中断描述符表IDT"><a href="#中断门与保护模式下的中断描述符表IDT" class="headerlink" title="中断门与保护模式下的中断描述符表IDT"></a>中断门与保护模式下的中断描述符表IDT</h4><p>中断向量号为 中断向量表&#x2F;中断描述符表 的索引下标</p>
<p>IDTR(Interrupt Descriptor Table Register) 中保存 IDT 位置信息</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240417105214817.png" class="" title="image-20240417105214817">

<p>IDT(Interrupt Descriptor Table)</p>
<p>item: 门</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240417104649804.png" class="" title="image-20240417104649804">



<h3 id="中断初始化"><a href="#中断初始化" class="headerlink" title="中断初始化"></a>中断初始化</h3><p>tiny_os</p>
<p>实模式由BIOS创建中断向量表和中断处理程序；</p>
<p>保护模式由kernel创建中断描述符表和中断处理程序；</p>
<p>1.kernel进行中断初始化：中断描述符初始化，异常处理程序初始化，pic初始化</p>
<p>2.中断描述符初始化：创建中断描述符，描述符中的偏移量由一阶中断处理程序的偏移给出(位于kernel.asm，一阶中断处理程序在进行上下文保存后call二阶中断处理程序，即该初始化中创建的异常处理程序)，写入中断描述符表</p>
<p>3.异常处理程序初始化：将异常处理程序的地址写入二阶中断处理程序表，该表保存函数地址，由一阶中断处理程序call；</p>
<p>4.pic初始化</p>
<p>5.将中断描述符表地址写入idtr</p>
<h3 id="中断处理过程"><a href="#中断处理过程" class="headerlink" title="中断处理过程"></a>中断处理过程</h3><p>1.中断请求</p>
<p>中断请求可能为内部中断或者外部中断</p>
<p>外部中断如下：</p>
<p>当外设发出中断信号后，信号被送入8259A；</p>
<p>8259A检查IMR寄存器中是否屏蔽了来自该IRQ的信号，若IMR寄存器中对应的位为1，表示屏蔽了IRQ代表的中断，则丢掉此中断信号，若IMR寄存器中对应的位为0，表示未屏蔽此中断，则将IRR寄存器中与此中断对应的位 置1。</p>
<p>PR优先级裁决器从IRR寄存器中挑选一个优先级最大的中断，然后8259A向CPU发送INTR信号。</p>
<p>内部中断由代码通过中断向量号触发。</p>
<p>2.(外)中断响应</p>
<p>CPU收到INTR信号后便知道有新的中断了，在执行完当前指令后，向8259A发送一个中断回复信号。</p>
<p>8259A收到回复信号后，将选出来的优先级最大的中断在ISR寄存器中相应的位 置1，表示该中断正在处理，同时将此中断在IRR寄存器中相应的位 置0，相当于将此中断从中断请求队列中去掉。</p>
<p>CPU再向8259A发送INTR信号，表示想要获取中断向量号。</p>
<p>8259A通过数据总线向CPU发送中断向量号，<strong>中断向量号 &#x3D; 起始向量号 + IRQ接口号</strong>，一般起始向量号为32，从中断向量表可看出0—31已经被占用，后面的32—127是分配给可屏蔽中断的，所以此处外设的中断设置的起始向量号便为32。</p>
<p>3.保护现场__压栈</p>
<p>CPU据中断向量号去IDT中获取中断描述符，取出选择子中的DPL与当前特权级CPL进行比较，若特权级发生变化，则需要切换栈（不同特权级有着不同的栈，如Linux使用了0， 3特权级，则有两个栈，一个内核栈，一个用户栈）。</p>
<p>程序跳转至中断服务程序，进入内核态。</p>
<p>处理器将当前的旧栈SS和ESP的值压入中断栈(中断栈位于当前进程的PCB页中内核栈的最顶端，<strong>每个进程都拥有独立的内核栈</strong>)，从TSS（每一个任务有一个TSS结构，其中保存着不同特权级栈的SS和ESP值）中获取与DPL特权级同的栈信息加载到SS和ESP寄存器。再将旧栈SS和ESP的值压入新栈中。若没有特权级变化，则跳过此步骤。</p>
<p>向中断栈压入程序状态信息，即EFLAGS寄存器</p>
<p>向中断栈压入断点，即返回地址，即当前任务的CS，EIP值。</p>
<p>若该中断有错误码，压入错误码</p>
<p>图示为发生了特权级改变时的压栈情况</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240421154614894.png" class="" title="image-20240421154614894">

<p>4.定位中断服务程序</p>
<p>访问中断处理程序的流程</p>
<p>1.通过idtr获得idt的地址，将中断向量号作为索引，访问到中断描述符</p>
<p>2.中断描述符给出<strong>一阶中断处理程序</strong>的段选择子和偏移</p>
<p>3.通过gdtr获得gdt的地址，将段选择子作为索引，访问到段描述符</p>
<p>4.段描述符给出<strong>一阶中断处理程序</strong>的段<strong>虚拟地址</strong>，</p>
<p>5.通过段<strong>虚拟地址</strong>和偏移访问到<strong>一阶中断处理程序</strong>，</p>
<p>一阶中断处理程序进行上下文的保存并call<strong>二阶中断处理程序</strong>(真正的中断处理程序)，并在<strong>二阶中断处理程序</strong>执行完成后进行后进行上下文的恢复和返回</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240417110140482.png" class="" title="image-20240417110140482">



<p>5.中断处理过程</p>
<p>中断的实际处理过程就是执行中断处理程序，Linux将中断处理程序分为上下两部分，需要紧急处理立即执行的归为上半部，不那么紧急的归为下半部。</p>
<p>开中断，即EFLAGS的IF位置1，表示允许响应中断；关中断，即EFLAGS的IF位置0，表示不允许响应中断。</p>
<p>上半部分是刻不容缓的，需要立即执行的部分，所以要在关中断的状态下执行。</p>
<p>而下半部分不那么紧急，在开中断的情况下进行，如果此时有新的中断发生，当前中断处理程序便会换下CPU，CPU会另寻时间重新调度，完成整个中断处理程序。</p>
<p>6.中断返回——出栈</p>
<p>中断返回就是出栈的过程，将第三步保护现场压入栈中的信息弹出。</p>
<p>有错误码弹出错误码。</p>
<p>此时的栈顶指针ESP应指向EIP_old，剩余栈中的信息使用iret指令弹出，CPU执行到iret指令时再次检查和比较特权级是否变化。</p>
<p>弹出EIP_old, CS_old</p>
<p>若特权级变化，将ESP_old, SS_old, 加载到ESP，SS寄存器。</p>
<h3 id="中断处理程序"><a href="#中断处理程序" class="headerlink" title="中断处理程序"></a>中断处理程序</h3><p>由kernel实现中断处理程序</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (vec_nr == <span class="number">0x27</span> || vec_nr == <span class="number">0x2f</span>) &#123;</span><br><span class="line">    <span class="comment">// 伪中断，无需处理</span></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在屏幕的左上角打印异常信息</span></span><br><span class="line">set_cursor(<span class="number">0</span>);</span><br><span class="line"><span class="type">int</span> cursor_pos = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (cursor_pos &lt; <span class="number">320</span>) &#123;</span><br><span class="line">    put_char(<span class="string">&#x27; &#x27;</span>);</span><br><span class="line">    ++cursor_pos;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">set_cursor(<span class="number">0</span>);</span><br><span class="line">put_str(<span class="string">&quot;---------------Exception message:\n&quot;</span>);</span><br><span class="line">set_cursor(<span class="number">88</span>);</span><br><span class="line">put_str(intr_name[vec_nr]);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (vec_nr == <span class="number">14</span>) &#123;</span><br><span class="line">    <span class="comment">// Pagefault，打印缺失的地址</span></span><br><span class="line">    <span class="type">int</span> page_fault_vaddr = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">asm</span> (<span class="string">&quot;movl %%cr2, %0&quot;</span> : <span class="string">&quot;=r&quot;</span> (page_fault_vaddr));</span><br><span class="line">    put_str(<span class="string">&quot;\nPage fault address is: &quot;</span>);</span><br><span class="line">    put_int(page_fault_vaddr);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">put_str(<span class="string">&quot;\n---------------Exception message end.\n&quot;</span>);</span><br><span class="line"><span class="comment">// 不再继续向下执行，以便于查看异常信息</span></span><br><span class="line"><span class="keyword">while</span> (<span class="number">1</span>);</span><br></pre></td></tr></table></figure>



<h2 id="进程与线程"><a href="#进程与线程" class="headerlink" title="进程与线程"></a>进程与线程</h2><h3 id="TSS"><a href="#TSS" class="headerlink" title="TSS"></a>TSS</h3><p>TR(Task Register)保存TSS虚拟地址</p>
<p>Task State Segment	</p>
<p>TSS描述符保存于GDT</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240506191146645.png" class="" title="image-20240506191146645">

<p>TSS内容</p>
<p>保存任务快照（任务被换上&#x2F;换下CPU时）</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240506191245406.png" class="" title="image-20240506191245406">

<img src="/2024/04/12/Op-sys-Learning-Record/image-20240428111029824.png" class="" title="image-20240428111029824">

<p>Linux系统不通过切换TSS实现任务切换</p>
<p>每个CPU上的所有任务共享一个TSS，并由该CPU的TR保存；</p>
<p>进程切换时，改变TSS中SS0和esp0的值，任务上下文压入<strong>内核中断栈</strong>保存。</p>
<p>ss0, esp0对应0特权级的段选择子和偏移量，ss3, esp3对应3特权级的段选择子和偏移量（实际上不记录），Linux系统仅使用该部分。</p>
<p>所有进程的代码段，数据段等描述符均保存在GDT中</p>
<h3 id="PCB"><a href="#PCB" class="headerlink" title="PCB"></a>PCB</h3><p>进程&#x3D;线程+资源</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240424100100197.png" class="" title="image-20240424100100197">

<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>self_kstack</td>
<td>内核栈指针</td>
</tr>
<tr>
<td>pid</td>
<td></td>
</tr>
<tr>
<td>status</td>
<td>线程状态</td>
</tr>
<tr>
<td>name</td>
<td>线程名</td>
</tr>
<tr>
<td>priority</td>
<td>优先级</td>
</tr>
<tr>
<td>tick</td>
<td>每次在处理器上运行的时钟数</td>
</tr>
<tr>
<td>elapsed_tick</td>
<td>已使用时钟数</td>
</tr>
<tr>
<td>general_tag</td>
<td>线程在队列上的tag</td>
</tr>
<tr>
<td>all_list_tag</td>
<td>线程队列</td>
</tr>
<tr>
<td>pgdir</td>
<td>页表</td>
</tr>
<tr>
<td>userprog_vaddr</td>
<td>用户进程虚拟地址池</td>
</tr>
<tr>
<td>u_block_desc[DESC_CNT]</td>
<td>用户内存块描述符表</td>
</tr>
<tr>
<td>stack_magic</td>
<td>栈溢出标记，位于内核栈下方</td>
</tr>
</tbody></table>
<h3 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h3><p>进程创建过程</p>
<p>kernel启动进程，传入进程参数（进程文件名和进程名），启动进程。</p>
<p>1.初始化线程</p>
<p>从内核空间申请页面，清洗页面，初始化线程名为线程&#x2F;进程名；</p>
<p>根据线程&#x2F;进程名初始化线程状态；</p>
<p>初始化优先级，时间片上限，已使用时间片，页目录表地址，内核栈指针，魔数等；</p>
<p>加入线程队列，线程指针thread指向该页面底部；</p>
<p>2.创建用户进程的虚拟地址池</p>
<p>用户进程虚拟地址池位于PCB中</p>
<p>设置虚拟地址池起始于0x00000000</p>
<p>设置虚拟地址池所管理的空间的页数和虚拟地址池长度</p>
<p>3.初始化内核栈</p>
<p>线程内核栈位置为PCB页面顶部（高地址），thread-&gt;self_kstack为线程内核栈指针；</p>
<p>下移thread-&gt;self_kstack，留出中断栈空间sizeof(intr_stack)</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240425101059850.png" class="" title="image-20240425101059850">

<p>下移thread-&gt;self_kstack，留出线程栈空间sizeof(thread_stack)</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240425101121206.png" class="" title="image-20240425101121206">



<p>创建线程栈指针kthread_stack，指向当前thread-&gt;self_kstack指针位置，</p>
<p>初始化线程栈，依次写入eip，函数名，函数参数和寄存器初始值ebp,ebx,esi,edi；</p>
<p>thread-&gt;self_kstack此时位于线程栈栈顶，栈顶内容为edi，esi，ebx，ebp，函数参数，函数名，eip；</p>
<p>&#x2F;&#x2F;不执行ret指令，将压栈的函数参数，函数名和返回地址弹出，并前往执行对应函数；</p>
<p>4.创建用户进程的页目录表</p>
<p>从内核分配一页作为进程页目录表</p>
<p>将内核页目录表的高半部分（对应内核空间部分）项复制到进程页目录表中</p>
<p>设置最后一项页表的地址为页目录表地址</p>
<p>4.5 初始化用户内存块表</p>
<p>5.加入调度链表</p>
<p>关中断；</p>
<p>将进程加入调度链表；</p>
<p>开中断；</p>
<p>进程执行</p>
<p>构建用户进程初始上下文信息</p>
<p>移动线程栈指针kthread_stack指向intr_stack顶部，线程栈底部</p>
<p>写入中断栈，依次写入edi, esi, ebp, esp_dummy</p>
<p>ebx, edx, ecx, eax, gs, </p>
<p>ds, es, fs, eip, cs, eflag, esp, ss, 此类寄存器中值均为用户空间，为通过中断返回的方式进入3特权级做准备</p>
<p>thread-&gt;self_kstack此时位于中断栈栈顶，栈顶内容为：</p>
<p>vec_no, edi, esi, ebp, esp_dummy……</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240507185615764.png" class="" title="image-20240507185615764">



<p>将esp设置为中断栈栈顶指针thread-&gt;self_kstack，</p>
<p>jmp intr_exit，跳过压入的中断向量号vec_no，popad弹出所有通用寄存器值，pop gs，fs，es，ds，跳过错误码；此时栈顶为：eip值为filename（指向用户进程入口地址），cs值为SELECTOR_U_CODE，eflag为(EFLAGS_IOPL_0 | EFLAGS_MBS | EFLAGS_IF_1);</p>
<p>前往执行对应函数，进入特权级3；</p>
<h3 id="基于线程的调度"><a href="#基于线程的调度" class="headerlink" title="基于线程的调度"></a>基于线程的调度</h3><img src="/2024/04/12/Op-sys-Learning-Record/image-20240608144715684.png" class="" title="image-20240608144715684">

<p>每一个线程均拥有一个PCB，PCB是调度的基础</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240507170811516.png" class="" title="image-20240507170811516">

<p>1.发生中断时，CPU根据中断向量号和IDTR给出中断入口程序的段选择子和偏移：段选择子-&gt;GDT-&gt;页表+偏移得到物理地址；</p>
<p>2.中断入口程序保护线程上下文，将寄存器ds, es, fs, gs压入当前线程的中断栈，根据中断向量号前往中断服务程序</p>
<p>3.中断服务程序提供服务：进行线程调度</p>
<p>此时已内核，中断服务程序进行判断：若时间片耗尽，将该线程加入就绪队列；若未耗尽，时间片自减；</p>
<p>根据调度链表获得下一个线程，修改CR3为该线程所属进程的页表物理地址<a href="%E6%AD%A4%E6%97%B6%E9%A1%B5%E8%A1%A8%E5%B7%B2%E7%BB%8F%E4%BB%8E%E5%8E%9F%E7%BA%BF%E7%A8%8B%E5%88%87%E6%8D%A2%E4%B8%BA%E4%B8%8B%E4%B8%80%E7%BA%BF%E7%A8%8B%EF%BC%8C%E4%BD%86%E7%94%B1%E4%BA%8E%E6%89%80%E6%9C%89%E8%BF%9B%E7%A8%8B%E5%85%B1%E4%BA%AB%E5%86%85%E6%A0%B8%E7%A9%BA%E9%97%B4%EF%BC%8C%E6%95%85%E9%A1%B5%E8%A1%A8%E7%9A%84%E5%88%87%E6%8D%A2%E4%B8%8D%E5%BD%B1%E5%93%8D%E5%86%85%E6%A0%B8%E7%A9%BA%E9%97%B4%E4%B8%AD%E5%AF%B9PCB%E7%9A%84%E6%93%8D%E4%BD%9C">^注</a></p>
<p>4.开始任务切换</p>
<p>中断服务程序call函数swith_to</p>
<p>swith_to函数保护内核上下文，将寄存器esi, edi, ebx, ebp压入被中断线程PCB的内核栈，将当前栈指针esp写入被中断线程PCB的self_kstack；</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240507173112800.png" class="" title="image-20240507173112800">

<p>将下一个线程的self_kstack写入esp，切换内核栈从原线程到下一线程，将下一线程内核栈中的寄存器值esi, edi, ebx, ebp出栈，根据栈顶返回地址执行ret指令；</p>
<p>5.ret执行返回至中断入口程序，中断入口程序jmp至intr_exit，将下一线程的上下文恢复至寄存器中，</p>
<p>&#x2F;&#x2F;4.返回中断入口程序，中断入口程序恢复原线程上下文</p>
<p>&#x2F;&#x2F;5.中断返回程序</p>
<h2 id="特权级"><a href="#特权级" class="headerlink" title="特权级"></a>特权级</h2><p>CPL(Current Privilege Level)  当前代码特权级</p>
<p>为当前运行代码段的特权级，保存于CS寄存器中段选择子的RPL(2位)处</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240416193845119.png" class="" title="image-20240416193845119">

<p>eg: 运行用户程序代码，CPL &#x3D; 3</p>
<p>​	  运行内核程序代码，CPL &#x3D; 0</p>
<p>RPL(Request Privilege Level) 访问者特权级</p>
<p>访问者代码所在段的描述符的DPL为RPL值</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240416192952163.png" class="" title="image-20240416192952163">

<p>DPL(Descriptor Privilege Level) 受访者特权级</p>
<p>受访者代码所在段的描述符的DPL</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240416192952163.png" class="" title="image-20240416192952163">

<h3 id="门描述符"><a href="#门描述符" class="headerlink" title="门描述符"></a>门描述符</h3><img src="/2024/04/12/Op-sys-Learning-Record/image-20240417104649804.png" class="" title="image-20240417104649804">

<p>门描述符含有DPL，访问者的权限需大于等于门描述符的权限</p>
<p>eg: 访问者RPL&#x3D;3, 门描述符DPL&#x3D;3</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240417104649804.png" class="" title="image-20240417104649804">

<p>门描述符给出目标中断程序的段描述符选择子，</p>
<p>段描述符选择子含有DPL，门描述符的特权小于被访问者(中断程序)的权限</p>
<p>eg: 门描述符DPL&#x3D;3，中断程序段描述符DPL&#x3D;0</p>
<h3 id="访问过程"><a href="#访问过程" class="headerlink" title="访问过程"></a>访问过程</h3><img src="/2024/04/12/Op-sys-Learning-Record/image-20240506164402606.png" class="" title="image-20240506164402606">



<h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><p>物理内存被划分为内核内存池和用户内存池</p>
<p>所有程序（用户程序和内核）存在虚拟地址池，记录虚拟内存使用情况</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240425160621730.png" class="" title="image-20240425160621730">

<h3 id="内存管理启动"><a href="#内存管理启动" class="headerlink" title="内存管理启动"></a>内存管理启动</h3><p>Linux</p>
<p>内存池创建</p>
<p>Linux不对主内存(物理内存)进行内核使用和用户使用的划分；</p>
<p>所有物理内存的申请均在主内存实现</p>
<p>tiny_os</p>
<p>内存池创建：</p>
<p>1.kernel创建内核内存池和用户内存池，两个池均位于物理地址0x0009a000；分别管理物理内存的0x00102000~?和0x?(大小为物理内存剩下的一半)</p>
<p>2.kernel创建kernel的虚拟地址池，位于kbm和ubm之后；管理0xc0100000~?(大小同内核内存池)的虚拟地址</p>
<p>3.初始化内核内存池，用户内存池，虚拟地址池，</p>
<p>申请内存过程：</p>
<p>1.在虚拟地址池中申请连续虚拟内存</p>
<p>扫描虚拟地址池位图，判断是否可申请对应页数；若允许，修改位图并返回页<strong>起始虚拟地址</strong>。</p>
<p>2.在给定的物理内存池中逐一申请物理页(不要求连续)</p>
<p>判断需要在哪个物理池中进行申请；扫描物理地址池位图，修改位图并返回单页<strong>起始物理地址</strong>，重复。</p>
<p>3.通过页表建立虚拟页与物理页的映射关系.</p>
<p>将多页虚拟地址差分为单页，传入单页虚拟地址的和单页物理地址；</p>
<p>将虚拟地址高10位作为页目录索引v_pdi，中10位作为页表索引v_pti；</p>
<p>若页目录索引v_pdi处存在PDE，使用单页物理地址构造页表项PTE，写入PDE指向的页表的页表索引v_pti处；</p>
<p>若页目录索引v_pdi处为空，在内核中申请一个物理页作为页表，使用单页物理地址构造页表项PTE，写入PDE指向的页表的页表索引v_pti处；</p>
<p>申请过程中会将所申请到的页清理干净</p>
<h2 id="字符设备与块设备"><a href="#字符设备与块设备" class="headerlink" title="字符设备与块设备"></a>字符设备与块设备</h2><p>在 Linux 系统中，硬件设备被抽象为文件，称为设备文件。这些文件通常位于 &#x2F;dev 目录下，用户可以通过文件操作函数来访问硬件设备</p>
<h3 id="字符设备"><a href="#字符设备" class="headerlink" title="字符设备"></a>字符设备</h3><p>字符设备是一种以字节流的方式进行数据传输的设备，它每次只传输一个字节的数据，并且数据传输没有缓存</p>
<ul>
<li>键盘、鼠标：每次按键或移动鼠标都会产生一个字符流。</li>
<li>串口、并口：数据以字节流的形式进行传输。</li>
<li>终端设备 (tty, Teletypewriter)：用于与用户交互，每次输入或输出一个字符。</li>
</ul>
<h3 id="块设备"><a href="#块设备" class="headerlink" title="块设备"></a>块设备</h3><p>块设备是一种以数据块为单位进行数据传输的设备，它可以随机访问数据块，并且数据传输通常有缓存</p>
<ul>
<li>硬盘、SSD：数据以块的形式存储和读取。</li>
<li>USB 存储设备：例如 U 盘、移动硬盘等。</li>
<li>CD&#x2F;DVD 光驱：数据以扇区 (sector) 为单位存储，每个扇区通常为 512 字节。</li>
</ul>
<h2 id="kernel程序"><a href="#kernel程序" class="headerlink" title="kernel程序"></a>kernel程序</h2><h3 id="VX6-kernel程序内容"><a href="#VX6-kernel程序内容" class="headerlink" title="VX6 kernel程序内容"></a>VX6 kernel程序内容</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kinit：设置好页表分配器（page allocator）</span><br><span class="line"></span><br><span class="line">kvminit：为kernel分配页表，将低地址设备映射到kernel地址空间（即在kernel页表中写入PTE指向设备）</span><br><span class="line"></span><br><span class="line">kvminithart：页表开始生效，程序计数器将会通过MMU翻译为物理地址</span><br><span class="line"></span><br><span class="line">processinit：设置好初始进程或者说设置好进程表单</span><br><span class="line"></span><br><span class="line">trapinit/trapinithart：设置好user/kernel mode转换代码</span><br><span class="line"></span><br><span class="line">plicinit/plicinithart：设置好中断控制器PLIC（Platform Level Interrupt Controller），我们后面在介绍中断的时候会详细的介绍这部分，这是我们用来与磁盘和console交互方式</span><br><span class="line"></span><br><span class="line">binit：分配buffer cache</span><br><span class="line"></span><br><span class="line">iinit：初始化inode缓存</span><br><span class="line"></span><br><span class="line">fileinit：初始化文件系统</span><br><span class="line"></span><br><span class="line">virtio_disk_init：初始化磁盘</span><br><span class="line"></span><br><span class="line">userinit：最后当所有的设置都完成了，操作系统也运行起来了，会通过userinit运行第一个进程</span><br></pre></td></tr></table></figure>



<h3 id="tiny-os-kernel程序内容"><a href="#tiny-os-kernel程序内容" class="headerlink" title="tiny-os kernel程序内容"></a>tiny-os kernel程序内容</h3><p>loader程序与kernel程序对比</p>
<p>loader为纯二进制程序 ，仅包含机器码，根据机器码内容装载至内存后直接开始运行；</p>
<p>kernel为elf文件，存在文件头，节等内容，在进行segment展开后成为可运行的机器码</p>
<img src="/2024/04/12/Op-sys-Learning-Record/image-20240421144651548.png" class="" title="image-20240421144651548">



<p>kernel代码</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//interrupt.c</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">idt_init</span><span class="params">()</span> &#123;</span><br><span class="line">    idt_desc_init(); <span class="comment">//创建门描述符，写入属性和一阶中断程序入口地址，填入idt</span></span><br><span class="line">    exception_handler_init();</span><br><span class="line">    <span class="comment">//异常处理函数初始化，创建中断处理函数，使一阶中断程序有可用调用的函数</span></span><br><span class="line">    pic_init(); <span class="comment">//初始化主片8259A</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将idt地址写入idtr</span></span><br><span class="line">    <span class="type">uint64_t</span> idt_operand = ((<span class="keyword">sizeof</span>(idt) - <span class="number">1</span>) | ((<span class="type">uint64_t</span>) ((<span class="type">uint32_t</span>) idt &lt;&lt; <span class="number">16</span>)));</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span> <span class="params">(<span class="string">&quot;lidt %0&quot;</span> : : <span class="string">&quot;m&quot;</span> (idt_operand))</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//timer.c</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">timer_init</span><span class="params">()</span> &#123;</span><br><span class="line">    frequency_set(COUNTER0_PORT, COUNTER0_NO, READ_WRITE_LATCH, COUNTER_MODE, COUNTER0_VALUE);<span class="comment">//设置8253发中断的周期</span></span><br><span class="line">    register_handler(<span class="number">0x20</span>, intr_timer_handler);</span><br><span class="line">    <span class="comment">//触发0x20中断时调用intr_timer_handler函数，用于进行调度</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//memory.c</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">mem_init</span><span class="params">(<span class="type">void</span>)</span> &#123;</span><br><span class="line">    <span class="type">uint32_t</span> total_memory = (*(<span class="type">uint32_t</span>*) (<span class="number">0xb00</span>));</span><br><span class="line">    mem_pool_init(total_memory);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//init.c</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">init_all</span><span class="params">()</span> &#123;</span><br><span class="line">    idt_init();</span><br><span class="line">    timer_init</span><br><span class="line">    <span class="title function_">mem_init</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//thread.h</span></span><br><span class="line"><span class="keyword">struct</span> task_struct* <span class="title function_">thread_start</span><span class="params">(<span class="type">char</span>* name, <span class="type">int</span> prio, thread_func function, <span class="type">void</span>* func_args)</span> &#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span>* <span class="title">thread</span> =</span> get_kernel_pages(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">//此为内核拉起的内核线程，故申请内核页作为PCB</span></span><br><span class="line"></span><br><span class="line">    init_thread(thread, name, prio);</span><br><span class="line">    <span class="comment">//将申请到的内核页清空并写入PCB中的属性</span></span><br><span class="line">    </span><br><span class="line">    thread_create(thread, function, func_args);</span><br><span class="line">    <span class="comment">//初始化PCB中的栈，创建中断栈和线程栈，压入线程需要执行的函数的相关数据和寄存器初始值</span></span><br><span class="line"></span><br><span class="line">    ASSERT(!list_find(&amp;thread_ready_list, &amp;thread-&gt;general_tag));</span><br><span class="line">    list_append(&amp;thread_ready_list, &amp;thread-&gt;general_tag);</span><br><span class="line">    <span class="comment">//将线程加入就绪队列，general_tag为就绪队列/运行队列/阻塞队列等使用</span></span><br><span class="line"></span><br><span class="line">    ASSERT(!list_find(&amp;thread_all_list, &amp;thread-&gt;all_list_tag));</span><br><span class="line">    list_append(&amp;thread_all_list, &amp;thread-&gt;all_list_tag);</span><br><span class="line">    <span class="comment">//将线程加入系统线程队列，all_list_tag为系统线程队列使用</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span> <span class="params">(<span class="string">&quot;movl %0, %%esp; pop %%ebp; pop %%ebx; pop %%edi; pop %%esi; ret&quot;</span> : : <span class="string">&quot;g&quot;</span> (thread-&gt;self_kstack) : <span class="string">&quot;memory&quot;</span>)</span>;</span><br><span class="line">    <span class="comment">//将压栈的寄存器初始值弹栈，ret将线程需要执行的函数的相关数据弹栈并前往执行</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> thread;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//main.c</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">void</span>)</span> &#123;</span><br><span class="line">    init_all();</span><br><span class="line">    thread_start(<span class="string">&quot;k_thread_1&quot;</span>, <span class="number">31</span>, k_thread_function, <span class="string">&quot;skywalker &quot;</span>);</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>operation system</category>
      </categories>
      <tags>
        <tag>operation system</tag>
      </tags>
  </entry>
  <entry>
    <title>pic_test</title>
    <url>/2024/04/12/pic-test/</url>
    <content><![CDATA[<p>图片插入测试：</p>
<img src="/2024/04/12/pic-test/img.png" class="">]]></content>
  </entry>
  <entry>
    <title>第一篇博客</title>
    <url>/2024/04/11/first_post/</url>
    <content><![CDATA[<p>写一个测试试一下</p>
<p>数学公式测试：$r_n&#x3D;W_n\log_2\left(1+\frac{P_nd_n^{-\beta}h_n^2}{N_0+I_n}\right)$</p>
<p>$$t_n^{loc}&#x3D;\frac{(1-\rho_n)C_n}{f_n}$$</p>
]]></content>
  </entry>
  <entry>
    <title>reinforce-learning-record</title>
    <url>/2024/06/10/reinforce-learning-record/</url>
    <content><![CDATA[<p>reinforce learning</p>
<span id="more"></span>

<h1 id="reinforce-learning-record"><a href="#reinforce-learning-record" class="headerlink" title="reinforce learning record"></a>reinforce learning record</h1><h2 id="MRP"><a href="#MRP" class="headerlink" title="MRP"></a>MRP</h2><img src="/2024/06/10/reinforce-learning-record/image-20240610205131367.png" class="" title="image-20240610205131367">

<h3 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h3><p>在一个马尔可夫奖励过程中，从第t时刻状态S_t开始，直到终止状态时，所有奖励的衰减之和称为<strong>回报</strong>（Return）</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610203108806.png" class="" title="image-20240610203108806">

<ul>
<li>Rt表示在时刻t获得的奖励</li>
<li><strong>到达状态Si</strong>，得到奖励r(s)；若在时刻t到达状态Si，则<strong>Rt&#x3D;r(Si)</strong></li>
<li>对于某状态序列<img src="/2024/06/10/reinforce-learning-record/image-20240610203423677.png" class="" title="image-20240610203423677">我们有对应于该状态序列的奖励G<img src="/2024/06/10/reinforce-learning-record/image-20240610203434567.png" class="" title="image-20240610203434567"></li>
</ul>
<h3 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h3><p>在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的<strong>价值</strong>（value）。所有状态的价值就组成了<strong>价值函数</strong>（value function），价值函数的输入为某个状态，输出为这个状态的价值。</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610203509585.png" class="" title="image-20240610203509585">

<ul>
<li>对于某个状态s，从该状态出发可得到的状态序列有很多条</li>
<li>对于某个状态s，价值函数为<strong>从该状态出发，可能存在的所有状态序列的奖励的均值</strong></li>
</ul>
<p>贝尔曼方程</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610203841177.png" class="" title="image-20240610203841177">

<h2 id="MDP"><a href="#MDP" class="headerlink" title="MDP"></a>MDP</h2><p>与MRP相比，奖励与状态和动作相关</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610205112917.png" class="" title="image-20240610205112917">

<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>策略表示在状态s下采取动作a的<strong>概率</strong>，概率可能为1</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610204522539.png" class="" title="image-20240610204522539">

<h3 id="状态价值函数"><a href="#状态价值函数" class="headerlink" title="状态价值函数"></a>状态价值函数</h3><p>在 MDP 中基于策略Π的状态价值函数（state-value function），定义为从状态s出发遵循策略Π能获得的期望回报</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610204014373.png" class="" title="image-20240610204014373">

<h3 id="动作价值函数"><a href="#动作价值函数" class="headerlink" title="动作价值函数"></a>动作价值函数</h3><p>在 MDP 中，由于动作的存在，我们额外定义一个<strong>动作价值函数</strong>（action-value function）。表示在 MDP 遵循策略Π时，对当前状态s执行动作a得到的期望回报</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610204152914.png" class="" title="image-20240610204152914">

<ul>
<li><strong>采取动作a到达状态s</strong>，得到奖励R</li>
</ul>
<h3 id="状态价值函数与动作价值函数关系"><a href="#状态价值函数与动作价值函数关系" class="headerlink" title="状态价值函数与动作价值函数关系"></a>状态价值函数与动作价值函数关系</h3><img src="/2024/06/10/reinforce-learning-record/image-20240610213003398.png" class="" title="image-20240610213003398">

<ul>
<li>从状态s出发，可能采取动作a1, a2, a3, …;对于每个可能采取的动作，均有动作价值Q， 状态价值为所有动作价值的和</li>
</ul>
<img src="/2024/06/10/reinforce-learning-record/image-20240610213050168.png" class="" title="image-20240610213050168">

<ul>
<li>在状态s执行动作a后，得到奖励r(s,a)；由于可能到达多个不同的状态s’，动作价值为即时奖励r(s,a)+可能到达的所有状态的价值的期望</li>
</ul>
<h3 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h3><p>采取策略Π</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610204627264.png" class="" title="image-20240610204627264">

<ul>
<li>对于状态s，s的状态价值为 ( 折损的 <strong>所有可达的下一状态的价值的期望</strong> + <strong>到达所有可达的下一状态的r(s’,a)的期望</strong> )</li>
<li>对于状态s和动作a，在状态s进行动作a的价值为 ( <strong>在状态s执行动作a的r(s,a)的期望</strong>(实际上就是r(s,a)) + 折损的 <strong>在执行动作a后所有可达的下一状态的所有可执行动作的价值的期望</strong> )</li>
</ul>
<img src="/2024/06/10/reinforce-learning-record/shiow.PNG" class="" title="shiow">

<h3 id="贝尔曼最优方程"><a href="#贝尔曼最优方程" class="headerlink" title="贝尔曼最优方程"></a>贝尔曼最优方程</h3><p>存在策略*，使对于任意的状态s，均有基于策略*状态价值函数大于基于策略Π的状态价值函数</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610205402396.png" class="" title="image-20240610205402396">

<ul>
<li>对于状态价值：非最优形式是求期望，最优形式是直接选取当前状态，所有动作中，未来状态序列中价值最大的</li>
<li>对于动作价值：非最优形式是求期望，最优形式是直接选取下一状态，所有动作中，未来状态序列中动作价值最大的</li>
</ul>
<h2 id="策略迭代算法-基于策略函数的"><a href="#策略迭代算法-基于策略函数的" class="headerlink" title="策略迭代算法&#x2F;基于策略函数的"></a>策略迭代算法&#x2F;基于策略函数的</h2><p>策略评估：</p>
<p>基于当前策略Π，在已知状态转移函数的情况下（即我们可以知道采取动作a后有多高的几率到达哪个状态），使用<strong>贝尔曼期望方程</strong>迭代更新状态价值函数</p>
<p>ps: 原文中迭代更新状态价值函数是通过</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610213050168.png" class="" title="image-20240610213050168">

<img src="/2024/06/10/reinforce-learning-record/image-20240610213003398.png" class="" title="image-20240610213003398">

<p>以上两个公式实现的，也即</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">qsa_list = []</span><br><span class="line">	<span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 迷宫环境，四相运动</span></span><br><span class="line">		qsa = <span class="number">0</span>  <span class="comment"># 当前动作的动作价值</span></span><br><span class="line">        <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:  <span class="comment"># 有模型，即状态转移函数已知</span></span><br><span class="line">        	p, next_state, r, done = res</span><br><span class="line">            qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))  <span class="comment"># 第一个公式，计算该动作的价值(开头的p为动作成功概率)</span></span><br><span class="line">         qsa_list.append(self.pi[s][a] * qsa)  <span class="comment"># 乘采取该动作的几率</span></span><br><span class="line">	new_v[s] = <span class="built_in">sum</span>(qsa_list)  <span class="comment"># 第二个公式，根据所有动作的价值计算状态价值</span></span><br></pre></td></tr></table></figure>

<p>target: 更新<strong>状态价值函数</strong>，使之收敛</p>
<p>策略提升：</p>
<p>由于</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240610213003398.png" class="" title="image-20240610213003398">

<p>故修改策略，使得动作价值最大的动作以更高的概率(若最大值唯一，该动作将成为唯一被选取动作；若不唯一，这些动作将均分概率1)被选取，可使新策略下的状态价值函数增大，也即<img src="/2024/06/10/reinforce-learning-record/image-20240611150030644.png" class="" title="image-20240611150030644"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">qsa_list = []</span><br><span class="line">	<span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):  <span class="comment"># 迷宫环境，四相运动</span></span><br><span class="line">    	qsa = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:</span><br><span class="line">        	p, next_state, r, done = res</span><br><span class="line">            qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))</span><br><span class="line">        qsa_list.append(qsa)</span><br><span class="line">     maxq = <span class="built_in">max</span>(qsa_list)</span><br><span class="line">     cntq = qsa_list.count(maxq)  <span class="comment"># 计算有几个动作得到了最大的Q值</span></span><br><span class="line">     <span class="comment"># 让这些动作均分概率</span></span><br><span class="line">     self.pi[s] = [<span class="number">1</span> / cntq <span class="keyword">if</span> q == maxq <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> q <span class="keyword">in</span> qsa_list]</span><br></pre></td></tr></table></figure>

<p>target: 修改<strong>策略</strong>，使在新策略下，有基于新策略的状态价值函数大于原策略的状态价值函数</p>
<h2 id="价值迭代算法-基于价值函数的"><a href="#价值迭代算法-基于价值函数的" class="headerlink" title="价值迭代算法&#x2F;基于价值函数的"></a>价值迭代算法&#x2F;基于价值函数的</h2><p>使用贝尔曼最优方程</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611191853492.png" class="" title="image-20240611191853492">

<p>迭代形式</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611152610430.png" class="" title="image-20240611152610430">

<p>进行价值迭代；</p>
<p>使用</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611152643864.png" class="" title="image-20240611152643864">

<p>从迭代完成的状态价值函数中获取策略，即 从当前状态出发，哪个动作的 (即时奖励+下一状态价值) 最大，策略就为哪个动作</p>
<p>ps: 程序实际使用公式</p>
<p>最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611153610316.png" class="" title="image-20240611153610316">

<img src="/2024/06/10/reinforce-learning-record/image-20240610213050168.png" class="" title="image-20240610213050168">

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_diff = <span class="number">0</span></span><br><span class="line">new_v = [<span class="number">0</span>] * self.env.ncol * self.env.nrow  <span class="comment"># 迷宫环境，初始化状态价值</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.env.ncol * self.env.nrow):</span><br><span class="line">    qsa_list = []</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        qsa = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> res <span class="keyword">in</span> self.env.P[s][a]:</span><br><span class="line">            p, next_state, r, done = res</span><br><span class="line">            qsa += p * (r + self.gamma * self.v[next_state] * (<span class="number">1</span> - done))</span><br><span class="line">        qsa_list.append(qsa)  <span class="comment"># 记录状态s下的所有Q(s,a)价值</span></span><br><span class="line">     new_v[s] = <span class="built_in">max</span>(qsa_list)  <span class="comment"># 取最大作为新状态价值</span></span><br><span class="line">     max_diff = <span class="built_in">max</span>(max_diff, <span class="built_in">abs</span>(new_v[s] - self.v[s]))</span><br><span class="line">self.v = new_v</span><br><span class="line"><span class="keyword">if</span> max_diff &lt; self.theta: <span class="keyword">break</span>  <span class="comment"># 满足收敛条件,退出评估迭代</span></span><br><span class="line">cnt += <span class="number">1</span></span><br></pre></td></tr></table></figure>



<h2 id="有模型-无模型-在线策略-离线策略"><a href="#有模型-无模型-在线策略-离线策略" class="headerlink" title="有模型&amp;无模型+在线策略&amp;离线策略"></a>有模型&amp;无模型+在线策略&amp;离线策略</h2><p><strong>有模型强化学习：</strong></p>
<p>智能体学习环境的状态转移函数</p>
<p>环境的状态转移函数已知</p>
<p>智能体可以直接根据状态转移函数得到在对环境进行动作a后环境的下一状态</p>
<p># 状态转移函数P[state][action] &#x3D; [(p, next_state, reward, done)]包含转移成功概率，下一个状态，奖励和是否完成</p>
<ul>
<li>Dyna-Q</li>
<li>Monte Carlo Tree Search (MCTS)</li>
<li>PILCO (Probabilistic Inference for Learning Control)</li>
</ul>
<p><strong>无模型强化学习：</strong></p>
<p>智能体通过与环境交互学习状态和奖励之间的映射关系</p>
<p>环境的状态转移函数未知</p>
<p>智能体必须通过与环境的交互才能得到环境的下一状态</p>
<ul>
<li>Q-learning</li>
<li>SARSA</li>
<li>Deep Q-Network (DQN)</li>
<li>Policy Gradient methods (e.g., REINFORCE, A2C, PPO)</li>
</ul>
<p>样本：当前状态，下一状态，采取动作，奖励</p>
<p><strong>在线策略学习：</strong></p>
<p>不保存样本</p>
<p><strong>离线策略学习：</strong></p>
<p>使用经验回放池保存样本</p>
<h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h3><p>Value-based + online</p>
<p>从某状态s出发，基于策略Π，获得一条状态序列，该状态序列对应一个回报G；该过程为一次采样。</p>
<p>反复采样，得到N个状态序列+回报和M，由大数定律，可得该状态的状态价值</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611161858965.png" class="" title="image-20240611161858965">

<p>使用增量更新方法</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611161157050.png" class="" title="image-20240611161157050">

<p>补：增量更新原理</p>
<p>新均值 &#x3D; 旧均值 + 1&#x2F;总量 * (新值 - 旧均值) ？</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611161942958.png" class="" title="image-20240611161942958">

<h3 id="时序差分算法"><a href="#时序差分算法" class="headerlink" title="时序差分算法"></a>时序差分算法</h3><p>Value-based + online</p>
<p>类似于蒙特卡洛，更新状态s的状态价值时，不使用完整的状态序列，在得到下一状态时立即对状态s的状态价值进行更新</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611162513650.png" class="" title="image-20240611162513650">

<p>蒙特卡洛使用第三行对状态价值进行更新，时序差分使用第四行对状态价值进行更新</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611162624769.png" class="" title="image-20240611162624769">

<ul>
<li>只向前走了一步的蒙特卡洛</li>
<li>使用下一状态的状态价值代替了很长的状态序列的回报(状态价值本身就是所有未来时刻的回报期望和)</li>
<li>增量更新体现在 减号前部分为 (新 状态s的状态价值)，减号后部分为 (旧 状态s的状态价值)</li>
</ul>
<h3 id="Sarsa算法"><a href="#Sarsa算法" class="headerlink" title="Sarsa算法"></a>Sarsa算法</h3><p>Value-based + online</p>
<p>类似于时序差分算法，对动作价值进行更新</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611191221769.png" class="" title="image-20240611191221769">

<p>ε-greedy 选择 随机动作 or 最大动作价值-&gt;动作</p>
<h3 id="Q-learning算法"><a href="#Q-learning算法" class="headerlink" title="Q-learning算法"></a>Q-learning算法</h3><p>Value-based + offline</p>
<p>对动作价值进行更新</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611194333945.png" class="" title="image-20240611194333945">

<h3 id="DQN-DDQN"><a href="#DQN-DDQN" class="headerlink" title="DQN&#x2F;DDQN"></a>DQN&#x2F;DDQN</h3><p>Value-based + offline</p>
<p>将Q-learning的Q表换成net</p>
<h3 id="策略梯度算法-REINFORCE"><a href="#策略梯度算法-REINFORCE" class="headerlink" title="策略梯度算法(REINFORCE)"></a>策略梯度算法(REINFORCE)</h3><p>Policy-based + online</p>
<p>算法流程：</p>
<img src="/2024/06/10/reinforce-learning-record/image-20240611200059062.png" class="" title="image-20240611200059062">

<h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h3>]]></content>
      <tags>
        <tag>python, reinforce learning</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch_learning_record</title>
    <url>/2024/05/08/pytorch-learning-record/</url>
    <content><![CDATA[<p>dqn与pytorch</p>
<span id="more"></span>

<h1 id="pytorch-learning-record"><a href="#pytorch-learning-record" class="headerlink" title="pytorch_learning_record"></a>pytorch_learning_record</h1><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p>附带额外属性的标量，二维数组，多维数组…</p>
<p>主要属性</p>
<ul>
<li><p>T: dimensions reversed</p>
</li>
<li><p>H: 共轭转置</p>
</li>
<li><p>data: 值</p>
</li>
<li><p>grad_fn: 创建该Tensor所使用的函数，在子节点标记为requires_grad时存在</p>
</li>
<li><p>requires_grad: 该Tensor是否需要梯度，生成该Tensor的子节点若为true，该节点同为true</p>
</li>
<li><p>is_leaf: 指示是否叶子节点</p>
</li>
<li><p>dtype: 张量的数据类型，如 torch.FloatTensor，torch.cuda.FloatTensor</p>
</li>
<li><p>shape: 张量的形状。如 (64, 3, 224, 224)</p>
</li>
<li><p>device: 张量所在设备 (CPU&#x2F;GPU)</p>
</li>
<li><p>grad: 张量的梯度，该属性在进行backforward后被填充，非叶节点默认为None。节点的grad属性值在每次backward后进行值的追加</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.tensor(data, dtype=<span class="literal">None</span>, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>, pin_memory=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">arr = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">t= torch.tensor(arr)</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">1.</span>], requires_grad=<span class="literal">True</span>)  <span class="comment"># 1.的原因是Only Tensors of floating point and complex dtype can require gradients</span></span><br></pre></td></tr></table></figure>



<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><p>Add代表加法；</p>
<p>Sub代表减法；</p>
<p>Mul代表乘法；</p>
<p>Mm代表矩阵乘法；</p>
<p>Div代表除法；</p>
<p>T代表矩阵转置；</p>
<p>Pow代表乘方；</p>
<p>Squeeze, Unsqueeze, Relu, Sigmoid？</p>
<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>pytorch为动态图计算机制，在每一次反向传播计算梯度的循环内，pytorch先建立正向计算图，然后使用反向传播计算梯度，同时被销毁计算图</p>
<p>自动求导机制中只保存叶子节点的grad，中间tensor即使是requires_grad &#x3D; True在计算完成梯度后会自动释放以节省空间，不会保存，即在进行backward后 中间tensor.grad &#x3D; None</p>
<p>y &#x3D; (x + w) * (w + 1)</p>
<p>a &#x3D; x+w</p>
<p>b &#x3D; w + 1</p>
<img src="/2024/05/08/pytorch-learning-record/image-20240508155001152.png" class="" title="image-20240508155001152">



<p>通常</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">w = torch.tensor([<span class="number">1.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">x = torch.tensor([<span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># y=(x+w)*(w+1)</span></span><br><span class="line">a = torch.add(w, x)</span><br><span class="line">b = torch.add(w, <span class="number">1</span>)</span><br><span class="line">y = torch.mul(a, b)</span><br><span class="line"></span><br><span class="line">y.backward()  <span class="comment"># backward方法默认释放计算图</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="built_in">print</span>(a.grad)  <span class="comment"># 非叶子节点的梯度为空，可使用retain_grad()保留</span></span><br><span class="line"><span class="built_in">print</span>(b.grad)  <span class="comment"># 非叶子节点的梯度为空，可使用retain_grad()保留</span></span><br></pre></td></tr></table></figure>



<p>自动梯度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.autograd.grad(outputs,</span><br><span class="line">	inputs,grad_outputs=<span class="literal">None</span>,</span><br><span class="line">	retain_graph=<span class="literal">None</span>,create_graph=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># outputs对inputs求导</span></span><br><span class="line"><span class="comment"># create_graph: 创建计算图</span></span><br><span class="line"><span class="comment"># retain_graph: 保存计算图</span></span><br><span class="line"><span class="comment"># grad_outputs: 多梯度权重</span></span><br><span class="line">	</span><br><span class="line">x = torch.tensor([<span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.<span class="built_in">pow</span>(x, <span class="number">2</span>)     <span class="comment"># y = x**2</span></span><br><span class="line"></span><br><span class="line">grad_1 = torch.autograd.grad(y, x, create_graph=<span class="literal">True</span>)   <span class="comment"># grad_1 = dy/dx = 2x = 2 * 3 = 6</span></span><br><span class="line"><span class="built_in">print</span>(grad_1)</span><br><span class="line"></span><br><span class="line">grad_2 = torch.autograd.grad(grad_1[<span class="number">0</span>], x)              <span class="comment"># grad_2 = d(dy/dx)/dx = d(2x)/dx = 2，create_graph=True的作用在此，使grad_1的grad_fn属性存在，可以继续求导</span></span><br><span class="line"><span class="built_in">print</span>(grad_2)</span><br></pre></td></tr></table></figure>



<p>torchviz</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>],dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">w = torch.tensor([<span class="number">4</span>],dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([<span class="number">0.5</span>],dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">h = w*x + b</span><br><span class="line">graph_forward = make_dot(h)</span><br></pre></td></tr></table></figure>

<p>图中节点的说明（以下由 AI 生成，使用的模型为 ChatGPT ）：</p>
<ul>
<li>AccumulateGrad: 这个节点表示梯度的累积。在反向传播过程中，梯度是被累积的，因为在每一次反向传播调用中，梯度都会被计算并加到之前的梯度上，以便进行梯度下降更新。</li>
<li>TBackward0: 这个节点通常代表Tensor的反向传播操作。当在Tensor上进行某些操作并计算梯度时，这个节点将会出现，表示该Tensor的反向传播。</li>
<li>AddmmBackward0: 这是矩阵乘法操作的反向传播。在神经网络中，矩阵乘法常用于线性层（fully connected layer）的计算，而这个节点则表示反向传播的计算。</li>
<li>ConvolutionBackward0: 这是卷积操作的反向传播。在卷积神经网络中，卷积操作是一种常见的操作，这个节点表示卷积层的反向传播计算。</li>
<li>MaxPool2DWithIndicesBackward0: 这个节点是最大池化操作的反向传播。在最大池化层中，通过取池化窗口中的最大值来减小输入的空间大小，这个节点表示最大池化操作的反向传播。</li>
<li>ReluBackward0: 这是ReLU激活函数的反向传播。ReLU函数是一种常用的非线性激活函数，在深度学习中广泛使用。这个节点表示ReLU函数的反向传播计算。</li>
<li>ReshapeAliasBackward0: 这个节点是reshape操作的反向传播。当对张量进行reshape操作时，这个节点表示reshape操作的反向传播计算。</li>
</ul>
<img src="/2024/05/08/pytorch-learning-record/image-20240508181305285.png" class="" title="image-20240508181305285">



<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Optimizer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params, defaults</span>): </span><br><span class="line">        self.defaults = defaults</span><br><span class="line">        self.state = defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">        self.param_groups = []  <span class="comment"># 优化器所管理的参数组</span></span><br><span class="line">        </span><br><span class="line">        self.optimizer = </span><br><span class="line">        </span><br><span class="line">torch.optim.Adam(self.eval_net.parameters(), lr=LR) <span class="comment">#  lr为学习率</span></span><br></pre></td></tr></table></figure>



<p>optimizer.step()</p>
<p>优化器执行一次梯度更新</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight = torch.randn((<span class="number">2</span>, <span class="number">2</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">weight.grad = torch.ones((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD([weight], lr=<span class="number">0.1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weight before step:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.data))</span><br><span class="line">optimizer.step()        <span class="comment"># lr为0.1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weight after step:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(weight.data))</span><br></pre></td></tr></table></figure>

<p>即为weight.data - 0.1 * [[1, 1], [1, 1]]</p>
<p>optimizer.zero_grad()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">zero_grad</span>(<span class="params">self</span>):  <span class="comment"># 循环将所管理的tensor的grad清零</span></span><br><span class="line">    <span class="keyword">for</span> group <span class="keyword">in</span> self.param_groups:</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&#x27;params&#x27;</span>]:</span><br><span class="line">            <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                p.grad.detach_()</span><br><span class="line">                p.grad.zero_()</span><br></pre></td></tr></table></figure>





<h2 id="nn-Module"><a href="#nn-Module" class="headerlink" title="nn.Module"></a>nn.Module</h2><p>使用 Pytorch 的 nn.Module 建立网络时，其网络内部参数自动设置为 requires_grad&#x3D;True </p>
<img src="/2024/05/08/pytorch-learning-record/image-20240511103728550.png" class="" title="image-20240511103728550">

<p>而我们使用反向传播时，其实根据全连接层的偏导数计算公式，可知链式求导和 <em>w</em> ，<em>b</em> 的梯度无关，而与其中一个连接层的输出梯度有关，这也是为什么冻结了网络的参数，还是可以输出对输入求导。如下式：</p>
<img src="/2024/05/08/pytorch-learning-record/image-20240511100910357.png" class="" title="image-20240511100910357">



<h2 id="Method-and-Attr"><a href="#Method-and-Attr" class="headerlink" title="Method and Attr"></a>Method and Attr</h2><p>.requires_grad 属性</p>
<p>在pytorch中，tensor有一个requires_grad参数，如果设置为True，则反向传播时，该tensor若为叶节点则其grad值会被写入。</p>
<p> tensor的requires_grad的属性默认为False。</p>
<p>若一个节点requires_grad被设置为True，那么它的所有子节点requires_grad都为True （即使子节点的其他父节点的requires_grad &#x3D; False）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f = torch.tensor(<span class="number">5.</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># grad值为6</span></span><br><span class="line">g = torch.tensor(<span class="number">6.</span>, requires_grad=<span class="literal">False</span>) <span class="comment"># grad值为None</span></span><br><span class="line">r3 = f * g</span><br><span class="line">r3.backward()</span><br></pre></td></tr></table></figure>





<p>.grad_fn 属性</p>
<p>tensor在计算图中的父运算函数</p>
<p>.detach()与.detach_()</p>
<p>[.detach()]: <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html#torch.Tensor.detach">https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html#torch.Tensor.detach</a>	“Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks.”</p>
<p><code>.detach_()</code>: is the inplace operation of <code>detach()</code>.</p>
<p>相同：</p>
<p>新tensor的requires_grad为False，grad_fn为None，为<strong>叶节点</strong>；</p>
<p><strong>修改新tensor时原tensor同样发生改变；</strong></p>
<p>不同：</p>
<p>m.detach()后m不发生改变，原m在计算图中的父子关系不改变；</p>
<p>m.detach_()后m发生改变，原m在计算图中和父节点断开，子节点关系不改变;</p>
<p>#与父节点断开不影响梯度传导至叶子节点？？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># grad = 4</span></span><br><span class="line">b = torch.tensor(<span class="number">4.</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># grad = 2</span></span><br><span class="line">c = torch.tensor(<span class="number">6.</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># grad = 1 + 1</span></span><br><span class="line">m = a * b</span><br><span class="line">r1 = m + c</span><br><span class="line"></span><br><span class="line">m_ = m.detach()</span><br><span class="line"><span class="comment"># 执行后仅m_的requires_grad=False</span></span><br><span class="line">m_ = m.detach_()</span><br><span class="line"><span class="comment"># 执行后c和out均为requires_grad=False</span></span><br><span class="line"></span><br><span class="line">r2 = m_ + c</span><br><span class="line">r1.backward()</span><br><span class="line">r2.backward()</span><br></pre></td></tr></table></figure>









<p>torch.no_grad()</p>
<p>是一个上下文管理器，被该语句 wrap 起来的部分将不会track梯度。</p>
<p>with torch.no_grad()所包裹的语句或者@torch.no_grad()所修饰的函数中发生改变或新建的tensor的requires_grad为False</p>
<p>例外：所有factory function，或者创建一个新的tensor时显式指明requires_grad&#x3D;True，都不会受到这种模式的影响。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y = x * <span class="number">2</span>  <span class="comment"># requires_grad=False</span></span><br><span class="line">    z = torch.tensor([<span class="number">1.</span>], requires_grad=<span class="literal">True</span>) <span class="comment">#   requires_grad=True</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">doubler</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * <span class="number">2</span></span><br><span class="line">z = doubler(x)</span><br></pre></td></tr></table></figure>



<p>.backward()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([<span class="number">2.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">c = a**<span class="number">2</span></span><br><span class="line">d = b*c</span><br><span class="line">c.backward()  <span class="comment"># c.backward(retain_graph=True)</span></span><br><span class="line">d.backward()</span><br></pre></td></tr></table></figure>

<p>当在调用.backward()或autograd.grad()时，将释放图中保存的中间值。如果需要第二次向后遍历图，或者在调用backward后需要访问保存的张量，则指定retain_graph&#x3D;True。</p>
<p>.backward()方法仅适用于标量</p>
<p>.register_hook()</p>
<p><a href="https://oldpan.me/archives/pytorch-autograd-hook">https://oldpan.me/archives/pytorch-autograd-hook</a></p>
<p>获取中间tensor的grad值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">1.5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">c = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">m1 = a * b</span><br><span class="line">m1.register_hook(<span class="built_in">print</span>)  <span class="comment"># 控制台输出tensor(1.)</span></span><br><span class="line">r1 = m1 + c</span><br><span class="line">r1.backward()</span><br></pre></td></tr></table></figure>



]]></content>
      <tags>
        <tag>python, pytorch</tag>
      </tags>
  </entry>
</search>
