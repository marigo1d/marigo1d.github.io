<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"marigo1d.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="LLM相关">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM&amp;Rela">
<meta property="og:url" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/index.html">
<meta property="og:site_name" content="Marigold">
<meta property="og:description" content="LLM相关">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/8361e16bac5ee3235ef89c78b1a1cf6b.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250402161828313.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250402162106999.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325195407601.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325195428329.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250404192612286.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325195443397.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250404192637755.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325195459862.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250404193434062.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250404100511037.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250404195056421.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250404195113603.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250404195352067.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/v2-eaaf1c00d0c4ea350cd3a79b47de26d3_1440w.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/v2-2bcd98f6541da0b6f14dc9082ee2dcda_1440w.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325131654259.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325151928459.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325153214542.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325153304648.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325153337812.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325195333997.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325201341810.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325202638658.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325201533207.png">
<meta property="article:published_time" content="2025-03-19T02:50:14.000Z">
<meta property="article:modified_time" content="2025-04-04T11:53:54.004Z">
<meta property="article:author" content="marigo1d">
<meta property="article:tag" content="Python, Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/8361e16bac5ee3235ef89c78b1a1cf6b.png">

<link rel="canonical" href="https://marigo1d.github.io/2025/03/19/LLM-Rela/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>LLM&Rela | Marigold</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Marigold</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Salt, Pepper and Birds~</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://marigo1d.github.io/2025/03/19/LLM-Rela/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="marigo1d">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Marigold">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LLM&Rela
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-19 10:50:14" itemprop="dateCreated datePublished" datetime="2025-03-19T10:50:14+08:00">2025-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-04-04 19:53:54" itemprop="dateModified" datetime="2025-04-04T19:53:54+08:00">2025-04-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>27 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>LLM相关</p>
<span id="more"></span>
<h1 id="LLM-amp-Rela"><a href="#LLM-amp-Rela" class="headerlink" title="LLM&amp;Rela"></a>LLM&amp;Rela</h1><h2 id="Prompt-Engineering"><a href="#Prompt-Engineering" class="headerlink" title="Prompt Engineering"></a>Prompt Engineering</h2><h3 id="Zero-Shot-Prompting"><a href="#Zero-Shot-Prompting" class="headerlink" title="Zero-Shot Prompting"></a>Zero-Shot Prompting</h3><h3 id="Few-Shot-Prompting"><a href="#Few-Shot-Prompting" class="headerlink" title="Few-Shot Prompting"></a>Few-Shot Prompting</h3><h3 id="Chain-of-Thought-CoT-Prompting"><a href="#Chain-of-Thought-CoT-Prompting" class="headerlink" title="Chain-of-Thought (CoT) Prompting"></a>Chain-of-Thought (CoT) Prompting</h3><h2 id="LLM-base"><a href="#LLM-base" class="headerlink" title="LLM-base"></a>LLM-base</h2><h3 id="期望与方差"><a href="#期望与方差" class="headerlink" title="期望与方差"></a>期望与方差</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><strong>性质</strong></th>
<th style="text-align:center"><strong>期望 E[⋅]</strong></th>
<th style="text-align:center"><strong>方差 Var(⋅)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>线性变换</strong></td>
<td style="text-align:center">$E[aX+b]=aE[X]+b$</td>
<td style="text-align:center">$Var(aX+b)=a^2Var(X)$</td>
</tr>
<tr>
<td style="text-align:center"><strong>独立性影响</strong></td>
<td style="text-align:center">$E[XY]=E[X]E[Y]$（若独立）</td>
<td style="text-align:center">$Var(X+Y)=Var(X)+Var(Y)$（若独立）</td>
</tr>
<tr>
<td style="text-align:center"><strong>与矩的关系</strong></td>
<td style="text-align:center">一阶原点矩</td>
<td style="text-align:center">二阶中心矩</td>
</tr>
</tbody>
</table>
</div>
<h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><script type="math/tex; mode=display">Cov(X, Y) = E[XY] - E[X]E[Y]</script><p>X, Y相互独立 -&gt; $Cov(X, Y) = 0$</p>
<p>协方差为零仅排除线性关系，而独立性排除所有形式的依赖，在联合正态分布中，两者等价，这是特例而非普遍规律。</p>
<blockquote>
<p>没有线性关系意味着 不能通过线性方程 （如 $Y = aX + b$）来描述变量间的关系，但是它们可能存在非线性关系（如 $Y = aX^2 + bX + c$ 或 $Y = sin(X)$）</p>
<p><strong>独立性</strong> 意味着两个随机变量 $X$ 和 $Y$ 的联合分布完全由它们的边缘分布决定，即 $P(X,Y)=P(X)P(Y)$，且<strong>无法通过用任何有统计意义的任何函数（无论是线性还是非线性）从其中一个变量预测另一个变量</strong>。</p>
</blockquote>
<h3 id="通道"><a href="#通道" class="headerlink" title="通道"></a>通道</h3><img src="/2025/03/19/LLM-Rela/8361e16bac5ee3235ef89c78b1a1cf6b.png" class="" title="channel">
<p>多通道卷积过程</p>
<p>输入一张三通道的图片，有多个卷积核进行卷积，并且每个卷积核都有三通道，分别对这张输入图片的三通道进行卷积操作。每个卷积核，分别输出三个通道，这三个通道进行求和，得到一个featuremap，有多少个卷积核，就有多少个featuremap</p>
<h3 id="梯度消失-amp-爆炸"><a href="#梯度消失-amp-爆炸" class="headerlink" title="梯度消失&amp;爆炸"></a>梯度消失&amp;爆炸</h3><p>梯度消失是指，损失函数对网络中某参数计算梯度，由于链式法则，计算链过长，得到的梯度计算结果接近0</p>
<p>可能出现：</p>
<ol>
<li>参数初始化存在问题，权重参数w_i过小导致梯度计算值接近0</li>
<li>参数/激活函数存在指数运算，值被快速缩小导致梯度计算值接近0</li>
</ol>
<p>参数的更新依赖于梯度，当出现梯度消失时，参数无法得到更新</p>
<img src="/2025/03/19/LLM-Rela/image.png" class="" title="image">
<p>优化方式：</p>
<p>1.使用加而不是乘的方式</p>
<p>残差网络 ResNet</p>
<p>2.更好的权重初始化</p>
<h3 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h3><p><strong>Element-wise multiplication（逐元素乘法）</strong></p>
<p>假设有两个 (2 \times 2) 矩阵：</p>
<script type="math/tex; mode=display">
A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}, \quad B = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}</script><p>逐元素乘法的结果：</p>
<script type="math/tex; mode=display">
A \odot B = \begin{bmatrix} 1 \times 5 & 2 \times 6 \\ 3 \times 7 & 4 \times 8 \end{bmatrix} = \begin{bmatrix} 5 & 12 \\ 21 & 32 \end{bmatrix}</script><p>Mask</p>
<p>在深度学习和图像处理中，<strong>掩膜（mask）</strong> 是一种通过逐元素乘法（element-wise multiplication）来选择性过滤或加权张量（如权重、滤波器、通道等）的技术。</p>
<p><strong>权重掩膜（Weight Mask）</strong></p>
<p><strong>作用</strong>：对神经网络的权重进行选择性屏蔽，例如在剪枝（pruning）中去除不重要的连接。</p>
<p>假设有一个全连接层的权重矩阵 ( W ) 和一个二进制掩膜 ( M )（0表示屏蔽，1表示保留）：</p>
<script type="math/tex; mode=display">
W = \begin{bmatrix} 0.1 & -0.2 \\ 0.3 & 0.4 \end{bmatrix}, \quad M = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}</script><p>逐元素乘法后的结果：</p>
<script type="math/tex; mode=display">
W \odot M = \begin{bmatrix} 0.1 \times 1 & -0.2 \times 0 \\ 0.3 \times 0 & 0.4 \times 1 \end{bmatrix} = \begin{bmatrix} 0.1 & 0 \\ 0 & 0.4 \end{bmatrix}</script><p><strong>效果</strong>：第二行第一列和第一行第二列的权重被置零，实现了稀疏化。</p>
<p><strong>滤波器掩膜（Filter Mask）</strong></p>
<p><strong>作用</strong>：在卷积神经网络中，对滤波器的通道或空间区域进行屏蔽。</p>
<p><strong>通道掩膜（Channel Mask）</strong></p>
<p><strong>作用</strong>：对特征图的特定通道进行加权或屏蔽，例如在注意力机制中。</p>
<p>假设特征图 ( F ) 的形状为 ( $2 \times 2 \times 3$ )（高×宽×通道），掩膜 ( M ) 对通道加权：</p>
<script type="math/tex; mode=display">
F = \begin{bmatrix} \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} & \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} & \begin{bmatrix} 9 & 10 \\ 11 & 12 \end{bmatrix} \end{bmatrix}, \quad M = \begin{bmatrix} 0.5 & 1.0 & 0.2 \end{bmatrix}</script><p>逐通道乘法：</p>
<script type="math/tex; mode=display">
F \odot M = \begin{bmatrix} 0.5 \times \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} & 1.0 \times \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} & 0.2 \times \begin{bmatrix} 9 & 10 \\ 11 & 12 \end{bmatrix} \end{bmatrix}</script><p><strong>效果</strong>：第一通道权重减半，第三通道权重缩小为 20%。</p>
<p><strong>空间掩膜（Spatial Mask）</strong></p>
<p><strong>作用</strong>：对特征图的特定空间区域（如像素）进行屏蔽，常见于图像分割或遮挡实验。</p>
<p>输入图像 ( I ) 和矩形掩膜 ( M )（黑色区域为0）：</p>
<script type="math/tex; mode=display">
I = \begin{bmatrix} 255 & 128 \\ 64 & 32 \end{bmatrix}, \quad M = \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}</script><p>逐元素乘法：</p>
<script type="math/tex; mode=display">
I \odot M = \begin{bmatrix} 255 & 0 \\ 64 & 32 \end{bmatrix}</script><p><strong>效果</strong>：图像右上角像素被屏蔽（变黑）。</p>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>输出</p>
<p>短期记忆 $h_t$，长期记忆 $c_t$，input $x_t$</p>
<script type="math/tex; mode=display">
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)</script><script type="math/tex; mode=display">
h_t = o_t \odot \tanh(c_t)</script><p><strong>遗忘门(蓝色)</strong></p>
<script type="math/tex; mode=display">
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)</script><p><strong>输入门和候选记忆</strong></p>
<ul>
<li>输入门控制当前输入信息 $x_t$ 的写入程度：</li>
</ul>
<script type="math/tex; mode=display">
  i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)</script><ul>
<li>候选记忆生成新的候选信息 $\tilde{c}_t$：</li>
</ul>
<script type="math/tex; mode=display">
\tilde{c}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)</script><p><strong>短期记忆更新</strong></p>
<p>结合遗忘门和输入门的结果更新短期记忆 $c_t$：</p>
<script type="math/tex; mode=display">
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t</script><ul>
<li>$\odot$ 表示逐元素相乘。</li>
<li>第一项 $f_t \odot c_{t-1}$ 保留历史信息，第二项 $i_t \odot \tilde{c}_t$ 添加新信息。</li>
</ul>
<img src="/2025/03/19/LLM-Rela/image-20250402161828313.png" class="" title="image-20250402161828313">
<p>recurrent LSTM</p>
<img src="/2025/03/19/LLM-Rela/image-20250402162106999.png" class="" title="image-20250402162106999">
<h3 id="自回归模型"><a href="#自回归模型" class="headerlink" title="自回归模型"></a>自回归模型</h3><p>自回归（Autoregressive, AR）模型的核心是通过条件概率逐步生成序列，数学表示为：</p>
<script type="math/tex; mode=display">p(x_1, x_2, ..., x_T) = \prod_{t=1}^T p(x_t | x_1, ..., x_{t-1})</script><p>这种生成方式适用于文本、图像等序列数据，无论底层模型架构如何（如RNN、CNN或Transformer）。</p>
<p>注意：</p>
<ul>
<li><p><strong>Decoder-Only是自回归的实现方式之一</strong>：Transformer的Decoder-Only架构天然适合自回归生成，因其单向注意力与自回归的逐步生成逻辑一致</p>
</li>
<li><p><strong>自回归不限于Decoder-Only</strong>：例如，扩散模型也可通过多步生成实现自回归效果（如逐像素生成图像）。传统RNN或CNN同样能构建自回归模型。</p>
</li>
</ul>
<h2 id="LLM-struc"><a href="#LLM-struc" class="headerlink" title="LLM-struc"></a>LLM-struc</h2><h3 id="Transformer结构"><a href="#Transformer结构" class="headerlink" title="Transformer结构"></a>Transformer结构</h3><img src="/2025/03/19/LLM-Rela/image-20250325195407601.png" class="" title="image-20250325195407601">
<p>过程</p>
<img src="/2025/03/19/LLM-Rela/image-20250325195428329.png" class="" title="image-20250325195428329">
<h3 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h3><p>单个词 token → 词向量</p>
<p>训练方法</p>
<p>Word2Vec的CBOW（Continuous Bag of Words）模型是一种通过上下文词预测目标词的神经网络模型。以下是其训练流程的详细说明，并结合具体例子进行解释：</p>
<ol>
<li>数据准备</li>
</ol>
<p>首先，需要准备训练数据，通常是大量的文本语料。文本数据需要进行分词等预处理，将文本转换为词语序列。例如，句子“I learn NLP everyday”会被分词为<code>[&quot;I&quot;, &quot;learn&quot;, &quot;NLP&quot;, &quot;everyday&quot;]</code>。</p>
<ol>
<li>创建上下文窗口</li>
</ol>
<p>对于每个目标词，CBOW模型定义了一个上下文窗口。窗口大小由超参数<code>window</code>指定，表示目标词左右两侧的词语数目。例如，窗口大小为2时，目标词“NLP”的上下文词为<code>[&quot;I&quot;, &quot;learn&quot;, &quot;everyday&quot;]</code>。</p>
<ol>
<li>构建训练样本</li>
</ol>
<p>对于每个目标词，CBOW模型从其上下文窗口中收集上下文词。每个训练样本由上下文词构成，目标是预测目标词。例如，目标词“NLP”的训练样本为<code>&#123;&quot;context&quot;: [&quot;I&quot;, &quot;learn&quot;, &quot;everyday&quot;], &quot;target&quot;: &quot;NLP&quot;&#125;</code>。</p>
<ol>
<li>模型结构</li>
</ol>
<p>CBOW模型是一个简单的三层神经网络，包括输入层、隐藏层和输出层： • <strong>输入层</strong>：上下文词用one-hot向量表示。例如，词汇表大小为10,000，单词“I”可能表示为<code>[1, 0, 0, ..., 0]</code>。 • <strong>隐藏层</strong>：通过词向量矩阵（Embedding Matrix）将输入的one-hot向量转换为低维词向量（通常是100～300维）。然后将所有上下文词的词向量相加取平均，作为隐藏层向量。 • <strong>输出层</strong>：隐藏层向量乘以输出权重矩阵，得到输出向量。使用Softmax函数计算目标词的概率分布。</p>
<ol>
<li>训练目标</li>
</ol>
<p>CBOW模型的训练目标是最大化给定上下文词时目标词的条件概率，即最大化$P(w_t | w_{t-c}, w_{t-c+1}, …, w_{t+c})$，其中$w_t$是目标词，$w_{t-c}$到$w_{t+c}$是上下文词。</p>
<ol>
<li>梯度下降</li>
</ol>
<p>使用梯度下降或其变种，通过反向传播算法调整嵌入层的权重，使得模型的预测更接近实际的目标词。</p>
<ol>
<li>重复迭代</li>
</ol>
<p>重复以上步骤多次，直到模型收敛到一个合适的状态。每一轮迭代都遍历整个训练数据。</p>
<p>具体例子</p>
<p>假设语料为“I learn NLP everyday”，目标词为“NLP”，上下文词为<code>[&quot;I&quot;, &quot;learn&quot;, &quot;everyday&quot;]</code>：</p>
<ol>
<li>将上下文词转换为one-hot向量。</li>
<li>将one-hot向量乘以输入权重矩阵，得到词向量。</li>
<li>将所有上下文词的词向量相加取平均，得到隐藏层向量。</li>
<li>将隐藏层向量乘以输出权重矩阵，得到输出向量。</li>
<li>使用Softmax函数计算目标词“NLP”的概率分布。</li>
<li>通过损失函数（如负对数似然）计算预测误差，并使用梯度下降更新模型参数。</li>
</ol>
<p>以上便是CBOW模型的完整训练流程。</p>
<p>Word2Vec的<strong>Skip-Gram</strong>模型与CBOW模型相反，它通过<strong>目标词</strong>预测其<strong>上下文词</strong>。Skip-Gram模型的训练流程如下，并结合具体例子详细说明：</p>
<hr>
<ol>
<li>数据准备</li>
</ol>
<p>首先，准备训练数据，通常是大量的文本语料。文本数据需要进行分词等预处理，将文本转换为词语序列。例如，句子“I learn NLP everyday”会被分词为<code>[&quot;I&quot;, &quot;learn&quot;, &quot;NLP&quot;, &quot;everyday&quot;]</code>。</p>
<hr>
<ol>
<li>创建上下文窗口</li>
</ol>
<p>对于每个目标词，Skip-Gram模型定义了一个上下文窗口。窗口大小由超参数<code>window</code>指定，表示目标词左右两侧的词语数目。例如，窗口大小为2时，目标词“learn”的上下文词为<code>[&quot;I&quot;, &quot;NLP&quot;, &quot;everyday&quot;]</code>。</p>
<hr>
<ol>
<li>构建训练样本</li>
</ol>
<p>对于每个目标词，Skip-Gram模型从其上下文窗口中收集上下文词。每个训练样本由目标词和上下文词组成。例如，目标词“learn”的训练样本为： • <code>&#123;&quot;target&quot;: &quot;learn&quot;, &quot;context&quot;: &quot;I&quot;&#125;</code> • <code>&#123;&quot;target&quot;: &quot;learn&quot;, &quot;context&quot;: &quot;NLP&quot;&#125;</code> • <code>&#123;&quot;target&quot;: &quot;learn&quot;, &quot;context&quot;: &quot;everyday&quot;&#125;</code></p>
<hr>
<ol>
<li>模型结构</li>
</ol>
<p>Skip-Gram模型是一个简单的三层神经网络，包括输入层、隐藏层和输出层： • <strong>输入层</strong>：目标词用one-hot向量表示。例如，词汇表大小为10,000，单词“learn”可能表示为<code>[0, 1, 0, ..., 0]</code>。 • <strong>隐藏层</strong>：通过词向量矩阵（Embedding Matrix）将输入的one-hot向量转换为低维词向量（通常是100～300维）。这个词向量就是隐藏层的输出。 • <strong>输出层</strong>：隐藏层向量乘以输出权重矩阵，得到输出向量。使用Softmax函数计算上下文词的概率分布。</p>
<hr>
<ol>
<li>训练目标</li>
</ol>
<p>Skip-Gram模型的训练目标是最大化给定目标词时上下文词的条件概率，即最大化$P(w_{t+j} | w_t)$，其中$w_t$是目标词，$w_{t+j}$是上下文词。</p>
<hr>
<ol>
<li>梯度下降</li>
</ol>
<p>使用梯度下降或其变种，通过反向传播算法调整嵌入层的权重，使得模型的预测更接近实际的上下文词。</p>
<hr>
<ol>
<li>重复迭代</li>
</ol>
<p>重复以上步骤多次，直到模型收敛到一个合适的状态。每一轮迭代都遍历整个训练数据。</p>
<hr>
<p>具体例子</p>
<p>假设语料为“I learn NLP everyday”，目标词为“learn”，上下文词为<code>[&quot;I&quot;, &quot;NLP&quot;, &quot;everyday&quot;]</code>：</p>
<ol>
<li>将目标词“learn”转换为one-hot向量。</li>
<li>将one-hot向量乘以输入权重矩阵，得到词向量。</li>
<li>将词向量乘以输出权重矩阵，得到输出向量。</li>
<li>使用Softmax函数计算上下文词“I”、“NLP”、“everyday”的概率分布。</li>
<li>通过损失函数（如负对数似然）计算预测误差，并使用梯度下降更新模型参数。</li>
</ol>
<hr>
<p>Skip-Gram与CBOW的区别</p>
<ul>
<li><strong>CBOW</strong>：通过上下文词预测目标词，适合大规模数据，计算效率高，但对罕见词的学习效果较弱。 • <strong>Skip-Gram</strong>：通过目标词预测上下文词，适合小规模数据，对罕见词的学习效果更好，但计算复杂度较高。</li>
</ul>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>词向量 → 含位置信息词向量</p>
<p>加入位置信息（多个正弦函数）到词向量</p>
<p>傅里叶级数思路</p>
<p>绝对位置编码 ps: 注意力机制的矩阵A为相对位置编码</p>
<h4 id="RoPE"><a href="#RoPE" class="headerlink" title="RoPE"></a>RoPE</h4><h3 id="Muti-Head-Attention"><a href="#Muti-Head-Attention" class="headerlink" title="Muti-Head Attention"></a>Muti-Head Attention</h3><h4 id="自注意力"><a href="#自注意力" class="headerlink" title="自注意力"></a>自注意力</h4><img src="/2025/03/19/LLM-Rela/image-20250404192612286.png" class="" title="image-20250404192612286">
<img src="/2025/03/19/LLM-Rela/image-20250325195443397.png" class="" title="image-20250325195443397">
<blockquote>
<p>原始词向量为客观语义，注意力矩阵为主观语义</p>
</blockquote>
<script type="math/tex; mode=display">
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{\top}}{\sqrt{d}}\right)V</script><p>矩阵A 相当于计算 $token_i$ 与 $token_j$ 对应词向量的相似度</p>
<p>$q_i$ 为 query化后 $token_i$ 的词向量</p>
<p>$k_i$ 为 key化后 $token_i$ 的词向量</p>
<p>矩阵A 与 矩阵V 相乘</p>
<p>对每个词 value 化 词向量矩阵 进行相关性修正，得到注意力矩阵（包含其他相似词词向量加和的词向量）</p>
<h4 id="交叉注意力"><a href="#交叉注意力" class="headerlink" title="交叉注意力"></a>交叉注意力</h4><img src="/2025/03/19/LLM-Rela/image-20250404192637755.png" class="" title="image-20250404192637755">
<img src="/2025/03/19/LLM-Rela/image-20250325195459862.png" class="" title="image-20250325195459862">
<h4 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h4><img src="/2025/03/19/LLM-Rela/image-20250404193434062.png" class="" title="image-20250404193434062">
<p>图中为加和，实际使用为concat拼接</p>
<p>1.拆分</p>
<p>原始的单头注意力中，$W_K, W_Q, W_V \in \mathbb{R}^{d \times d}$（维度为 $d \times d$）。<br>在多头机制中，每个头的参数矩阵被水平拆分为更小的矩阵：</p>
<ul>
<li><p><strong>第 $h$ 个头的参数</strong>：</p>
<p>  $W_K^{(h)}, W_Q^{(h)}, W_V^{(h)} \in \mathbb{R}^{d \times d_h}$，其中 $d_h = d/N_h$。例如，若总维度 $d=512$，头数 $N_h=8$，则每个头的维度 $d_h=64$。</p>
</li>
</ul>
<p><strong>拆分方式</strong>：  </p>
<ul>
<li><strong>水平拆分</strong>：将原始矩阵按列切分（如 $W_K$ 被拆为 $[W_K^{(1)}, W_K^{(2)}, …, W_K^{(N_h)}]$），每个子矩阵对应一个头的参数。</li>
</ul>
<p>2.独立计算注意力</p>
<p>每个头 $h$ 使用自己的参数矩阵独立计算注意力：</p>
<ul>
<li><p><strong>输入 $x$ 通过第 $h$ 个头</strong>：</p>
<p> $K^{(h)} = x W_K^{(h)}$， </p>
<p> $Q^{(h)} = x W_Q^{(h)}$， </p>
<p>$V^{(h)} = x W_V^{(h)}$。  </p>
</li>
<li><p><strong>计算注意力输出</strong>：<br> $\text{Attn}_h(x) = \text{softmax}\left(\frac{Q^{(h)} K^{(h)\top}}{\sqrt{d_h}}\right) V^{(h)}$。</p>
</li>
</ul>
<p>3.整合</p>
<p>所有头的输出通过<strong>拼接（Concatenate）</strong>整合：</p>
<ul>
<li><p><strong>拼接（标准Transformer）</strong>：</p>
<p> $\text{MHA}(x) = [\text{Attn}_1(x), \text{Attn}_2(x), …, \text{Attn}_{N_h}(x)] W_O$，其中 $W_O \in \mathbb{R}^{d \times d}$ 是输出投影矩阵。</p>
</li>
</ul>
<h4 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h4><ol>
<li><p>为什么 $A  = X <em> W_q </em> [X <em> W_k]^T$，而不是 $A = X </em> W_A$ ?</p>
<p>第一种为二次型，第二种为线性，二次型可表达映射关系更复杂；</p>
</li>
<li><p>为什么Attenion公式中要除以 $\sqrt d$（d为Q, K矩阵的输出维度） ？</p>
<ul>
<li>当向量维度变大的时候，d变大， q 和 k 的点积的方差变大</li>
<li>由于要对 q 和 k 的点积的每一行进行softmax，过大的方差将导致softmax极端化，得到类似于 $[1,0,0,…]$ 的one-hot分布</li>
<li>当输出接近one-hot时，非最大值的梯度趋近于0，反向传播时，这些位置的参数无法得到更新</li>
<li>因此，设置 softmax 的 temperature 来缓解这个问题，这里 temperature 被设置为了 $\sqrt d$ .</li>
</ul>
<p>如下图所示，假设随机向量 $X$ 满足均值为 0，协方差矩阵为单位矩阵（即各变量独立且方差为 1）的<strong>多元标准正态分布</strong>，可计算得到 $XY^T$ 满足均值为 0，协方差矩阵为 $D_{out} I$ 的<strong>多元正态分布</strong>，通过除以 $\sqrt d$ 将 $XY^T$ 的方差缩放为1</p>
</li>
</ol>
<img src="/2025/03/19/LLM-Rela/image-20250404100511037.png" class="" title="image-20250404100511037">
<ol>
<li><p>为什么选择多头注意力？</p>
<p>有说法认为，克服<strong>「模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置」</strong>，或者<strong>表达能力提升</strong>：多个低秩注意力头（$d_h &lt; d$）的集成，能捕捉更复杂的交互模式；</p>
<p>ref: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.10650">https://arxiv.org/pdf/1905.10650</a></p>
<img src="/2025/03/19/LLM-Rela/image-20250404195056421.png" class="" title="image-20250404195056421">
<img src="/2025/03/19/LLM-Rela/image-20250404195113603.png" class="" title="image-20250404195113603">
<p>结论：</p>
<img src="/2025/03/19/LLM-Rela/image-20250404195352067.png" class="" title="image-20250404195352067">
</li>
</ol>
<h3 id="Layer-Norm-amp-Batch-Norm"><a href="#Layer-Norm-amp-Batch-Norm" class="headerlink" title="Layer Norm &amp; Batch Norm"></a>Layer Norm &amp; Batch Norm</h3><p>Batch Normalization 是对 <strong>所有样本的同一特征维度</strong> 分别做归一化（按列操作）</p>
<p>Layer Normalization 是对 <strong>单个样本的所有特征维度</strong> 做归一化（按行操作）</p>
<p>例如：BN是对特征 $i$ 进行归一，LN是对样本 $x_i$ 进行归一</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">样本</th>
<th style="text-align:center">特征1</th>
<th style="text-align:center">特征2</th>
<th style="text-align:center">特征3</th>
<th style="text-align:center">特征4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>x₁</strong></td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">4.0</td>
</tr>
<tr>
<td style="text-align:center"><strong>x₂</strong></td>
<td style="text-align:center">5.0</td>
<td style="text-align:center">6.0</td>
<td style="text-align:center">7.0</td>
<td style="text-align:center">8.0</td>
</tr>
<tr>
<td style="text-align:center"><strong>x₃</strong></td>
<td style="text-align:center">9.0</td>
<td style="text-align:center">10.0</td>
<td style="text-align:center">11.0</td>
<td style="text-align:center">12.0</td>
</tr>
</tbody>
</table>
</div>
<p>为什么BN在NLP中效果差</p>
<ul>
<li>BN计算特征的均值和方差是需要在batch_size维度，而这个维度表示一个特征，比如身高、体重、肤色等，如果将BN用于NLP中，其需要对每一个单词做处理，让每一个单词是对应到了MLP中的每一个特征明显是违背直觉得；</li>
<li>BN是对单词做缩放，在NLP中，单词由词向量来表达，本质上是对词向量进行缩放。词向量是什么？是我们学习出来的参数来表示词语语义的参数，不是真实存在的。</li>
</ul>
<p>为什么LayerNorm单独对一个样本的所有单词做缩放可以起到效果</p>
<ul>
<li>layner-norm 针对每一个样本做特征的缩放。换句话讲，保留了N维度，在C/H/W维度上做缩放。</li>
<li>layner-norm 也是在对同一个特征下的元素做归一化，只不过这里不再是对应N（或者说batch size），而是对应的文本长度。</li>
</ul>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><h3 id="FFN"><a href="#FFN" class="headerlink" title="FFN"></a>FFN</h3><p><strong>Feed-Forward Network (FFN)</strong> FFN 的结构通常是一个简单的两层 MLP（多层感知机），其作用是对每个位置的特征进行进一步的处理和增强。</p>
<p>如果无法对模型进行微调，但希望模型在特定任务上表现更好，可以考虑以下方法：</p>
<ol>
<li><strong>提示工程（Prompt Engineering）</strong>：通过设计更精准的提示词（prompt）来引导模型生成符合预期的输出。</li>
<li><strong>上下文学习（In-context Learning）</strong>：在API调用时提供上下文信息或示例，帮助模型更好地理解任务。</li>
<li><strong>结合外部工具</strong>：将ChatGPT API与其他工具或系统结合，通过后处理或规则来优化输出。</li>
</ol>
<h2 id="LLM-Train"><a href="#LLM-Train" class="headerlink" title="LLM-Train"></a>LLM-Train</h2><h3 id="Tensor-Parallelism"><a href="#Tensor-Parallelism" class="headerlink" title="Tensor Parallelism"></a>Tensor Parallelism</h3><h3 id="BF16-amp-FP16"><a href="#BF16-amp-FP16" class="headerlink" title="BF16&amp;FP16"></a>BF16&amp;FP16</h3><h2 id="LLM-Inference"><a href="#LLM-Inference" class="headerlink" title="LLM-Inference"></a>LLM-Inference</h2><h3 id="vLLM"><a href="#vLLM" class="headerlink" title="vLLM"></a>vLLM</h3><h3 id="Temperature"><a href="#Temperature" class="headerlink" title="Temperature"></a>Temperature</h3><h3 id="Top-K-amp-Top-P"><a href="#Top-K-amp-Top-P" class="headerlink" title="Top-K&amp;Top-P"></a>Top-K&amp;Top-P</h3><h2 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine-Tuning"></a>Fine-Tuning</h2><p>高效微调技术分类：</p>
<ul>
<li>增加额外参数（A）<ul>
<li>类适配器（Adapter-like）方法</li>
<li>软提示（Soft prompts）</li>
</ul>
</li>
<li>选取一部分参数更新（S）</li>
<li>引入重参数化（R）</li>
</ul>
<img src="/2025/03/19/LLM-Rela/v2-eaaf1c00d0c4ea350cd3a79b47de26d3_1440w.jpg" class="" title="img">
<h3 id="BitFit-Prefix-Tuning-amp-Prompt-Tuning"><a href="#BitFit-Prefix-Tuning-amp-Prompt-Tuning" class="headerlink" title="BitFit, Prefix Tuning &amp; Prompt Tuning"></a>BitFit, Prefix Tuning &amp; Prompt Tuning</h3><p>BitFit（论文：<strong>BitFit: Simple Parameter-efficient Fine-tuning or Transformer-based Masked Language-models</strong>）是一种稀疏的微调方法，它训练时只更新bias的参数或者部分bias参数。</p>
<p>涉及到的bias参数有attention模块中计算query,key,value跟合并多个attention结果时涉及到的bias，MLP层中的bias，Layernormalization层的bias参数。</p>
<p>Prefix Tuning（论文：<strong>Prefix-Tuning: Optimizing Continuous Prompts for Generation</strong>），在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而PLM(Pretrain LM)中的其他部分参数固定。</p>
<p>Prompt Tuning（论文：<strong>The Power of Scale for Parameter-Efficient Prompt Tuning</strong>），该方法可以看作是Prefix Tuning的简化版本，它给每个任务定义了自己的Prompt，然后拼接到数据上作为输入，但<strong>只在输入层加入prompt tokens</strong>，并且不需要加入 MLP 进行调整来解决难训练的问题。</p>
<h3 id="P-Tuning"><a href="#P-Tuning" class="headerlink" title="P-Tuning"></a>P-Tuning</h3><p>P-Tuning（论文：<strong>GPT Understands, Too</strong>），该方法将Prompt转换为可以学习的Embedding层，并用MLP+LSTM的方式来对Prompt Embedding进行一层处理。</p>
<p>相比Prefix Tuning，P-Tuning加入的可微的virtual token，但仅限于输入层，没有在每一层都加；另外，virtual token的位置也不一定是前缀，插入的位置是可选的。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual token。</p>
<p>P-Tuning v2（论文： <strong>P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</strong>），该方法在每一层都加入了Prompts tokens作为输入，而不是仅仅加在输入层</p>
<h3 id="Adapter-Tuning"><a href="#Adapter-Tuning" class="headerlink" title="Adapter Tuning"></a>Adapter Tuning</h3><p>Adapter Tuning（论文：<strong>Parameter-Efficient Transfer Learning for NLP</strong>），该方法设计了Adapter结构，并将其嵌入Transformer的结构里面，针对每一个Transformer层，增加了两个Adapter结构(分别是多头注意力的投影之后和第二个feed-forward层之后)，在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构和 Layer Norm 层进行微调，从而保证了训练的高效性。</p>
<p>Adapter Fusion（论文：<strong>AdapterFusion:Non-Destructive Task Composition for Transfer Learning</strong>），一种融合多任务信息的Adapter的变体，在 Adapter 的基础上进行优化，通过将学习过程分为两阶段来提升下游任务表现。</p>
<p>AdapterDrop（论文：AdapterDrop: On the Efficiency of Adapters in Transformers），在不影响任务性能的情况下，对Adapter动态高效的移除，尽可能的减少模型的参数量，提高模型在反向传播（训练）和正向传播（推理）时的效率。</p>
<h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h3><p>LoRA（论文：<strong>LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</strong>），该方法的核心思想就是通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。</p>
<p>AdaLoRA（论文：<strong>ADAPTIVE BUDGET ALLOCATION FOR PARAMETEREFFICIENT FINE-TUNING</strong>），是对LoRA的一种改进，它根据重要性评分动态分配参数预算给权重矩阵。</p>
<p>QLoRA（论文： <strong>QLORA: Efficient Finetuning of Quantized LLMs</strong>），使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。QLORA 有一种低精度存储数据类型（4 bit），还有一种计算数据类型（BFloat16）。实际上，这意味着无论何时使用 QLoRA 权重张量，我们都会将张量反量化为 BFloat16，然后执行 16 位矩阵乘法。QLoRA提出了两种技术实现高保真 4 bit微调——4 bit NormalFloat(NF4) 量化和双量化。此外，还引入了分页优化器，以防止梯度检查点期间的内存峰值，从而导致内存不足的错误，这些错误在过去使得大型模型难以在单台机器上进行微调。</p>
<h3 id="MAM-Adapter-amp-UniPELT"><a href="#MAM-Adapter-amp-UniPELT" class="headerlink" title="MAM Adapter &amp; UniPELT"></a>MAM Adapter &amp; UniPELT</h3><p>MAM Adapter（论文：TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING），一个在Adapter、Prefix Tuning和LoRA之间建立联系的统一方法。</p>
<p>UniPELT（论文： UNIPELT: A Unified Framework for Parameter-Efficient Language Model Tuning）是 LoRA、Prefix Tuning和Adapter的门控组合。</p>
<h2 id="MCP"><a href="#MCP" class="headerlink" title="MCP"></a>MCP</h2><p>Model Context Protocol</p>
<p>ref: <a target="_blank" rel="noopener" href="https://modelcontextprotocol.io/introduction">https://modelcontextprotocol.io/introduction</a></p>
<p>ref: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29001189476">https://zhuanlan.zhihu.com/p/29001189476</a></p>
<img src="/2025/03/19/LLM-Rela/v2-2bcd98f6541da0b6f14dc9082ee2dcda_1440w.jpg" class="" title="mcp">
<p><a target="_blank" rel="noopener" href="https://github.com/modelcontextprotocol/python-sdk/tree/main/examples/clients/simple-chatbot/mcp_simple_chatbot">example</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># client </span></span><br><span class="line">   ... <span class="comment"># 省略了无关的代码</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">start</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># 初始化所有的 mcp server</span></span><br><span class="line">    <span class="keyword">for</span> server <span class="keyword">in</span> self.servers:</span><br><span class="line">        <span class="keyword">await</span> server.initialize()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取所有的 tools 命名为 all_tools</span></span><br><span class="line">    all_tools = []</span><br><span class="line">    <span class="keyword">for</span> server <span class="keyword">in</span> self.servers:</span><br><span class="line">        tools = <span class="keyword">await</span> server.list_tools()</span><br><span class="line">        all_tools.extend(tools)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将所有的 tools 的功能描述格式化成字符串供 LLM 使用</span></span><br><span class="line">    <span class="comment"># tool.format_for_llm() 我放到了这段代码最后，方便阅读。</span></span><br><span class="line">    tools_description = <span class="string">&quot;\n&quot;</span>.join(</span><br><span class="line">        [tool.format_for_llm() <span class="keyword">for</span> tool <span class="keyword">in</span> all_tools]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里就不简化了，以供参考，实际上就是基于 prompt 和当前所有工具的信息</span></span><br><span class="line">    <span class="comment"># 询问 LLM（Claude） 应该使用哪些工具。</span></span><br><span class="line">    system_message = (</span><br><span class="line">        <span class="string">&quot;You are a helpful assistant with access to these tools:\n\n&quot;</span></span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;tools_description&#125;</span>\n&quot;</span></span><br><span class="line">        <span class="string">&quot;Choose the appropriate tool based on the user&#x27;s question. &quot;</span></span><br><span class="line">        <span class="string">&quot;If no tool is needed, reply directly.\n\n&quot;</span></span><br><span class="line">        <span class="string">&quot;IMPORTANT: When you need to use a tool, you must ONLY respond with &quot;</span></span><br><span class="line">        <span class="string">&quot;the exact JSON object format below, nothing else:\n&quot;</span></span><br><span class="line">        <span class="string">&quot;&#123;\n&quot;</span></span><br><span class="line">        <span class="string">&#x27;    &quot;tool&quot;: &quot;tool-name&quot;,\n&#x27;</span></span><br><span class="line">        <span class="string">&#x27;    &quot;arguments&quot;: &#123;\n&#x27;</span></span><br><span class="line">        <span class="string">&#x27;        &quot;argument-name&quot;: &quot;value&quot;\n&#x27;</span></span><br><span class="line">        <span class="string">&quot;    &#125;\n&quot;</span></span><br><span class="line">        <span class="string">&quot;&#125;\n\n&quot;</span></span><br><span class="line">        <span class="string">&quot;After receiving a tool&#x27;s response:\n&quot;</span></span><br><span class="line">        <span class="string">&quot;1. Transform the raw data into a natural, conversational response\n&quot;</span></span><br><span class="line">        <span class="string">&quot;2. Keep responses concise but informative\n&quot;</span></span><br><span class="line">        <span class="string">&quot;3. Focus on the most relevant information\n&quot;</span></span><br><span class="line">        <span class="string">&quot;4. Use appropriate context from the user&#x27;s question\n&quot;</span></span><br><span class="line">        <span class="string">&quot;5. Avoid simply repeating the raw data\n\n&quot;</span></span><br><span class="line">        <span class="string">&quot;Please use only the tools that are explicitly defined above.&quot;</span></span><br><span class="line">    )</span><br><span class="line">    messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: system_message&#125;]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># Final... 假设这里已经处理了用户消息输入.</span></span><br><span class="line">        messages.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: user_input&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 system_message 和用户消息输入一起发送给 LLM</span></span><br><span class="line">        llm_response = self.llm_client.get_response(messages)</span><br><span class="line"></span><br><span class="line">    ... <span class="comment"># 后面和确定使用哪些工具无关</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># server</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tool</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Represents a tool with its properties and formatting.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, name: <span class="built_in">str</span>, description: <span class="built_in">str</span>, input_schema: <span class="built_in">dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.name: <span class="built_in">str</span> = name</span><br><span class="line">        self.description: <span class="built_in">str</span> = description</span><br><span class="line">        self.input_schema: <span class="built_in">dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>] = input_schema</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把工具的名字 / 工具的用途（description）和工具所需要的参数（args_desc）转化为文本</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">format_for_llm</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Format tool information for LLM.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            A formatted string describing the tool.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        args_desc = []</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;properties&quot;</span> <span class="keyword">in</span> self.input_schema:</span><br><span class="line">            <span class="keyword">for</span> param_name, param_info <span class="keyword">in</span> self.input_schema[<span class="string">&quot;properties&quot;</span>].items():</span><br><span class="line">                arg_desc = (</span><br><span class="line">                    <span class="string">f&quot;- <span class="subst">&#123;param_name&#125;</span>: <span class="subst">&#123;param_info.get(<span class="string">&#x27;description&#x27;</span>, <span class="string">&#x27;No description&#x27;</span>)&#125;</span>&quot;</span></span><br><span class="line">                )</span><br><span class="line">                <span class="keyword">if</span> param_name <span class="keyword">in</span> self.input_schema.get(<span class="string">&quot;required&quot;</span>, []):</span><br><span class="line">                    arg_desc += <span class="string">&quot; (required)&quot;</span></span><br><span class="line">                args_desc.append(arg_desc)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Tool: <span class="subst">&#123;self.name&#125;</span></span></span><br><span class="line"><span class="string">Description: <span class="subst">&#123;self.description&#125;</span></span></span><br><span class="line"><span class="string">Arguments:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;<span class="built_in">chr</span>(<span class="number">10</span>).join(args_desc)&#125;</span></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="LLM-PPO"><a href="#LLM-PPO" class="headerlink" title="LLM-PPO"></a>LLM-PPO</h2><p>状态空间：模型输入prompt</p>
<p>动作空间：模型输出output</p>
<p>Actor：SFT后的LLM网络，进行更新</p>
<p>Critic：Value网络，进行更新</p>
<blockquote>
<p>Reward网络在RM过程中训练完成，基于SFT Model（移除最后一层softmax，替换为线性层），输出为scalar reward，表示<strong>对完整输出序列的整体评价</strong>；</p>
<p>Reference网络为SFT Model；</p>
<p>在RL-PPO过程中这两个网络不更新</p>
</blockquote>
<img src="/2025/03/19/LLM-Rela/image-20250325131654259.png" class="" title="image-20250325131654259">
<p>SFT LLM</p>
<p>train Reward Model</p>
<img src="/2025/03/19/LLM-Rela/image-20250325151928459.png" class="" title="image-20250325151928459">
<p>use LLM_sft to be the Actor(reference model)</p>
<img src="/2025/03/19/LLM-Rela/image-20250325153214542.png" class="" title="image-20250325153214542">
<img src="/2025/03/19/LLM-Rela/image-20250325153304648.png" class="" title="image-20250325153304648">
<img src="/2025/03/19/LLM-Rela/image-20250325153337812.png" class="" title="image-20250325153337812">
<blockquote>
<p>广义优势A^t为多步时序差分的指数加权平均，详见reinforce-learning-record，参数含Value值和Reward值</p>
</blockquote>
<p><strong>为什么引入KL散度项？</strong></p>
<p>This KL term serves two purposes. First, it acts as an entropy bonus, encouraging the policy to explore and deterring it from collapsing to a single mode. Second, it ensures the policy doesn’t learn to produce outputs that are too different from those that the reward model has seen during training.</p>
<h2 id="LLM-GRPO"><a href="#LLM-GRPO" class="headerlink" title="LLM-GRPO"></a>LLM-GRPO</h2><img src="/2025/03/19/LLM-Rela/image-20250325195333997.png" class="" title="image-20250325195333997">
<p>由于PPO算法中使用的价值函数通常是与策略模型规模相当的另一个模型，这会带来巨大的内存和计算负担。此外，在强化学习训练过程中，价值函数被作为计算优势函数（advantage）的基线以实现方差缩减。然而在大型语言模型（LLM）场景中，通常只有最后一个token会被奖励模型分配奖励分数，这可能导致对每个token都精确建模价值函数的训练变得复杂。为解决这一问题，如图4所示，我们提出了组相对策略优化（Group Relative Policy Optimization, GRPO）</p>
<p>每一个 o_g 为策略模型对于输入 q 的输出，每一个 r_g 为 RM 对 每一个 o_g 的评分</p>
<img src="/2025/03/19/LLM-Rela/image-20250325201341810.png" class="" title="image-20250325201341810">
<p>不同点：</p>
<ol>
<li><p>优势函数：过程监督与结果监督</p>
<img src="/2025/03/19/LLM-Rela/image-20250325202638658.png" class="" title="image-20250325202638658">
</li>
<li><p>超参数ε控制策略更新幅度，β调节KL散度约束</p>
</li>
</ol>
<img src="/2025/03/19/LLM-Rela/image-20250325201533207.png" class="" title="image-20250325201533207">

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python-Pytorch/" rel="tag"># Python, Pytorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/02/19/Computer-Network/" rel="prev" title="Computer-Network">
      <i class="fa fa-chevron-left"></i> Computer-Network
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/03/28/Netty/" rel="next" title="Netty">
      Netty <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#LLM-amp-Rela"><span class="nav-text">LLM&amp;Rela</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Prompt-Engineering"><span class="nav-text">Prompt Engineering</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Zero-Shot-Prompting"><span class="nav-text">Zero-Shot Prompting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Few-Shot-Prompting"><span class="nav-text">Few-Shot Prompting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Chain-of-Thought-CoT-Prompting"><span class="nav-text">Chain-of-Thought (CoT) Prompting</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-base"><span class="nav-text">LLM-base</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%9F%E6%9C%9B%E4%B8%8E%E6%96%B9%E5%B7%AE"><span class="nav-text">期望与方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%8F%E6%96%B9%E5%B7%AE"><span class="nav-text">协方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E9%81%93"><span class="nav-text">通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1-amp-%E7%88%86%E7%82%B8"><span class="nav-text">梯度消失&amp;爆炸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mask"><span class="nav-text">Mask</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM"><span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-text">自回归模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-struc"><span class="nav-text">LLM-struc</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformer%E7%BB%93%E6%9E%84"><span class="nav-text">Transformer结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tokenization"><span class="nav-text">Tokenization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Positional-Encoding"><span class="nav-text">Positional Encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RoPE"><span class="nav-text">RoPE</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Muti-Head-Attention"><span class="nav-text">Muti-Head Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-text">自注意力</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-text">交叉注意力</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-text">多头注意力</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98"><span class="nav-text">相关问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Layer-Norm-amp-Batch-Norm"><span class="nav-text">Layer Norm &amp; Batch Norm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet"><span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FFN"><span class="nav-text">FFN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-Train"><span class="nav-text">LLM-Train</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor-Parallelism"><span class="nav-text">Tensor Parallelism</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BF16-amp-FP16"><span class="nav-text">BF16&amp;FP16</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-Inference"><span class="nav-text">LLM-Inference</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#vLLM"><span class="nav-text">vLLM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Temperature"><span class="nav-text">Temperature</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Top-K-amp-Top-P"><span class="nav-text">Top-K&amp;Top-P</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fine-Tuning"><span class="nav-text">Fine-Tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BitFit-Prefix-Tuning-amp-Prompt-Tuning"><span class="nav-text">BitFit, Prefix Tuning &amp; Prompt Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#P-Tuning"><span class="nav-text">P-Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adapter-Tuning"><span class="nav-text">Adapter Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LoRA"><span class="nav-text">LoRA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MAM-Adapter-amp-UniPELT"><span class="nav-text">MAM Adapter &amp; UniPELT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MCP"><span class="nav-text">MCP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-PPO"><span class="nav-text">LLM-PPO</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-GRPO"><span class="nav-text">LLM-GRPO</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="marigo1d"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">marigo1d</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/marigo1d" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;marigo1d" rel="noopener" target="_blank">GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">marigo1d</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">400k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">12:07</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
