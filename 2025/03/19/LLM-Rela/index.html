<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"marigo1d.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="LLM相关 施工中…">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM&amp;Rela">
<meta property="og:url" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/index.html">
<meta property="og:site_name" content="Marigold">
<meta property="og:description" content="LLM相关 施工中…">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/8361e16bac5ee3235ef89c78b1a1cf6b.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/e38fac064524158e493a66adb2caed6e.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250402161828313.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250402162106999.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/v2-ee9b5d4a0761d2d1d10acb37cebefba3_1440w.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250706100412469.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325195428329.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250706105452257.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/LLM-structure.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/LLM-structure-moe.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250404192612286.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325195443397.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9F%A9%E9%98%B5%E5%9B%BE.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325195459862.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/multi-head-%E6%8B%BC%E6%8E%A5.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/mask-attention-map.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/v2-b257d8660af7678f9c9bdc14d095b6d3_1440w.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/v2-1eda59791cb4a0a09f9ebce4fbb23865_1440w.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250404100511037.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250405101512560.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250613213945162.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/0.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/4.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/6.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/6.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/v2-eaaf1c00d0c4ea350cd3a79b47de26d3_1440w.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325131654259.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325151928459.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325153214542.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325153304648.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325153337812.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325195333997.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325201341810.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325202638658.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325201533207.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/LLM-structure.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/LLM-structure-moe.png">
<meta property="article:published_time" content="2025-03-19T02:50:14.000Z">
<meta property="article:modified_time" content="2025-07-07T01:23:06.003Z">
<meta property="article:author" content="marigo1d">
<meta property="article:tag" content="Python, Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/8361e16bac5ee3235ef89c78b1a1cf6b.png">

<link rel="canonical" href="https://marigo1d.github.io/2025/03/19/LLM-Rela/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>LLM&Rela | Marigold</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Marigold</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Salt, Pepper and Birds~</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://marigo1d.github.io/2025/03/19/LLM-Rela/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="marigo1d">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Marigold">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LLM&Rela
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-19 10:50:14" itemprop="dateCreated datePublished" datetime="2025-03-19T10:50:14+08:00">2025-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-07-07 09:23:06" itemprop="dateModified" datetime="2025-07-07T09:23:06+08:00">2025-07-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>35k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1:04</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>LLM相关</p>
<p>施工中…</p>
<span id="more"></span>
<h1>LLM&amp;Rela</h1>
<h2 id="LLM-base">LLM-base</h2>
<h3 id="期望与方差">期望与方差</h3>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>性质</strong></th>
<th style="text-align:center"><strong>期望 E[⋅]</strong></th>
<th style="text-align:center"><strong>方差 Var(⋅)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>线性变换</strong></td>
<td style="text-align:center">$E[aX+b]=aE[X]+b$</td>
<td style="text-align:center">$Var(aX+b)=a^2Var(X)$</td>
</tr>
<tr>
<td style="text-align:center"><strong>独立性影响</strong></td>
<td style="text-align:center">$E[XY]=E[X]E[Y]$（若独立）</td>
<td style="text-align:center">$Var(X+Y)=Var(X)+Var(Y)$（若独立）</td>
</tr>
<tr>
<td style="text-align:center"><strong>与矩的关系</strong></td>
<td style="text-align:center">一阶原点矩</td>
<td style="text-align:center">二阶中心矩</td>
</tr>
</tbody>
</table>
<br>
<h3 id="协方差">协方差</h3>
<p>$$Cov(X, Y) = E[XY] - E[X]E[Y]$$</p>
<p>X, Y相互独立 -&gt; $Cov(X, Y) = 0$</p>
<p>协方差为零仅排除线性关系，而独立性排除所有形式的依赖，在联合正态分布中，两者等价，这是特例而非普遍规律。</p>
<blockquote>
<p>没有线性关系意味着 不能通过线性方程 （如 $Y = aX + b$）来描述变量间的关系，但是它们可能存在非线性关系（如 $Y = aX^2 + bX + c$ 或 $Y = sin(X)$）</p>
<p><strong>独立性</strong> 意味着两个随机变量 $X$ 和 $Y$ 的联合分布完全由它们的边缘分布决定，即 $P(X,Y)=P(X)P(Y)$，且<strong>无法通过用任何有统计意义的任何函数（无论是线性还是非线性）从其中一个变量预测另一个变量</strong>。</p>
</blockquote>
<br>
<h3 id="激活函数">激活函数</h3>
<p>激活函数在神经网络的每一层中引入非线性，使得神经网络能够拟合复杂的非线性模式。常见的激活函数有 Sigmoid、ReLU（Rectified Linear Unit）、Tanh、Leaky ReLU 等</p>
<p><strong>Sigmoid 函数</strong></p>
<blockquote>
<p>sigmoid 乙型形状 $(-\infty, 0) \to (0,\frac{1}{2})$ &amp; $(0, +\infty) \to (\frac{1}{2}, 1)$</p>
</blockquote>
<p><strong>公式：</strong></p>
<p>$\sigma(x) = \frac{1}{1 + e^{-x}}$</p>
<p><strong>特点：</strong></p>
<ul>
<li>输出范围在 (0, 1) 之间</li>
<li>常用于二分类问题的输出层</li>
<li>缺点：在极端值处梯度很小，容易导致梯度消失</li>
</ul>
<br>
<p><strong>Tanh（双曲正切）函数</strong></p>
<p><strong>公式：</strong></p>
<p>$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$</p>
<p><strong>特点：</strong></p>
<ul>
<li>输出范围在 (-1, 1) 之间</li>
<li>相比 Sigmoid 更适合隐藏层，因为它的均值为 0</li>
<li>同样存在梯度消失问题（但稍弱于 Sigmoid）</li>
</ul>
<br>
<p><strong>ReLU（Rectified Linear Unit）函数</strong></p>
<p><strong>公式：</strong></p>
<p>$\text{ReLU}(x) = \max(0, x)$</p>
<p><strong>特点：</strong></p>
<ul>
<li>简单、高效，收敛速度快</li>
<li>输出范围：[0, +∞)</li>
<li>缺点：负值部分梯度为 0，可能导致“神经元死亡”</li>
</ul>
<br>
<p><strong>Leaky ReLU 函数</strong></p>
<p><strong>公式：</strong></p>
<p>$\text{Leaky ReLU}(x) = \begin{cases} x &amp; \text{if } x \geq 0 \ \alpha x &amp; \text{if } x &lt; 0 \end{cases}$</p>
<p>其中，$\alpha$ 是一个很小的正数（如 0.01）</p>
<p><strong>特点：</strong></p>
<ul>
<li>改进了 ReLU 的“死亡神经元”问题</li>
<li>允许负方向有微小的梯度，避免完全失活</li>
</ul>
<br>
<h3 id="损失函数">损失函数</h3>
<p><strong>均方误差（Mean Squared Error, MSE）</strong></p>
<p><strong>公式：</strong></p>
<p>$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$</p>
<ul>
<li>$y_i$：真实值</li>
<li>$\hat{y}_i$：预测值</li>
<li>$n$：样本数</li>
</ul>
<p><strong>应用场景：</strong></p>
<ul>
<li>回归问题（如房价预测）</li>
<li>对异常值敏感，平方项放大误差</li>
</ul>
<br>
<p><strong>平均绝对误差（Mean Absolute Error, MAE）</strong></p>
<p><strong>公式：</strong></p>
<p>$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$</p>
<p><strong>应用场景：</strong></p>
<ul>
<li>回归问题</li>
<li>更鲁棒，不像 MSE 那样对异常值敏感</li>
</ul>
<br>
<p><strong>Huber Loss（平滑的 MSE 和 MAE 的结合）</strong></p>
<p><strong>公式：</strong></p>
<p>$L_\delta(a) = \begin{cases} \frac{1}{2} a^2 &amp; \text{if } |a| \leq \delta \ \delta (|a| - \frac{1}{2} \delta) &amp; \text{otherwise} \end{cases}$</p>
<p>其中 $a = y - \hat{y}$</p>
<p><strong>应用场景：</strong></p>
<ul>
<li>回归问题</li>
<li>同时兼顾 MAE 的鲁棒性和 MSE 的可导性</li>
</ul>
<br>
<p><strong>交叉熵损失（Cross-Entropy Loss）</strong></p>
<p>▶ 二分类交叉熵（Binary Cross-Entropy）：</p>
<p><strong>公式：</strong></p>
<p>$\text{Loss} = - \left[ y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right]$</p>
<ul>
<li>$y \in {0, 1}$ 为真实标签</li>
<li>$\hat{y}$ 是预测概率</li>
</ul>
<p><strong>应用场景：</strong></p>
<ul>
<li>二分类问题（如猫 vs 狗）</li>
</ul>
<blockquote>
<p>预测值在log内</p>
<p>有负号</p>
</blockquote>
<br>
<p>▶ 多分类交叉熵（Categorical Cross-Entropy）：</p>
<p><strong>公式（softmax 输出）：</strong></p>
<p>$\text{Loss} = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)$</p>
<ul>
<li>$C$ 是类别总数</li>
<li>$y_i$ 是 one-hot 编码的真实标签</li>
<li>$\hat{y}_i$ 是第 ii 类的预测概率</li>
</ul>
<p><strong>应用场景：</strong></p>
<ul>
<li>多分类问题（如数字识别）</li>
</ul>
<br>
<p><strong>KL 散度（Kullback–Leibler Divergence）</strong></p>
<p><strong>公式：</strong></p>
<p>$D_{KL}(P \parallel Q) = \sum_{i} P(i) \log\left(\frac{P(i)}{Q(i)}\right)$</p>
<ul>
<li>$P$：真实分布</li>
<li>$Q$：预测分布</li>
</ul>
<p><strong>应用场景：</strong></p>
<ul>
<li>分布之间的距离度量（如在生成模型中）</li>
</ul>
<blockquote>
<p>交叉熵 = KL散度 + 熵</p>
</blockquote>
<br>
<h3 id="优化算法">优化算法</h3>
<h4 id="梯度下降">梯度下降</h4>
<p>Gradient Descent</p>
<p><strong>公式：</strong></p>
<p>$\theta := \theta - \eta \cdot \nabla_\theta J(\theta)$</p>
<ul>
<li>$\theta$：模型参数</li>
<li>$\eta$：学习率（learning rate）</li>
<li>$\nabla_\theta J(\theta)$：损失函数 $J$ 关于参数 $\theta$ 的梯度</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>每次用<strong>全部数据</strong>计算梯度，更新参数</li>
<li>精度高但计算开销大，适合小数据集</li>
</ul>
<br>
<h4 id="随机梯度下降">随机梯度下降</h4>
<p>Stochastic Gradient Descent, SGD</p>
<p><strong>公式：</strong></p>
<p>$\theta := \theta - \eta \cdot \nabla_\theta J(\theta; x^{(i)}, y^{(i)})$</p>
<ul>
<li>每次仅用<strong>一个样本</strong> $(x(i),y(i))(x^{(i)}, y^{(i)})$ 更新一次参数</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>计算效率高、更新频繁</li>
<li>噪声大，有时不稳定，但有助于跳出局部最优</li>
</ul>
<br>
<h4 id="小批量梯度下降">小批量梯度下降</h4>
<p>Adaptive Moment Estimation</p>
<p><strong>公式：</strong></p>
<p>$\theta := \theta - \eta \cdot \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\theta; x^{(i)}, y^{(i)})$</p>
<ul>
<li>每次用一小批（mini-batch）样本来计算梯度</li>
<li>折中效率与稳定性，是深度学习中<strong>最常用的方式</strong></li>
</ul>
<br>
<h4 id="Adam">Adam</h4>
<p>Adaptive Moment Estimation</p>
<p>Adam = <strong>Momentum（动量）</strong> + <strong>RMSProp（自适应学习率）</strong> + <strong>偏差修正</strong></p>
<br>
<p><strong>动量法（Momentum）来源</strong></p>
<p>先看普通的梯度下降更新：</p>
<p>$\theta := \theta - \eta \cdot \nabla_\theta J(\theta)$</p>
<p>但这个更新方向容易“来回震荡”，所以引入动量的思想，让参数更新像“带惯性的小球”那样滑下去：</p>
<p>动量法公式：</p>
<p>$v_t = \beta_1 v_{t-1} + (1 - \beta_1) \nabla_\theta J(\theta)θ:=θ−η⋅vt\theta := \theta - \eta \cdot v_t$</p>
<p>这个动量 $v_t$ 相当于对梯度的指数加权平均。</p>
<p>Adam 的第一部分：</p>
<p>$m_t = \beta_1 m_{t-1} + (1 - \beta_1) \cdot g_t$</p>
<p>就是来自这个思路，$m_t$ ≈ 平滑梯度（momentum）</p>
<br>
<p><strong>RMSProp 来源：自适应学习率</strong></p>
<p>RMSProp 想解决的问题是：<strong>不同参数的梯度尺度不同时，用相同学习率不合适</strong>。</p>
<p>所以它引入一个“平方梯度的指数平均”：</p>
<p>$s_t = \beta_2 s_{t-1} + (1 - \beta_2) g_t^2θ:=θ−ηst+ϵ⋅gt\theta := \theta - \frac{\eta}{\sqrt{s_t} + \epsilon} \cdot g_t$</p>
<p>Adam 的第二部分：</p>
<p>$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$</p>
<p>就是借鉴了 RMSProp，让学习率根据梯度历史自适应调整。</p>
<br>
<p><strong>偏差修正的来由</strong></p>
<p>Adam 一开始 m_1, v_1 都是从 0 开始的，但因为是指数加权平均，会导致初始几步偏小，<strong>“有偏估计”</strong>。</p>
<p>所以做了修正：</p>
<p>$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$</p>
<p>这个公式来源于“期望的无偏估计推导”，用数学方法校正初期偏小的问题。</p>
<blockquote>
<p>Q：为什么要做偏差修正？</p>
<p>A：因为初始时刻 $m_t$、$v_t$ 都从 0 开始，用指数加权会造成“低估”真实值（特别在前几步）。</p>
<p>所以 Adam 中引入了：</p>
<p>$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$</p>
<p>下面<strong>推导</strong>这个修正项是怎么来的。</p>
<p>推导一阶动量偏差修正项（以 $m_t$ 为例）</p>
<p>我们从递推公式出发（假设 $m_0$ = 0）：</p>
<p>$m_1 = (1 - \beta_1) g_1  $</p>
<p>$m_2 = \beta_1 m_1 + (1 - \beta_1) g_2 = \beta_1 (1 - \beta_1) g_1 + (1 - \beta_1) g_2$</p>
<p>$m_3 = \beta_1^2 (1 - \beta_1) g_1 + \beta_1 (1 - \beta_1) g_2 + (1 - \beta_1) g_3$</p>
<p>可推广为：</p>
<p>$m_t = (1 - \beta_1) \sum_{i=1}^{t} \beta_1^{t-i} g_i$</p>
<p>期望分析：$m_t$ 是一个<strong>有偏估计</strong></p>
<p>我们希望的是：</p>
<p>$\mathbb{E}[m_t] = \mathbb{E}[g_t]$</p>
<p>但是上面推导的形式中：</p>
<p>$\mathbb{E}[m_t] = (1 - \beta_1) \sum_{i=1}^{t} \beta_1^{t-i} \mathbb{E}[g_i]$</p>
<p>如果我们假设：</p>
<ul>
<li>
<p>梯度是平稳的（即各时刻期望相同）：</p>
<p>$\mathbb{E}[g_1] = \mathbb{E}[g_2] = \dots = \mathbb{E}[g_t] = \mu$</p>
</li>
</ul>
<p>那么就有：</p>
<p>$\mathbb{E}[m_t] = (1 - \beta_1) \cdot \mu \sum_{i=1}^{t} \beta_1^{t - i} = \mu \cdot (1 - \beta_1) \cdot \sum_{k=0}^{t-1} \beta_1^k$</p>
<p>这是一个等比数列，求和后得到：</p>
<p>$\mathbb{E}[m_t] = \mu \cdot (1 - \beta_1) \cdot \frac{1 - \beta_1^t}{1 - \beta_1} = \mu \cdot (1 - \beta_1^t)$</p>
<p>所以：</p>
<p>$\boxed{\mathbb{E}[m_t] = \mu \cdot (1 - \beta_1^t)} \quad \text{有偏！}$</p>
<p>如何修正？</p>
<p>为了得到无偏估计，我们让：</p>
<p>$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$</p>
<p>那么：</p>
<p>$\mathbb{E}[\hat{m}_t] = \frac{\mathbb{E}[m_t]}{1 - \beta_1^t} = \mu$</p>
<p>成功修正偏差 🎉！</p>
<p>同理：</p>
<p>对于二阶动量 $v_t$，完全一样的推理过程也可以得到：</p>
<p>$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$</p>
</blockquote>
<br>
<p><strong>Adam公式：</strong></p>
<ol>
<li>
<p>一阶矩估计（类似动量）：</p>
<p>$m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_\theta J(\theta)$</p>
</li>
<li>
<p>二阶矩估计（平方梯度）：</p>
<p>$v_t = \beta_2 v_{t-1} + (1 - \beta_2)(\nabla_\theta J(\theta))^2$</p>
</li>
<li>
<p>偏差校正：</p>
<p>$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$</p>
</li>
<li>
<p>参数更新：</p>
<p>$\theta := \theta - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$</p>
</li>
</ol>
<ul>
<li>通常 $\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>自适应调整每个参数的学习率</li>
<li>适用于大规模数据和参数模型，深度学习中的默认选择之一</li>
</ul>
<br>
<h3 id="梯度消失-爆炸">梯度消失&amp;爆炸</h3>
<p>梯度消失是指，损失函数对网络中某参数计算梯度，由于链式法则，计算链过长，得到的梯度计算结果接近0</p>
<p>可能出现：</p>
<ol>
<li>参数初始化存在问题，权重参数w_i过小导致梯度计算值接近0</li>
<li>参数/激活函数存在非线性运算，值被快速缩小导致梯度计算值接近0</li>
</ol>
<p>参数的更新依赖于梯度，当出现梯度消失时，参数无法得到更新</p>
<br>
<p><strong>解决梯度消失问题的方法：</strong></p>
<ol>
<li>使用恰当的激活函数：某些激活函数（如ReLU和Leaky ReLU）在反向传播过程中更不容易出现梯度消失问题，可以考虑使用它们替代Sigmoid和Tanh。</li>
<li>批量归一化（Batch Normalization）：批量归一化可以加速训练过程，还可以缓解梯度消失问题，使得网络更稳定和更易训练。</li>
<li>使用残差连接（Residual Connections）：残差连接可以跳过某些层，将输入直接与输出相加，有助于信息的传递和梯度的流动，减少梯度消失问题。</li>
<li>调整网络架构：适当调整网络的深度，避免设计过深的网络结构，也有助于减少梯度消失的影响。</li>
</ol>
<p><strong>解决梯度爆炸问题的方法：</strong></p>
<ol>
<li>梯度截断（Gradient Clipping）：设置一个梯度阈值，在反向传播过程中，如果梯度超过该阈值，则将其裁剪为阈值以内的数值，避免梯度爆炸。</li>
<li>使用恰当的权重初始化：合适的权重初始化可以减少梯度爆炸问题。例如，Xavier/Glorot初始化针对Sigmoid和Tanh激活函数的网络效果较好，而He初始化针对ReLU激活函数的网络效果较好。</li>
<li>减少学习率：较小的学习率可以缓解梯度爆炸的影响，但要注意不要将学习率设置得过小，以免影响收敛速度。</li>
<li>批量归一化（Batch Normalization）：同样，批量归一化在训练过程中有助于控制梯度的大小，减少梯度爆炸问题。</li>
</ol>
<br>
<h3 id="正则化-归一化">正则化&amp;归一化</h3>
<p><strong>正则化（Regularization）：</strong> 正则化是通过在损失函数中添加一个额外的项，来限制模型参数的大小，从而避免过拟合问题。常见的正则化项有L1正则化和L2正则化。</p>
<ul>
<li>L1正则化：在损失函数中添加模型参数的绝对值之和。L1正则化有助于稀疏模型，即将一些参数的值压缩为0，从而减少模型的复杂度。</li>
<li>L2正则化：在损失函数中添加模型参数的平方之和。L2正则化对参数的惩罚更加平滑，通常会让参数接近于0，但不会严格地等于0。</li>
</ul>
<p>正则化的目的是防止模型在训练集上过度拟合，使得模型能够更好地泛化到未见过的新数据上。</p>
<p>假设我们在线性回归中训练一个模型：</p>
<p>原始损失函数（MSE）：</p>
<p>$Loss = (y - ŷ )²$</p>
<p>加入 L2 正则化后：</p>
<p>$Loss = (y - ŷ )² + λ * ||w||²$</p>
<p>其中：</p>
<ul>
<li><code>||w||²</code> 表示所有权重的平方和</li>
<li><code>λ</code> 是正则化强度（超参数）</li>
</ul>
<p>➡️ <strong>作用：</strong> 如果某些权重太大，会被惩罚，从而让模型更简单、泛化能力更强。</p>
<br>
<p><strong>归一化（Normalization）：</strong> 归一化是将数据按比例缩放，使其值落在特定范围内。在深度学习中，常见的归一化方法是将输入特征缩放到0和1之间，或者使其均值为0，方差为1。</p>
<ul>
<li>最小-最大归一化（Normalization）：将数据缩放到指定的最小值和最大值之间，公式为：$(x - x_{min}) / (x_{max} - x_{min})$</li>
<li>均值-方差归一化（Standardization）：将数据缩放为均值为0，方差为1的分布，公式为：$(x - mean) / std$</li>
</ul>
<p>归一化的目的是将特征的值统一到相似的范围内，加速模型的训练过程，同时有助于梯度的传播和优化算法的收敛。</p>
<p>常见方法：</p>
<ul>
<li>
<p><strong>Min-Max 归一化：</strong> 把数据压缩到 [0,1] 区间</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x&#x27; = (x - min) / (max - min)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>Z-score 标准化（Standardization）：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x&#x27; = (x - mean) / std</span><br></pre></td></tr></table></figure>
</li>
</ul>
<br>
<p><strong>两者的作用：</strong></p>
<ul>
<li>正则化用于防止过拟合，通过约束模型参数的大小，减少模型的复杂度，提高模型的泛化能力。</li>
<li>归一化用于将数据的特征缩放到统一的范围内，使得训练过程更稳定，加速优化算法的收敛，并且有助于避免梯度消失或梯度爆炸问题。</li>
</ul>
<p>在实际应用中，正则化和归一化通常是一起使用的，以提高深度学习模型的性能和训练效果。</p>
<br>
<h3 id="batch">batch</h3>
<p>在机器学习，特别是深度学习训练中，batch_size 为 1 做两次训练（假设数据集相同）和 batch_size 为 2 做一次训练<strong>不等价</strong>。</p>
<p>这里涉及到的主要区别在于：</p>
<ol>
<li><strong>梯度计算：</strong>
<ul>
<li><strong>Batch Size = 1 (两次训练)：</strong> 每次训练迭代时，模型会计算<strong>一个样本</strong>的损失，并基于这一个样本的损失来计算梯度。然后，优化器会使用这个梯度来更新模型的权重。这意味着梯度更新的方差会非常大，因为每次更新都只依赖于一个非常小的“样本”信息。</li>
<li><strong>Batch Size = 2 (一次训练)：</strong> 每次训练迭代时，模型会计算<strong>两个样本</strong>的损失，然后将这两个样本的梯度<strong>平均</strong>（或求和后归一化）起来，得到一个更稳定的梯度。优化器会使用这个平均梯度来更新模型的权重。</li>
</ul>
</li>
<li><strong>梯度更新的频率与方向：</strong>
<ul>
<li>batch_size = 1 做两次训练：会发生两次<strong>独立的</strong>权重更新。两次更新的方向可能差异很大，因为它们基于不同的单个样本。</li>
<li>batch_size = 2 做一次训练：会发生一次权重更新，这个更新的方向是两个样本的梯度<strong>综合平均</strong>的结果。</li>
</ul>
</li>
<li><strong>损失函数评估：</strong>
<ul>
<li>batch_size = 1 做两次训练：损失是在单个样本上评估的。</li>
<li>batch_size = 2 做一次训练：损失是在两个样本的平均或总和上评估的。</li>
</ul>
</li>
<li><strong>收敛性与稳定性：</strong>
<ul>
<li><strong>Batch Size = 1 (随机梯度下降SGD)：</strong> 梯度方差大，导致训练过程更不稳定，路径更“抖动”。虽然理论上SGD在非凸优化中也能找到局部最优解，但收敛速度可能较慢，并且在实践中更容易陷入次优解或在训练过程中出现震荡。这种极端情况下的SGD通常被称为<strong>在线学习（Online Learning）</strong>。</li>
<li><strong>Batch Size = 2 (小批量梯度下降Mini-Batch GD)：</strong> 通过对多个样本的梯度进行平均，可以减少梯度的方差，使梯度方向更稳定、更准确地指向损失函数的下降方向。这通常能带来更稳定的训练过程和更快的收敛速度。</li>
</ul>
</li>
</ol>
<br>
<h3 id="通道">通道</h3>
<img src="/2025/03/19/LLM-Rela/8361e16bac5ee3235ef89c78b1a1cf6b.png" class="" title="channel">
<p>多通道卷积过程</p>
<p>输入一张三通道的图片，有多个卷积核进行卷积，并且每个卷积核都有三通道，分别对这张输入图片的三通道进行卷积操作。每个卷积核，分别输出三个通道，这三个通道进行求和，得到一个featuremap，有多少个卷积核，就有多少个featuremap</p>
<br>
<h3 id="RNN-LSTM">RNN&amp;LSTM</h3>
<h4 id="RNN">RNN</h4>
<img src="/2025/03/19/LLM-Rela/e38fac064524158e493a66adb2caed6e.png" class="" title="RNN">
<br>
<h4 id="LSTM">LSTM</h4>
<p><strong>输出</strong></p>
<p>短期记忆 $h_t$，长期记忆 $c_t$，input $x_t$<br>
$$<br>
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)<br>
$$</p>
<p>$$<br>
h_t = o_t \odot \tanh(c_t)<br>
$$</p>
<p><strong>遗忘门(蓝色)</strong><br>
$$<br>
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)<br>
$$</p>
<p><strong>输入门和候选记忆</strong></p>
<ul>
<li>输入门控制当前输入信息 $x_t$ 的写入程度：</li>
</ul>
<p>$$<br>
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)<br>
$$</p>
<ul>
<li>候选记忆生成新的候选信息 $\tilde{c}_t$：</li>
</ul>
<p>$$<br>
\tilde{c}<em>t = \tanh(W_c \cdot [h</em>{t-1}, x_t] + b_c)<br>
$$</p>
<br>
<p><strong>短期记忆更新</strong></p>
<p>结合遗忘门和输入门的结果更新短期记忆 $c_t$：<br>
$$<br>
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t<br>
$$</p>
<ul>
<li>$\odot$ 表示逐元素相乘。</li>
<li>第一项 $f_t \odot c_{t-1}$ 保留历史信息，第二项 $i_t \odot \tilde{c}_t$ 添加新信息。</li>
</ul>
<img src="/2025/03/19/LLM-Rela/image-20250402161828313.png" class="" title="image-20250402161828313">
<br>
<p><strong>LSTM Process</strong></p>
<img src="/2025/03/19/LLM-Rela/image-20250402162106999.png" class="" title="image-20250402162106999">
<br>
<h2 id="LLM-struc">LLM-struc</h2>
<p>大模型从模型架构上主要分为三种：Only-encoder, Only-Decoder, Encoder-Decoder三种模型架构</p>
<ul>
<li>Only-encoder：例如BERT，通过在大规模无标签文本上进行预训练，然后在下游任务上进行微调，具有强大的语言理解能力和表征能力。</li>
<li>Only-Decoder: 例如GPT，通过在大规模无标签文本上进行预训练，然后在特定任务上进行微调，具有很强的生成能力和语言理解能力。</li>
<li>Encoder-Decoder：例如T5（Text-to-Text Transfer Transformer）可以用于多种自然语言处理任务，如文本分类、机器翻译、问答等。</li>
</ul>
<img src="/2025/03/19/LLM-Rela/v2-ee9b5d4a0761d2d1d10acb37cebefba3_1440w.jpg" class="" title="img">
<br>
<h3 id="Encoder-Decoder">Encoder&amp;Decoder</h3>
<img src="/2025/03/19/LLM-Rela/image-20250706100412469.png" class="" title="image-20250706100412469">
<p>ref: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></p>
<br>
<img src="/2025/03/19/LLM-Rela/image-20250325195428329.png" class="" title="image-20250325195428329">
<p>Transformer架构下的完整流程</p>
<p>假设我们要将英文句子 “I am a student” 翻译成中文 “我 是 一名 学生”。</p>
<ul>
<li><strong>源序列</strong>: “I am a student” -&gt; <code>L_source = 4</code></li>
<li><strong>模型维度</strong>: <code>d_model = 512</code> (为了举例方便，我们忽略多头和维度<code>d_k</code>的切分，假设<code>d_k = d_model</code>)</li>
</ul>
<p><strong>第0步：编码器处理</strong></p>
<ol>
<li>输入 “I am a student” 进入编码器。</li>
<li>经过多层自注意力和前馈网络后，编码器输出一个上下文感知的向量序列。</li>
<li>这个输出的形状是 <code>(batch_size=1, L_source=4, d_model=512)</code>。</li>
<li>这个输出将作为<strong>固定不变</strong>的 <code>K</code> 和 <code>V</code> 提供给解码器的每一个时间步。我们称它们为 <code>K_encoder</code> 和 <code>V_encoder</code>。</li>
</ol>
<p><code>K_encoder</code> 形状: <code>(1, 4, 512)</code><br>
<code>V_encoder</code> 形状: <code>(1, 4, 512)</code></p>
<p><strong>第1步：解码器生成第一个token “我”</strong></p>
<ol>
<li>解码器的输入只有一个起始符 <code>&lt;start&gt;</code>。</li>
<li>经过嵌入和位置编码后，我们得到一个向量，形状为 <code>(1, 1, 512)</code>。</li>
<li>这个向量首先通过<strong>带掩码的自注意力</strong>（在这一步没什么可注意的，因为只有一个token）。</li>
<li>然后，它的输出作为 <code>Q</code> 进入<strong>交叉注意力</strong>层。
<ul>
<li><code>Q</code> 形状: <code>(1, L_q=1, 512)</code></li>
<li><code>K</code> (来自编码器): <code>(1, L_k=4, 512)</code></li>
</ul>
</li>
<li>计算注意力分数 <code>Q @ K^T</code>:
<ul>
<li><code>(1, 1, 512) @ (1, 512, 4)</code> -&gt; 结果形状 <code>(1, 1, 4)</code></li>
<li>这个 <code>(1, 1, 4)</code> 的向量表示 <code>&lt;start&gt;</code> 符为了生成下一个token，应该对 “I”, “am”, “a”, “student” 分别赋予多大的注意力。</li>
</ul>
</li>
<li>用这个分数加权 <code>V_encoder</code>，然后通过前馈网络，最终预测出第一个目标token “我”。</li>
</ol>
<p><strong>第2步：解码器生成第二个token “是”</strong></p>
<ol>
<li>现在解码器的输入是 <code>&lt;start&gt; 我</code>。</li>
<li>经过嵌入和位置编码，我们得到一个向量序列，形状为 <code>(1, 2, 512)</code>。</li>
<li>这个序列通过<strong>带掩码的自注意力</strong>（&quot;我&quot;会注意到<code>&lt;start&gt;</code>，但反之不行，因为有掩码）。</li>
<li>其输出作为 <code>Q</code> 进入<strong>交叉注意力</strong>层。
<ul>
<li><code>Q</code> 形状: <code>(1, L_q=2, 512)</code>  &lt;-- <strong>看，Q的序列长度维度变化了！</strong></li>
<li><code>K</code> (来自编码器): <code>(1, L_k=4, 512)</code> &lt;-- <strong>K保持不变！</strong></li>
</ul>
</li>
<li>计算注意力分数 <code>Q @ K^T</code>:
<ul>
<li><code>(1, 2, 512) @ (1, 512, 4)</code> -&gt; 结果形状 <code>(1, 2, 4)</code></li>
<li>这个 <code>(1, 2, 4)</code> 的矩阵包含了两行信息：
<ul>
<li>第一行：<code>&lt;start&gt;</code> 对源序列的注意力（通常在后续步骤中被忽略，因为我们只关心最后一个token的预测）。</li>
<li>第二行：token “我” 为了生成下一个token，对 “I”, “am”, “a”, “student” 的注意力。</li>
</ul>
</li>
</ul>
</li>
<li>解码器使用这个信息（特别是与 “我” 相关的部分）来预测出第二个token “是”。</li>
</ol>
<p><strong>第3步及以后</strong></p>
<p>这个过程会一直重复。在生成第 <code>t</code> 个token时：</p>
<ul>
<li>解码器的输入是 <code>&lt;start&gt;</code> 加上前 <code>t-1</code> 个已生成的token。</li>
<li>交叉注意力中的 <code>Q</code> 的形状是 <code>(1, L_q=t, 512)</code>。</li>
<li>交叉注意力中的 <code>K</code> 和 <code>V</code> 的形状<strong>始终</strong>是 <code>(1, L_k=4, 512)</code>。</li>
<li>注意力分数矩阵的形状是 <code>(1, t, 4)</code>。</li>
</ul>
<p>这个过程一直持续，直到解码器生成一个结束符 <code>&lt;end&gt;</code>。</p>
<br>
<h3 id="Encoder-Only">Encoder Only</h3>
<img src="/2025/03/19/LLM-Rela/image-20250706105452257.png" class="" title="image-20250706105452257">
<p>ref: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
<h3 id="Decoder-Only">Decoder Only</h3>
<img src="/2025/03/19/LLM-Rela/LLM-structure.png" class="" title="structure">
<img src="/2025/03/19/LLM-Rela/LLM-structure-moe.png" class="" title="structure-moe">
<p>ref: <a target="_blank" rel="noopener" href="https://github.com/jingyaogong/minimind">minimind</a></p>
<p>decoder-only GPT流程</p>
<p><strong>基本维度设定：</strong></p>
<ul>
<li><code>batch_size</code> (B): 一次并行处理的序列数量。</li>
<li><code>sequence_length</code> (L): 输入序列的长度（或最大上下文窗口）。</li>
<li><code>d_model</code> (D): 模型维度，也是每个token经过Transformer层后的隐藏状态维度。</li>
<li><code>vocab_size</code> (V): 词汇表的大小，即模型能生成或识别的唯一词汇的数量。</li>
</ul>
<br>
<p><strong>Transformer Decoder Block 内部的输出 (经过 Layer Norm 之前的 Feed Forward 输出，以及最终 Layer Norm 之后的输出)</strong></p>
<p>在每个 Transformer Decoder Block 内部，<code>Masked Multi Self Attention</code> 和 <code>Feed Forward</code> 都会处理输入，并输出与输入维度相同的张量。</p>
<ul>
<li><strong>输入 <code>Text &amp; Position Embed</code> 后的张量维度：</strong> <code>(B, L, D)</code></li>
<li><strong>经过 <code>Masked Multi Self Attention</code> 后的张量维度：</strong> <code>(B, L, D)</code></li>
<li><strong>经过第一个 <code>Layer Norm</code> 后的张量维度：</strong> <code>(B, L, D)</code></li>
<li><strong>经过 <code>Feed Forward</code> 后的张量维度：</strong> <code>(B, L, D)</code></li>
<li><strong>经过第二个 <code>Layer Norm</code> 后的张量维度：</strong> <code>(B, L, D)</code></li>
</ul>
<p>这个 <code>(B, L, D)</code> 的张量代表了输入序列中每个token在当前层经过处理后，所获得的上下文感知的隐藏状态（或特征向量）。GPT模型通常有多个这样的Decoder Block（例如GPT-2有12层，GPT-3有96层），每一层都会接收 <code>(B, L, D)</code> 的输入，并输出同样维度的张量，作为下一层的输入。</p>
<br>
<p><strong>整个GPT模型最终的输出 (用于文本预测)</strong></p>
<p>当所有 <code>12x</code> （或更多）的Decoder Block 处理完成后，最后一个Decoder Block 的输出依然是 <code>(B, L, D)</code>。</p>
<p>这个 <code>(B, L, D)</code> 张量，现在包含了每个token在<strong>所有层</strong>处理后、充分融合了上下文信息的表示。</p>
<p>为了预测下一个词，GPT通常会做以下处理：</p>
<ol>
<li>
<p><strong>取最后一个 token 的隐藏状态（用于生成）：</strong></p>
<ul>
<li>在自回归生成文本时，我们通常只关心预测序列中<strong>下一个词</strong>。这个预测是基于<strong>当前输入序列的完整上下文</strong>。</li>
<li>而根据掩码自注意力的原理，输入序列中<strong>最后一个 token 的隐藏状态</strong>(<code>L-1</code> 索引位置的 token) 已经包含了整个输入序列（从第一个 token 到最后一个 token）的上下文信息。</li>
<li>所以，我们会从 <code>(B, L, D)</code> 这个张量中，取出**最后一个时间步（<code>L-1</code> 索引）**的隐藏状态。</li>
<li>这个操作会得到一个维度为 <code>(B, D)</code> 的张量。</li>
</ul>
</li>
<li>
<p><strong>线性投影到词汇表大小：</strong></p>
<ul>
<li>这个 <code>(B, D)</code> 的张量会被送入一个<strong>线性层（Linear Layer）</strong>，有时也称为“语言模型头（Language Model Head）”或“文本预测头（Text Prediction Head）”。</li>
<li>这个线性层的权重矩阵维度是 <code>(D, V)</code>。</li>
<li>线性层进行矩阵乘法：<code>(B, D) @ (D, V) = (B, V)</code>。</li>
</ul>
</li>
<li>
<p><strong>最终输出维度：</strong></p>
<ul>
<li>所以，用于预测下一个词的**原始逻辑值（logits）**的张量维度是 <code>(B, V)</code>。</li>
<li>每一行（维度为 <code>V</code>）代表了该批次中一个序列的下一个词在词汇表上的概率分布（在经过 Softmax 激活函数之前）。</li>
</ul>
</li>
</ol>
<p>GPT在训练阶段（或标准的自回归生成时），最后从Decoder层输出的张量维度通常是 <code>(batch_size, sequence_length, d_model)</code>。但是，为了进行下一个词的预测，它会专门<strong>提取最后一个时间步的表示</strong>，然后通过一个线性层将其投影到词汇表大小的维度，最终得到 <code>(batch_size, vocab_size)</code> 的 logits 张量。</p>
<br>
<h3 id="Tokenizer">Tokenizer</h3>
<p>分词表</p>
<p>案例：BPE (Byte Pair Encoding)  ref: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.16609">QWEN TECHNICAL REPORT</a></p>
<p>假设我们有以下非常小的文本语料库：</p>
<p><code>&quot;low low low low low low low low low low&quot;</code><br>
<code>&quot;lower lower lower lower lower lower lower lower&quot;</code><br>
<code>&quot;newest newest newest newest&quot;</code><br>
<code>&quot;widest widest widest widest&quot;</code></p>
<p>并且我们希望通过 BPE 来构建一个词汇表。最初，我们的“词汇表”可能只包含所有的独立字符。</p>
<p><strong>初始状态：</strong></p>
<ul>
<li><strong>语料中的独立字符（初始token）：</strong> <code>l, o, w, e, r, n, s, t, i, d</code> (以及一个空格字符，我们这里为了简化暂时忽略它，但实际BPE会处理所有字节)</li>
<li><strong>当前词汇表：</strong> <code>[l, o, w, e, r, n, s, t, i, d]</code></li>
</ul>
<p><strong>BPE 步骤：</strong></p>
<p>BPE 的核心是<strong>迭代地寻找语料中出现频率最高的“字节对”（或字符对）并将其合并成一个新的单元。</strong></p>
<p><strong>第一次迭代：</strong></p>
<ol>
<li>
<p><strong>统计字符对频率：</strong></p>
<ul>
<li><code>lo</code>: 出现很多次（来自 “low”, “lower”）</li>
<li><code>ow</code>: 出现很多次（来自 “low”, “lower”）</li>
<li><code>er</code>: 出现很多次（来自 “lower”）</li>
<li><code>ne</code>: 出现多次（来自 “newest”）</li>
<li><code>es</code>: 出现多次（来自 “newest”）</li>
<li><code>st</code>: 出现多次（来自 “newest”, “widest”）</li>
<li><code>wi</code>: 出现多次（来自 “widest”）</li>
<li><code>id</code>: 出现多次（来自 “widest”）</li>
<li><code>de</code>: 出现多次（来自 “widest”）</li>
</ul>
</li>
<li>
<p><strong>找到频率最高的对：</strong> 假设 <code>(l, o)</code> 和 <code>(o, w)</code> 的频率非常高（因为 “low” 和 “lower” 都包含它们）。<br>
我们选择 <code>(l, o)</code> 和 <code>(o, w)</code> 进行合并。为了简化，我们先合并 <code>(l, o)</code>。</p>
</li>
<li>
<p><strong>合并 <code>lo</code>：</strong> 将所有 <code>l o</code> 替换为新的 token <code>lo</code>。</p>
<ul>
<li><code>&quot;low low ...&quot;</code> 变成 <code>&quot;low low ...&quot;</code> (lo 依然是 lo，但现在是一个整体token)</li>
<li><code>&quot;lower lower ...&quot;</code> 变成 <code>&quot;lower lower ...&quot;</code></li>
<li><strong>新的词汇表：</strong> <code>[l, o, w, e, r, n, s, t, i, d, lo]</code></li>
</ul>
</li>
</ol>
<p><strong>第二次迭代：</strong></p>
<ol>
<li>
<p><strong>统计新的字符/token 对频率：</strong></p>
<ul>
<li>现在 <code>(lo, w)</code> 出现的频率很高（来自 “low”, “lower”）</li>
<li><code>(e, r)</code> 频率很高</li>
<li><code>(n, e)</code> 频率很高</li>
<li><code>(e, s)</code> 频率很高</li>
<li><code>(s, t)</code> 频率很高</li>
</ul>
</li>
<li>
<p><strong>找到频率最高的对：</strong> 假设 <code>(lo, w)</code> 是频率最高的对。</p>
</li>
<li>
<p><strong>合并 <code>low</code>：</strong> 将所有 <code>lo w</code> 替换为新的 token <code>low</code>。</p>
<ul>
<li><code>&quot;low low ...&quot;</code> 变成 <code>&quot;low low ...&quot;</code> (现在 <code>low</code> 是一个整体 token)</li>
<li><code>&quot;lower lower ...&quot;</code> 中的 <code>low</code> 也变成一个整体 token。</li>
<li><strong>新的词汇表：</strong> <code>[l, o, w, e, r, n, s, t, i, d, lo, low]</code></li>
</ul>
</li>
</ol>
<p><strong>第三次迭代：</strong></p>
<ol>
<li>
<p><strong>统计新的字符/token 对频率：</strong></p>
<ul>
<li><code>(low, er)</code> 出现频率很高（来自 “lower”）</li>
<li><code>(n, ew)</code> (如果 <code>ew</code> 被合并了)</li>
<li><code>(ne, st)</code></li>
<li><code>(wi, de)</code> (如果 <code>de</code> 被合并了)</li>
</ul>
</li>
<li>
<p><strong>找到频率最高的对：</strong> 假设 <code>(low, er)</code> 是频率最高的对。</p>
</li>
<li>
<p><strong>合并 <code>lower</code>：</strong> 将所有 <code>low er</code> 替换为新的 token <code>lower</code>。</p>
<ul>
<li><code>&quot;lower lower ...&quot;</code> 变成 <code>&quot;lower lower ...&quot;</code> (现在 <code>lower</code> 是一个整体 token)</li>
<li><strong>新的词汇表：</strong> <code>[l, o, w, e, r, n, s, t, i, d, lo, low, lower]</code></li>
</ul>
</li>
</ol>
<p><strong>继续迭代…</strong></p>
<p>这个过程会一直重复：</p>
<ul>
<li>统计当前所有 token 序列中<strong>相邻 token 对</strong>的频率。</li>
<li>选择频率最高的对。</li>
<li>将该对合并成一个新的、更长的 token。</li>
<li>将新 token 加入词汇表。</li>
<li>更新语料中的表示（将旧的对替换为新的 token）。</li>
</ul>
<br>
<p>对应于项目中的 <code>tokenizer.json</code> 文件，它是一个至关重要的组件，其核心功能是将人类可读的文本转换为模型可理解的数字 ID 序列。可以将其理解为模型的“字典”或“编码器”。</p>
<p>具体来说，<code>tokenizer.json</code> 文件中包含了：</p>
<ol>
<li><strong>词表（Vocabulary）</strong>：这是最主要的部分，它列出了分词器识别的所有词元（tokens），每个词元都对应一个唯一的数字 ID。这些词元可以是单个字符、常用单词、词根、词缀甚至是不规则的子词单元（subword units），例如通过BPE（Byte Pair Encoding）或WordPiece等算法生成的。</li>
<li><strong>分词规则（Tokenization Rules）</strong>：除了词表本身，<code>tokenizer.json</code> 还定义了如何将原始文本拆分成这些词元序列的规则。这包括预处理步骤（如大小写转换、标点符号处理）、分词算法的配置以及如何处理未知词元（Out-Of-Vocabulary, OOV）等。</li>
</ol>
<p><strong>词表大小（Vocabulary Size）</strong> 指的是 <strong><code>tokenizer.json</code> 中定义的词元数量</strong>（即词表中词元ID的最大值加一），再加上 <strong>模型或分词器预设的特殊字符（Special Tokens）的数量</strong>。</p>
<p><strong>特殊字符</strong>通常包括：</p>
<ul>
<li><strong><code>[CLS]</code> (Classification Token)</strong>：在BERT等模型中用于表示句子的开头，其输出常用于分类任务。</li>
<li><strong><code>[SEP]</code> (Separation Token)</strong>：用于分隔不同的句子或文本片段。</li>
<li><strong><code>[PAD]</code> (Padding Token)</strong>：用于将不同长度的序列填充到相同的长度，以便于批处理。</li>
<li><strong><code>[UNK]</code> (Unknown Token)</strong>：当分词器遇到不在词表中的词元时，会用此标记替代。</li>
<li><strong><code>[MASK]</code> (Mask Token)</strong>：在预训练任务（如掩码语言模型）中用于替换被遮蔽的词元。</li>
</ul>
<p>因此，<strong>词表大小 = <code>tokenizer.json</code> 中唯一词元的数量 + 特殊字符的数量</strong>。这个大小直接决定了模型能够理解和表示的词元种类，也影响着模型的参数量和性能。</p>
<br>
<h3 id="Embedding">Embedding</h3>
<p>将 <strong>token ID 序列</strong> 转换为 <strong>连续的、稠密的向量表示</strong></p>
<p>例如，</p>
<ol>
<li>
<p><strong>原始文本:</strong> “The cat sat on the mat.”</p>
</li>
<li>
<p><strong>分词器 (Tokenizer):</strong></p>
<ul>
<li>将原始文本分解成一个个独立的单元，通常是词（word）或子词（subword）。</li>
<li>例如：<code>[&quot;The&quot;, &quot;cat&quot;, &quot;sat&quot;, &quot;on&quot;, &quot;the&quot;, &quot;mat&quot;, &quot;.&quot;]</code></li>
<li>分词器还会将这些词/子词映射到它们对应的<strong>整数ID (Integer ID)</strong>。</li>
<li>例如：<code>[101, 234, 567, 890, 101, 321, 999]</code> (这只是示例ID)</li>
</ul>
</li>
<li>
<p><strong>Embedding 层:</strong></p>
<ul>
<li>模型接收的输入是这些<strong>整数ID序列</strong>。</li>
<li>Embedding层本质上是一个<strong>查找表（lookup table）</strong>。</li>
<li>当接收到一个整数ID时，它会去这个查找表中找到该ID对应的<strong>预训练好的（或随机初始化后在训练中学习到的）低维、稠密的浮点数向量</strong>。</li>
<li>例如，ID <code>234</code> (对应“cat”) 可能被查找到一个像 <code>[0.1, -0.3, 0.8, ..., 0.5]</code> 这样的512维向量。</li>
<li>这些向量就是我们所说的<strong>词向量（Word Embeddings）<strong>或</strong>词嵌入</strong>。</li>
<li>这些词向量的特点是：
<ul>
<li><strong>低维：</strong> 相比独热编码的词汇表大小，词向量的维度通常是几十到几百（例如，50, 100, 300, 512, 768等）。</li>
<li><strong>稠密：</strong> 向量中的每个元素都是一个非零的浮点数。</li>
<li><strong>语义信息：</strong> 通过大量的语料库训练，这些向量能够捕捉词语的语义和语法信息，使得语义相似的词在向量空间中距离更近。</li>
</ul>
</li>
</ul>
</li>
</ol>
<br>
<p>案例：Word2Vec训练方法</p>
<p>Word2Vec的CBOW（Continuous Bag of Words）模型是一种通过上下文词预测目标词的神经网络模型。以下是其训练流程的详细说明，并结合具体例子进行解释：</p>
<ol>
<li>数据准备</li>
</ol>
<p>首先，需要准备训练数据，通常是大量的文本语料。文本数据需要进行分词等预处理，将文本转换为词语序列。例如，句子“I learn NLP everyday”会被分词为<code>[&quot;I&quot;, &quot;learn&quot;, &quot;NLP&quot;, &quot;everyday&quot;]</code>。</p>
<ol>
<li>
<p>创建上下文窗口</p>
<p>对于每个目标词，CBOW模型定义了一个上下文窗口。窗口大小由超参数<code>window</code>指定，表示目标词左右两侧的词语数目。例如，窗口大小为2时，目标词“NLP”的上下文词为<code>[&quot;I&quot;, &quot;learn&quot;, &quot;everyday&quot;]</code>。</p>
</li>
<li>
<p>构建训练样本</p>
<p>对于每个目标词，CBOW模型从其上下文窗口中收集上下文词。每个训练样本由上下文词构成，目标是预测目标词。例如，目标词“NLP”的训练样本为<code>&#123;&quot;context&quot;: [&quot;I&quot;, &quot;learn&quot;, &quot;everyday&quot;], &quot;target&quot;: &quot;NLP&quot;&#125;</code>。</p>
</li>
<li>
<p>模型结构</p>
<p>CBOW模型是一个简单的三层神经网络，包括输入层、隐藏层和输出层： - <strong>输入层</strong>：上下文词用one-hot向量表示。例如，词汇表大小为10,000，单词“I”可能表示为<code>[1, 0, 0, ..., 0]</code>。 - <strong>隐藏层</strong>：通过词向量矩阵（Embedding Matrix）将输入的one-hot向量转换为低维词向量（通常是100～300维）。然后将所有上下文词的词向量相加取平均，作为隐藏层向量。 - <strong>输出层</strong>：隐藏层向量乘以输出权重矩阵，得到输出向量。使用Softmax函数计算目标词的概率分布。</p>
</li>
<li>
<p>训练目标</p>
<p>CBOW模型的训练目标是最大化给定上下文词时目标词的条件概率，即最大化$P(w_t | w_{t-c}, w_{t-c+1}, …, w_{t+c})$，其中$w_t$是目标词，$w_{t-c}$到$w_{t+c}$是上下文词。</p>
</li>
<li>
<p>梯度下降</p>
<p>使用梯度下降或其变种，通过反向传播算法调整嵌入层的权重，使得模型的预测更接近实际的目标词。</p>
</li>
<li>
<p>重复迭代</p>
<p>重复以上步骤多次，直到模型收敛到一个合适的状态。每一轮迭代都遍历整个训练数据。</p>
</li>
</ol>
<br>
<p>例子</p>
<p>假设语料为“I learn NLP everyday”，目标词为“NLP”，上下文词为<code>[&quot;I&quot;, &quot;learn&quot;, &quot;everyday&quot;]</code>：</p>
<ol>
<li>将上下文词转换为one-hot向量。</li>
<li>将one-hot向量乘以输入权重矩阵，得到词向量。</li>
<li>将所有上下文词的词向量相加取平均，得到隐藏层向量。</li>
<li>将隐藏层向量乘以输出权重矩阵，得到输出向量。</li>
<li>使用Softmax函数计算目标词“NLP”的概率分布。</li>
<li>通过损失函数（如负对数似然）计算预测误差，并使用梯度下降更新模型参数。</li>
</ol>
<p>以上便是CBOW模型的完整训练流程。</p>
<br>
<h3 id="Positional-Encoding">Positional Encoding</h3>
<p>词向量 → 含位置信息词向量，加入位置信息（多个正弦函数）到词向量</p>
<h4 id="Base">Base</h4>
<p>$$<br>
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})<br>
$$</p>
<p>$$<br>
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})<br>
$$</p>
<p>其中，$pos$ 表示该token在token序列中的位置，$i$ 表示 $d_{model}$ 中的第 $i$ 个维度</p>
<p>ref: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></p>
<br>
<h4 id="RoPE">RoPE</h4>
<br>
<h3 id="Attention">Attention</h3>
<p>Transformer模型中隐藏层（hidden layer）的维度大小，也就是模型宽度（model width）</p>
<p>在Transformer架构中，这通常对应于：</p>
<ul>
<li>每个Transformer层的输入/输出维度（即隐藏状态的维度d_model）</li>
<li>注意力机制中Q/K/V向量的行维度（当使用标准实现时）</li>
<li>前馈网络层的输入/输出维度</li>
</ul>
<h4 id="注意力">注意力</h4>
<h5 id="自注意力-decoder">自注意力 - decoder</h5>
<img src="/2025/03/19/LLM-Rela/image-20250404192612286.png" class="" title="image-20250404192612286">
<img src="/2025/03/19/LLM-Rela/image-20250325195443397.png" class="" title="image-20250325195443397">
<p>ref: <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1XH4y1T76e/">从编解码和词嵌入开始，一步一步理解Transformer，注意力机制(Attention)的本质是卷积神经网络(CNN)</a><br>
$$<br>
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{\top}}{\sqrt{d}}\right)V<br>
$$<br>
$QK^T$ 相当于计算 $token_i$ 与 $token_j$ 对应词向量的相似度</p>
<img src="/2025/03/19/LLM-Rela/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9F%A9%E9%98%B5%E5%9B%BE.jpg" class="" title="img">
<p>softmax <strong>是按行（row-wise）进行的</strong></p>
<p>Attention是对每个token <strong>value化的词向量矩阵</strong> 进行相关性修正，得到注意力矩阵；</p>
<p>其中，每行<strong>每个修正后词向量</strong>包含其他相似词词向量的加和</p>
<blockquote>
<p>维度变化</p>
<p>假设 $W_Q$ 的维度为 ($d_{model}$, $d_k$)，输入形状为 (batch_size, $L_{actual}$, $d_{model}$)，这里的 $L_{actual}$ 为输入token序列长度，则 $QK^{\top}$ 的维度为 (batch_size, $L_{actual}$, $L_{actual}$)，V的维度为(batch_size, $L_{actual}$, $d_v$)</p>
</blockquote>
<br>
<h5 id="交叉注意力-encoder">交叉注意力 - encoder</h5>
<img src="/2025/03/19/LLM-Rela/image-20250325195459862.png" class="" title="image-20250325195459862">
<br>
<h4 id="多头注意力">多头注意力</h4>
<img src="/2025/03/19/LLM-Rela/multi-head-%E6%8B%BC%E6%8E%A5.jpg" class="" title="img">
<blockquote>
<p>将 $W_K, W_Q, W_V$ 竖着切分为 N 个，这意味着长度为  $L_{actual}$的token序列要与每个头进行注意力计算，得到 ($L_{actual}$, $d_h$) 的矩阵，然后拼接得到 ($L_{actual}$, $d_K$) 的矩阵， 进入(可选) 最终线性投影 $W_O$，size 为 $\mathbb{R}^{d_K \times d_{model}}$</p>
</blockquote>
<p>1.拆分</p>
<p>原始的单头注意力中，$W_K, W_Q, W_V \in \mathbb{R}^{d_{model} \times d_K}$<br>
在多头机制中，每个头的参数矩阵被水平拆分为更小的矩阵：</p>
<ul>
<li>
<p><strong>第 $h$ 个头的参数</strong>：</p>
<p>$W_K^{(h)}, W_Q^{(h)}, W_V^{(h)} \in \mathbb{R}^{d_{model} \times d_h}$，其中 $d_h = d_K/N_h$。例如，若总维度 $d_K=512$，头数 $N_h=8$，则每个头的维度 $d_h=64$。</p>
</li>
</ul>
<p><strong>拆分方式</strong>：</p>
<ul>
<li><strong>水平拆分</strong>：将原始矩阵按列切分（如 $W_K$ 被拆为 $[W_K^{(1)}, W_K^{(2)}, …, W_K^{(N_h)}]$），每个子矩阵对应一个头的参数。</li>
</ul>
<p>2.独立计算注意力</p>
<p>每个头 $h$ 使用自己的参数矩阵独立计算注意力：</p>
<ul>
<li>
<p><strong>输入 $x$ 通过第 $h$ 个头</strong>：</p>
<p>$K^{(h)} = x W_K^{(h)}$，</p>
<p>$Q^{(h)} = x W_Q^{(h)}$，</p>
<p>$V^{(h)} = x W_V^{(h)}$。</p>
<p>注意，当前步得到的 $K^{(h)}, Q^{(h)}, V^{(h)}$ 等价于直接从原始 $K, Q, V$ 中拆分</p>
</li>
<li>
<p><strong>计算注意力输出</strong>：<br>
$\text{Attn}_h(x) = \text{softmax}\left(\frac{Q^{(h)} K^{(h)\top}}{\sqrt{d_h}}\right) V^{(h)}$。</p>
</li>
</ul>
<blockquote>
<p>$K^{(h)}Q^{(h)}$ size 为 (L_actual, L_actual), $V^{(h)}$ size 为 (L_actual, d_h)</p>
</blockquote>
<p>3.整合</p>
<p>所有头的输出通过**拼接（Concatenate）**整合：</p>
<ul>
<li>
<p><strong>拼接（标准Transformer）</strong>：</p>
<p>$\text{MHA}(x) = Concat(\text{Attn}<em>1(x), \text{Attn}<em>2(x), …, \text{Attn}</em>{N_h}(x)) W_O$，其中 $W_O \in \mathbb{R}^{d</em>{K} \times d_{model}}$ 是输出投影矩阵。</p>
</li>
</ul>
<br>
<p>测试</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">Wq = Wk = Wv = np.array([</span><br><span class="line">    [1, 0, 0, 0],</span><br><span class="line">    [0, 2, 0, 0],</span><br><span class="line">    [0, 0, 3, 0],</span><br><span class="line">    [0, 0, 0, 4]</span><br><span class="line">]) </span><br><span class="line"></span><br><span class="line">输入 X:</span><br><span class="line"> [[1 0 1 0]</span><br><span class="line"> [0 2 0 2]]</span><br><span class="line"></span><br><span class="line">--- 单头 Attention ---</span><br><span class="line">QK^T / sqrt(dk):</span><br><span class="line"> [[ 5.  0.]</span><br><span class="line"> [ 0. 40.]]</span><br><span class="line">Attention Weights:</span><br><span class="line"> [[9.93307149e-01 6.69285092e-03]</span><br><span class="line"> [4.24835426e-18 1.00000000e+00]]</span><br><span class="line">Single-Head Output:</span><br><span class="line"> [[9.93307149e-01 2.67714037e-02 2.97992145e+00 5.35428074e-02]</span><br><span class="line"> [4.24835426e-18 4.00000000e+00 1.27450628e-17 8.00000000e+00]]</span><br><span class="line"></span><br><span class="line">--- 多头 Attention（Head 1）---</span><br><span class="line">Q1K1^T / sqrt(dk):</span><br><span class="line"> [[ 0.70710678  0.        ]</span><br><span class="line"> [ 0.         11.3137085 ]]</span><br><span class="line">Attention Weights Head 1:</span><br><span class="line"> [[6.69761549e-01 3.30238451e-01]</span><br><span class="line"> [1.22043184e-05 9.99987796e-01]]</span><br><span class="line">Head 1 Output:</span><br><span class="line"> [[6.69761549e-01 1.32095380e+00]</span><br><span class="line"> [1.22043184e-05 3.99995118e+00]]</span><br><span class="line"></span><br><span class="line">--- 多头 Attention（Head 2）---</span><br><span class="line">Q2K2^T / sqrt(dk):</span><br><span class="line"> [[ 6.36396103  0.        ]</span><br><span class="line"> [ 0.         45.254834  ]]</span><br><span class="line">Attention Weights Head 2:</span><br><span class="line"> [[9.98280432e-01 1.71956818e-03]</span><br><span class="line"> [2.21858114e-20 1.00000000e+00]]</span><br><span class="line">Head 2 Output:</span><br><span class="line"> [[2.99484130e+00 1.37565454e-02]</span><br><span class="line"> [6.65574341e-20 8.00000000e+00]]</span><br><span class="line"></span><br><span class="line">Multi-Head Output:</span><br><span class="line"> [[6.69761549e-01 1.32095380e+00 2.99484130e+00 1.37565454e-02]</span><br><span class="line"> [1.22043184e-05 3.99995118e+00 6.65574341e-20 8.00000000e+00]]</span><br></pre></td></tr></table></figure>
<br>
<h4 id="掩码注意力-decoder">掩码注意力 - decoder</h4>
<img src="/2025/03/19/LLM-Rela/mask-attention-map.jpg" class="" title="img">
<p><strong>掩码（Masking）在计算注意力权重（Attention Weights）时生效，注意力权重是通过Q（Query）和K（Key）的点积计算得出的。</strong> Value（V）是根据这些权重加权求和的。严格来说，掩码不是直接作用在Q、K、V的原始数值上，而是作用在<strong>Q和K计算得到的注意力分数（logits）上</strong>，以此来决定哪些K不能被Q关注到。</p>
<blockquote>
<p>需要说明的是，掩码通过作用于 $QK^T$ ，使得位置靠前的token 在 $softmax(\frac{QK^T}{\sqrt{d}})$ 后，其注意力权重向量（行向量）从结果上来说后位均为0，因此得到的修正词向量（注意力矩阵的每行），在原token序列越靠前的，其词向量变化越小（<strong>融入的上文信息量</strong>越少），越靠后的，在注意力机制下变化越大（融入的上文信息量越大）</p>
</blockquote>
<br>
<h4 id="MHA、MQA、GQA-MLA">MHA、MQA、GQA&amp;MLA</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/21151178690">https://zhuanlan.zhihu.com/p/21151178690</a></p>
<img src="/2025/03/19/LLM-Rela/v2-b257d8660af7678f9c9bdc14d095b6d3_1440w.jpg" class="" title="img">
<p>MQA(<strong>M</strong>ulti-<strong>Q</strong>uery <strong>A</strong>ttention): 每个 head 的 Query 共享K和V矩阵，KV cache的内存占用降为 $\frac{1}{n}$</p>
<p>GQA(<strong>G</strong>rouped-<strong>Q</strong>uery <strong>A</strong>ttention): 每个 head 的 Query 按组区分，共享K和V矩阵，$g = 1$ 为MQA，$g = n$ 为MHA</p>
<blockquote>
<p>GQA 相对于 MHA</p>
<p>MHA是对 $W_Q$ $W_K$ $W_V $按列划分为 num_heads 个头，每个头的维度为 $d_{head}$</p>
<p>GQA  是对 $W_Q$ 按列划分为 num_heads 个头，$W_K$ $W_V$ 按列划分为 num_heads / g 个头，每个头的维度为 $d_{head}$，头的维度一致</p>
<p>（拼起来的话K头和V头大小小于Q头</p>
</blockquote>
<p>MLA(<strong>M</strong>ulti-head <strong>L</strong>atent <strong>A</strong>ttention):</p>
<img src="/2025/03/19/LLM-Rela/v2-1eda59791cb4a0a09f9ebce4fbb23865_1440w.jpg" class="" title="img">
<br>
<h3 id="Layer-Norm-Residual-Network">Layer Norm &amp; Residual Network</h3>
<p>Batch Normalization 是对 <strong>所有样本的同一特征维度</strong> 分别做归一化（按列操作）</p>
<p>Layer Normalization 是对 <strong>单个样本的所有特征维度</strong> 做归一化（按行操作）</p>
<p>例如：BN是对特征 $i$ 进行归一，LN是对样本 $x_i$ 进行归一</p>
<table>
<thead>
<tr>
<th style="text-align:center">样本</th>
<th style="text-align:center">特征1</th>
<th style="text-align:center">特征2</th>
<th style="text-align:center">特征3</th>
<th style="text-align:center">特征4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>x₁</strong></td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">4.0</td>
</tr>
<tr>
<td style="text-align:center"><strong>x₂</strong></td>
<td style="text-align:center">5.0</td>
<td style="text-align:center">6.0</td>
<td style="text-align:center">7.0</td>
<td style="text-align:center">8.0</td>
</tr>
<tr>
<td style="text-align:center"><strong>x₃</strong></td>
<td style="text-align:center">9.0</td>
<td style="text-align:center">10.0</td>
<td style="text-align:center">11.0</td>
<td style="text-align:center">12.0</td>
</tr>
</tbody>
</table>
<br>
<h3 id="MLP-FFN">MLP&amp;FFN</h3>
<p>在Transformer的每个编码器和解码器层中，MLP（也称为<strong>Feed Forward Network, FFN</strong>）用于对自注意力层的输出进行非线性变换和特征映射。</p>
<p>MLP (Multi-Layer Perceptron) 是 Transformer 编码器中每个自注意力层之后的一个前馈网络模块。这个 MLP 通常包含两个线性层，中间有一个非线性激活函数（如 GELU）</p>
<br>
$$
FFN(x)=Linear_{2}(Activation(Linear_{1}(x)))
$$
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForwardNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, d_ff: <span class="built_in">int</span>, activation=<span class="string">&quot;relu&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear1 = nn.Linear(d_model, d_ff)  <span class="comment"># 扩展层</span></span><br><span class="line">        self.linear2 = nn.Linear(d_ff, d_model)  <span class="comment"># 收缩层</span></span><br><span class="line">        self.activation = nn.ReLU() <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span> <span class="keyword">else</span> nn.GELU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># x shape: [batch_size, seq_len, d_model]</span></span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> x  <span class="comment"># 输出维度保持 [batch_size, seq_len, d_model]</span></span><br></pre></td></tr></table></figure>
<br>
<p><strong>关于 MLP size</strong></p>
<p>“MLP size” 指的是<strong>这两个线性层中间的隐藏维度</strong>。</p>
<ul>
<li><strong>在 ViT 中的体现:</strong>
<ul>
<li>结构: <code>Linear(D -&gt; MLP_Size) -&gt; GELU -&gt; Linear(MLP_Size -&gt; D)</code></li>
<li>它是一个“瓶颈”结构，将 $D$ 维的输入先扩展到一个更大的维度 <code>MLP_Size</code>，再压缩回 $D$ 维。</li>
</ul>
</li>
<li><strong>作用:</strong>
<ul>
<li><strong>增加非线性表达能力:</strong> MLP 层是 Transformer 编码器中引入非线性的主要方式，使得模型能够学习更复杂的函数关系。</li>
<li><strong>提供“思考空间”:</strong> 扩展到更大的维度（MLP Size）可以被认为是给模型更多的“思考空间”来处理特征。</li>
<li><strong>影响计算量和参数量:</strong> MLP size 越大，这部分的计算量和参数量也会越大。</li>
</ul>
</li>
<li><strong>与 Hidden Size 的关系:</strong> MLP size 通常是 Hidden size $D$ 的一个倍数，例如 <strong>4 倍</strong>。</li>
<li><strong>示例:</strong> 在 ViT-Base 模型中，如果 $D = 768$，那么 MLP size 通常是 $768 \times 4 = 3072$。</li>
</ul>
<br>
<h3 id="相关问题">相关问题</h3>
<ol>
<li>
<p>为什么Attenion公式中要除以 $\sqrt d$（d为Q, K矩阵的输出维度）？</p>
<ul>
<li>当向量维度变大的时候，d变大， q 和 k 的点积的方差变大</li>
<li>由于要对 q 和 k 的点积的每一行进行softmax，过大的方差将导致softmax极端化，得到类似于 $[1,0,0,…]$ 的one-hot分布</li>
<li>当输出接近one-hot时，非最大值的梯度趋近于0，反向传播时，这些位置的参数无法得到更新</li>
<li>因此，设置 softmax 的 temperature 来缓解这个问题，这里 temperature 被设置为了 $\sqrt d$ .</li>
</ul>
<p>如下图所示，假设随机向量 $X$ 满足均值为 0，协方差矩阵为单位矩阵（即各变量独立且方差为 1）的<strong>多元标准正态分布</strong>，可计算得到 $XY^T$ 满足均值为 0，协方差矩阵为 $D_{out} I$ 的<strong>多元正态分布</strong>，通过除以 $\sqrt d$ 将 $XY^T$ 的方差缩放为1</p>
</li>
</ol>
<img src="/2025/03/19/LLM-Rela/image-20250404100511037.png" class="" title="image-20250404100511037">
<ol start="2">
<li>
<p>为什么选择多头注意力？</p>
<p>有说法认为，克服**「模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置」<strong>，或者</strong>表达能力提升**：多个低秩注意力头（$d_h &lt; d$）的集成，能捕捉更复杂的交互模式；</p>
<p>但是仍有相悖观点认为并非如此</p>
<p>ref: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.10650">https://arxiv.org/pdf/1905.10650</a></p>
<img src="/2025/03/19/LLM-Rela/image-20250405101512560.png" class="" title="image-20250405101512560">
</li>
<li>
<p>Q，K，V为什么名为Query，Key，Value？注意力机制的注意力体现在哪里？</p>
<p>在交叉注意力场景中，</p>
<ol>
<li>
<p><strong>解码器的任务：</strong> 解码器的目标是根据编码器输出的源语句信息，并结合已经生成的词，来预测下一个词。</p>
</li>
<li>
<p><strong>编码器的输出：</strong> 编码器处理完源语句后，会输出一系列的向量表示。这些向量捕获了源语句中每个词的上下文信息。在 Transformer 的原始设计中，这些输出向量就是解码器交叉注意力的 K 和 V。</p>
</li>
<li>
<p><strong>Q 来源解码器：</strong> 解码器在生成第 t 个词时，它已经生成了 t-1 个词（或者说，它知道要生成哪个位置的词）。解码器使用它自己当前的输入（通常是前一个生成的词的嵌入，或者一个特殊的表示当前位置的向量）来生成 <strong>查询向量 Q</strong>。这个 Q 代表了“我（解码器）现在想知道什么？我需要哪些信息来生成下一个词？”</p>
</li>
<li>
<p><strong>K，V 来源于编码器：</strong> 编码器处理完整个源语句后，会输出一系列上下文向量。这些向量被用作 <strong>键向量 K</strong> 和 <strong>值向量 V</strong>。</p>
<ul>
<li><strong>K（键）</strong>：代表了源语句中每个词的“身份”或“特征”。当解码器的 Q 去查询时，它会与这些 K 进行匹配，以判断源语句中哪些词与当前的 Q 更相关。</li>
<li><strong>V（值）</strong>：包含了源语句中每个词的实际“内容”或“信息”。一旦 Q 和 K 确定了相关性，V 就会提供这些相关词的实际信息，供解码器使用。</li>
</ul>
</li>
<li>
<p><strong>交叉注意力的过程：</strong></p>
<ul>
<li>解码器的 Q（来自当前生成词的表示）与编码器的 K 矩阵进行点积，并通过 softmax 得到注意力权重。这些权重表明了当前解码器关注点在源语句中各个词上的分布。</li>
<li>将这些权重应用于编码器的 V 矩阵，进行加权求和，得到一个 <strong>上下文向量</strong>。这个上下文向量浓缩了源语句中对当前生成词最重要的信息。</li>
<li>解码器将这个上下文向量与自己的内部状态（例如，通过自注意力获得的已生成词的信息）结合起来，用于预测下一个词。</li>
</ul>
<p>“通过已经生成的词和源语句做自注意力，就是确定源语句中哪些词对接下来的词的生成更有作用”正是 <strong>交叉注意力</strong> 的功能。解码器的 Q（代表当前生成词的意图）去查询编码器的 K/V（源语句信息），找到源语句中最重要的部分。</p>
</li>
</ol>
<p>对比：</p>
<ul>
<li><strong>LSTM Seq2Seq 的问题：</strong>
<ul>
<li><strong>信息瓶颈（Information Bottleneck）</strong>：在传统的 LSTM Encoder-Decoder 架构中，编码器会将整个源序列压缩成一个固定长度的 <strong>上下文向量 C</strong>。无论源序列多长，所有信息都必须挤进这个 C。这导致长序列的信息丢失，尤其在解码器生成后半段序列时，C 中关于源序列前半段的信息可能已经非常稀释。</li>
<li><strong>“每一次生成词，都是通过 C 的全部信息去生成”</strong>：解码器在每一步都依赖于这个固定的 C。这使得模型难以动态地关注源序列中与当前生成词最相关的部分。</li>
<li><strong>“很多信息对于当前生成词而言都是没有意义的”</strong>：没错，对于生成某个特定词，源序列中可能只有一两个词是真正相关的。LSTM 的 C 却包含了所有信息，无法做到“按需提取”。</li>
</ul>
</li>
<li><strong>Transformer 注意力机制的解决方案：</strong>
<ul>
<li><strong>动态聚焦：</strong> 通过交叉注意力，解码器在生成每一个词时，都能动态地计算源序列中不同词的注意力权重。这意味着它能根据当前生成词的需要，<strong>“按需”地从编码器输出中提取最相关的信息</strong>。</li>
<li><strong>避免信息瓶颈：</strong> 编码器不再需要将所有信息压缩成一个单一向量。它输出的是一系列的上下文向量（每个对应源序列中的一个词），解码器可以随时通过注意力机制访问这些向量。</li>
<li><strong>长距离依赖：</strong> 注意力机制可以直接连接源序列中的任意两个词，无论它们相距多远，有助于捕捉长距离依赖关系，这在 LSTM 中很难实现。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>为什么encoder需要多个注意力层？</p>
<p>我们可以把自注意力（Self-Attention）看作是一次信息交互的过程。在一个注意力层中，每个token会“看”到序列中所有其他的token，并根据相关性（Attention Score）从它们那里“借”一些信息来更新自己。</p>
<ul>
<li><strong>一层注意力 = 一次直接交互</strong>：<br>
在句子 “The cat that chased the dog was tired” 中，第一层注意力可以让 “tired” 直接关联到 “cat”。它能建立起直接的语法联系。<br>
但是，“tired” 与 “dog” 的关系是<em>间接</em>的（“tired” -&gt; “cat” -&gt; “chased” -&gt; “dog”）。在一层注意力中，“tired” 看到 “dog” 主要是因为它们同在一个句子里，但它们之间的深层逻辑关系（猫因为追狗而累）还很模糊。</li>
<li><strong>多层注意力 = 多次间接交互</strong>：
<ul>
<li><strong>第一层</strong>：每个词都与所有其他词进行了直接的信息交换。现在，每个词的向量表示（embedding）已经包含了它直接邻居的信息。比如，“cat” 的新向量里包含了 “chased” 的信息。</li>
<li><strong>第二层</strong>：当第二层注意力开始工作时，它的输入是第一层处理过的、已经“混合”了初步上下文的序列。现在，当 “tired” 再次审视 “cat” 时，它看到的 “cat” 已经不是最初的那个了，这个 “cat” 的向量里已经带有了 “chased” 和 “dog” 的“影子”。</li>
<li><strong>以此类推</strong>：每一层都建立在前一层的基础上，信息可以像涟漪一样，通过中间token一跳一跳地传播到更远的地方。经过多层堆叠，一个token的表示就能够聚合到跨越整个序列的、非常复杂的间接依赖关系。</li>
</ul>
</li>
</ul>
</li>
</ol>
<br>
<h2 id="LLM-Train">LLM-Train</h2>
<h3 id="Base-2">Base</h3>
<img src="/2025/03/19/LLM-Rela/image-20250613213945162.png" class="" title="image-20250613213945162">
<h3 id="Parallelism">Parallelism</h3>
<h4 id="DataParallel">DataParallel</h4>
<blockquote>
<p>这不联邦学习吗</p>
</blockquote>
<p>数据并行 DataParallel (DP) - 相同的设置被复制多次，每次都输入一部分数据。处理是并行进行的，所有设置在每个训练步骤结束时同步。</p>
<ol>
<li><strong>模型复制</strong>：将<strong>相同的模型</strong>（包括参数、优化器状态等）复制到多个GPU上。</li>
<li><strong>数据分片</strong>：将训练数据<strong>划分为多个子批次（mini-batch）</strong>，每个GPU处理一个子批次。</li>
<li><strong>并行计算</strong>：所有GPU<strong>并行执行前向传播和反向传播</strong>，计算各自子批次的梯度。</li>
<li><strong>梯度同步</strong>：通过<strong>全局通信</strong>（如AllReduce）收集所有梯度并求平均，更新一次全局模型参数。</li>
</ol>
<img src="/2025/03/19/LLM-Rela/0.png" class="" title="alt text">
<p>图中对比了不同数据并行策略的资源消耗（以N=64个GPU为例）：</p>
<p><strong>Baseline（纯数据并行）</strong></p>
<ul>
<li><strong>内存消耗</strong>：120GB（最高）
<ul>
<li>原因：每个GPU需存储完整的模型参数（蓝色）、梯度（橙色）和优化器状态（绿色），无任何分区优化。</li>
</ul>
</li>
<li><strong>通信量</strong>：1x（基准）
<ul>
<li>需同步所有GPU的梯度（通信量随GPU数量线性增长）。</li>
</ul>
</li>
</ul>
<p><strong>优化策略（$P_{os}、P_{os+g}、P_{os+g+p}$）</strong></p>
<ul>
<li><strong>$P_{os}$</strong>：仅对优化器状态分区
<ul>
<li>内存降至16.6GB（优化器状态分到不同GPU）。</li>
</ul>
</li>
<li><strong>$P_{os+g}$</strong>：优化器状态+梯度分区
<ul>
<li>内存进一步降低（梯度不再全存储）。</li>
</ul>
</li>
<li><strong>$P_{os+g+p}$</strong>：参数、梯度、优化器状态全分区
<ul>
<li>内存最低（1.9GB），但通信量增至1.5x（需额外同步参数）。</li>
</ul>
</li>
</ul>
<br>
<h4 id="TensorParallel">TensorParallel</h4>
<p>每个张量被分成多个块，因此不是将整个张量驻留在单个 gpu 上，而是将张量的每个分片驻留在其指定的 gpu 上。在处理过程中，每个分片在不同的 GPU 上单独并行处理，结果在步骤结束时同步。这就是所谓的水平并行，因为拆分发生在水平层面。</p>
<img src="/2025/03/19/LLM-Rela/4.png" class="" title="alt text">
<br>
<h4 id="PipelineParallel">PipelineParallel</h4>
<p>将模型的不同层分布在不同 GPU 上，每张 GPU 负责模型的一部分，<strong>输入数据按 micro-batch 流水处理</strong>。</p>
<p>假设模型有 8 层：</p>
<ul>
<li>GPU0 负责第 1~4 层</li>
<li>GPU1 负责第 5~8 层</li>
<li>将 batch size 为 64 分为 4 个 micro-batch（每个 16 条数据）</li>
<li>micro-batch1 流经 GPU0，GPU1，接着 micro-batch2 开始处理，实现流水线并行</li>
</ul>
<br>
<h3 id="Deepspeed">Deepspeed</h3>
<br>
<h3 id="FSDP">FSDP</h3>
<br>
<h2 id="LLM-Inference">LLM-Inference</h2>
<h3 id="Parameters">Parameters</h3>
<h4 id="Temperature">Temperature</h4>
<p><strong>温度参数控制输出随机性（多样性）的超参数。</strong></p>
<p>将模型输出的 logits（原始分数）除以温度值，然后再经过 softmax，计算出新的概率分布：<br>
$$<br>
P_i = \frac{e^{\frac{logit_i}{T}}}{\sum_j e^{\frac{logit_j}{T}}}<br>
$$</p>
<ul>
<li><strong>T &lt; 1</strong> → 增强高概率词，削弱低概率词</li>
<li><strong>T &gt; 1</strong> → 扁平化分布，低概率词获得更多机会</li>
<li><strong>T = 1</strong> → 原始 softmax 分布</li>
</ul>
<blockquote>
<p>温度对模型输出的影响相当于改进版的softmax层</p>
</blockquote>
<br>
<h4 id="Sampling">Sampling</h4>
<p>Top-K</p>
<p>Top-K控制的是“只在前K个最有可能的词中采样”。</p>
<ul>
<li><strong>K=1</strong> → 只选概率最大的词（等同于贪婪搜索）</li>
<li><strong>K=10</strong> → 从概率前10的词中进行随机选择</li>
<li><strong>K=100+</strong> → 越大，越接近全概率分布，输出更有创造性</li>
</ul>
<img src="/2025/03/19/LLM-Rela/6.png" class="" title="alt text">
<br>
<p>Top-p</p>
<p>使用随机策略选择一个输出，候选集为按概率排名靠前的连续结果，且累积概率&lt;=p</p>
<img src="/2025/03/19/LLM-Rela/6.png" class="" title="alt text">
<br>
<h2 id="Fine-Tuning">Fine-Tuning</h2>
<p>高效微调技术分类：</p>
<ul>
<li>增加额外参数（A）
<ul>
<li>类适配器（Adapter-like）方法</li>
<li>软提示（Soft prompts）</li>
</ul>
</li>
<li>选取一部分参数更新（S）</li>
<li>引入重参数化（R）</li>
</ul>
<img src="/2025/03/19/LLM-Rela/v2-eaaf1c00d0c4ea350cd3a79b47de26d3_1440w.jpg" class="" title="img">
<br>
<h3 id="BitFit-Prefix-Tuning-Prompt-Tuning">BitFit, Prefix Tuning &amp; Prompt Tuning</h3>
<p>BitFit（论文：<strong>BitFit: Simple Parameter-efficient Fine-tuning or Transformer-based Masked Language-models</strong>）是一种稀疏的微调方法，它训练时只更新bias的参数或者部分bias参数。</p>
<p>涉及到的bias参数有attention模块中计算query,key,value跟合并多个attention结果时涉及到的bias，MLP层中的bias，Layernormalization层的bias参数。</p>
<br>
<p>Prefix Tuning（论文：<strong>Prefix-Tuning: Optimizing Continuous Prompts for Generation</strong>），在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而PLM(Pretrain LM)中的其他部分参数固定。</p>
<br>
<p>Prompt Tuning（论文：<strong>The Power of Scale for Parameter-Efficient Prompt Tuning</strong>），该方法可以看作是Prefix Tuning的简化版本，它给每个任务定义了自己的Prompt，然后拼接到数据上作为输入，但<strong>只在输入层加入prompt tokens</strong>，并且不需要加入 MLP 进行调整来解决难训练的问题。</p>
<br>
<h3 id="P-Tuning">P-Tuning</h3>
<p>P-Tuning（论文：<strong>GPT Understands, Too</strong>），该方法将Prompt转换为可以学习的Embedding层，并用MLP+LSTM的方式来对Prompt Embedding进行一层处理。</p>
<p>相比Prefix Tuning，P-Tuning加入的可微的virtual token，但仅限于输入层，没有在每一层都加；另外，virtual token的位置也不一定是前缀，插入的位置是可选的。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual token。</p>
<p>P-Tuning v2（论文： <strong>P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</strong>），该方法在每一层都加入了Prompts tokens作为输入，而不是仅仅加在输入层</p>
<br>
<h3 id="Adapter-Tuning">Adapter Tuning</h3>
<p>Adapter Tuning（论文：<strong>Parameter-Efficient Transfer Learning for NLP</strong>），该方法设计了Adapter结构，并将其嵌入Transformer的结构里面，针对每一个Transformer层，增加了两个Adapter结构(分别是多头注意力的投影之后和第二个feed-forward层之后)，在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构和 Layer Norm 层进行微调，从而保证了训练的高效性。</p>
<p>Adapter Fusion（论文：<strong>AdapterFusion:Non-Destructive Task Composition for Transfer Learning</strong>），一种融合多任务信息的Adapter的变体，在 Adapter 的基础上进行优化，通过将学习过程分为两阶段来提升下游任务表现。</p>
<p>AdapterDrop（论文：AdapterDrop: On the Efficiency of Adapters in Transformers），在不影响任务性能的情况下，对Adapter动态高效的移除，尽可能的减少模型的参数量，提高模型在反向传播（训练）和正向传播（推理）时的效率。</p>
<br>
<h3 id="LoRA">LoRA</h3>
<h4 id="奇异值分解与低秩分解">奇异值分解与低秩分解</h4>
<p><strong>SVD</strong></p>
<p>对于任意一个 $m \times n$ 的实矩阵 $A$，可以分解成三个矩阵的乘积：<br>
$$<br>
A = U \Sigma V^T<br>
$$</p>
<ul>
<li>$U$：$m \times m$ 的正交矩阵（左奇异向量）</li>
<li>$\Sigma$：$m \times n$ 的对角矩阵，对角线上的值是奇异值（非负，按大小排列）</li>
<li>$V^T$：$n \times n$ 的正交矩阵（右奇异向量的转置）</li>
</ul>
<br>
<p><strong>低秩分解</strong></p>
<p>r是矩阵的秩，决定了分解后保留的信息量。如果只保留最大的几个奇异值（低秩近似），就能用更少的参数近似原矩阵</p>
<p>例如，存在矩阵<br>
$$<br>
S = \begin{bmatrix}<br>
1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \<br>
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br>
0 &amp; 3 &amp; 0 &amp; 0 &amp; 0 \<br>
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \<br>
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br>
\end{bmatrix}<br>
$$<br>
分解后的三个矩阵：<br>
$$<br>
U \approx \begin{bmatrix}<br>
0.3 &amp; 0 &amp; 0.34 &amp; -0.68 &amp; -0.58 \<br>
-0.22 &amp; 0 &amp; -0.76 &amp; 0.2 &amp; -0.58 \<br>
0 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \<br>
-0.77 &amp; 0 &amp; 0.36 &amp; 0.52 &amp; 0 \<br>
-0.52 &amp; 0 &amp; -0.42 &amp; -0.48 &amp; -0.58 \<br>
\end{bmatrix}, \quad<br>
\Sigma = \begin{bmatrix}<br>
7.03 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br>
0 &amp; 3 &amp; 0 &amp; 0 &amp; 0 \<br>
0 &amp; 0 &amp; 2.15 &amp; 0 &amp; 0 \<br>
0 &amp; 0 &amp; 0 &amp; 0.11 &amp; 0 \<br>
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br>
\end{bmatrix}, \quad<br>
V \approx \begin{bmatrix}<br>
0.34 &amp; -0.32 &amp; 0 &amp; -0.89 &amp; 0 \<br>
0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 \<br>
0.3 &amp; -0.93 &amp; 0 &amp; 0.22 &amp; 0 \<br>
-0.89 &amp; -0.19 &amp; 0 &amp; 0.41 &amp; 0 \<br>
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \<br>
\end{bmatrix}<br>
$$<br>
选择最大的三个奇异值重构，保留 $\sigma_{1} \approx 7.03$, $\sigma_{2} = 3$, $\sigma_{3}=2.15$，重构矩阵如下：</p>
<p>保留前三列：</p>
<p>$$<br>
U_{\text{trunc}} \approx<br>
\begin{bmatrix}<br>
0.3 &amp; 0 &amp; 0.34 \<br>
-0.22 &amp; 0 &amp; -0.76 \<br>
0 &amp; -1 &amp; 0 \<br>
-0.77 &amp; 0 &amp; 0.36 \<br>
-0.52 &amp; 0 &amp; -0.428<br>
\end{bmatrix}<br>
$$<br>
保留前三行和前三列：</p>
<p>$$<br>
\Sigma_{\text{trunc}} =<br>
\begin{bmatrix}<br>
7.03 &amp; 0 &amp; 0 \<br>
0 &amp; 3 &amp; 0 \<br>
0 &amp; 0 &amp; 2.15<br>
\end{bmatrix}<br>
$$<br>
保留前三行：</p>
<p>$$<br>
V_{\text{trunc}} \approx<br>
\begin{bmatrix}<br>
0.34 &amp; -0.32 &amp; 0 &amp; -0.89 &amp; 0 \<br>
0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 \<br>
0.3 &amp; -0.93 &amp; 0 &amp; 0.22 &amp; 0<br>
\end{bmatrix}<br>
$$<br>
根据，$S’ = U_{\text{trunc}} \times \Sigma_{\text{trunc}} \times V_{\text{trunc}}^T$，计算得到重构后：</p>
<p>$$<br>
S’ =<br>
\begin{bmatrix}<br>
0.93 &amp; -0.01 &amp; 0 &amp; 2.03 &amp; 0 \<br>
0.02 &amp; 2 &amp; 0 &amp; 0.99 &amp; 0 \<br>
0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 \<br>
2.05 &amp; 1.01 &amp; 0 &amp; 4.98 &amp; 0 \<br>
0.95 &amp; 1.99 &amp; 0 &amp; 3.02 &amp; 0<br>
\end{bmatrix}<br>
$$<br>
结果对比原始矩阵和重构矩阵，直观地看，基本保持一致。</p>
<p>事实上上面的结论：如果只保留最大的几个奇异值（低秩近似），就能用更少的参数近似 $W$。<br>
$$<br>
S =<br>
\begin{bmatrix}<br>
1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \<br>
0 &amp; 2 &amp; 0 &amp; 1 &amp; 0 \<br>
0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 \<br>
2 &amp; 1 &amp; 0 &amp; 5 &amp; 0 \<br>
1 &amp; 2 &amp; 0 &amp; 3 &amp; 0<br>
\end{bmatrix}<br>
\quad<br>
S’ =<br>
\begin{bmatrix}<br>
0.93 &amp; -0.01 &amp; 0 &amp; 2.03 &amp; 0 \<br>
0.02 &amp; 2 &amp; 0 &amp; 0.99 &amp; 0 \<br>
0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 \<br>
2.05 &amp; 1.01 &amp; 0 &amp; 4.98 &amp; 0 \<br>
0.95 &amp; 1.99 &amp; 0 &amp; 3.02 &amp; 0<br>
\end{bmatrix}<br>
$$<br>
实际上，可以通过保留的奇异值，计算重构后的矩阵，保留了多少信息，如下：</p>
<p>$$<br>
|A|_F^2 = 7.03^2 + 3^2 + 2.15^2 + 0.11^2 + 0^2 \approx 63.06<br>
\quad<br>
|A’|_F^2 = 7.03^2 + 3^2 + 2.15^2 \approx 63.04<br>
$$</p>
<p>$$<br>
\text{信息保留比例} = \frac{63.04}{63.06} \approx 99.97%<br>
$$</p>
<blockquote>
<p>为什么LoRA可以进行低秩分解？</p>
<p>通过对微调后的权重变化 $\Delta W$ 的奇异值分解发现，大部分信息集中在少数几个奇异值上；在GPT-3上测试时发现，$\Delta W$ 的前 10 - 20 个奇异值占据了 90% 的信息</p>
<p>可以对原始权重 $W$ 进行分解吗？</p>
<p>不可以，$W$ 接近满秩</p>
</blockquote>
<p>假设对一个 $512 \times 512$ 的权重矩阵 $W$ 进行微调</p>
<ul>
<li>全微调：可能需要调整 262144 个参数</li>
<li>LoRA：假设 r = 8，只需要调整 $A(512 \times 8)$ 和 $B(8 \times 512)$，共 8192 个参数</li>
</ul>
<br>
<h4 id="LoRA应用位置">LoRA应用位置</h4>
<p><strong>注意力层</strong></p>
<p>多应用与 $W_q$ 和 $W_v$ 上</p>
<p><strong>FFN层</strong></p>
<p>$W_1$ (升维)和 $W_2$ (降维)</p>
<br>
<h4 id="LoRA改进">LoRA改进</h4>
<h5 id="LoRA-2">LoRA+</h5>
<p><strong>核心思想</strong>：对低秩矩阵 $A$ 和 $B$ 设置不同的学习率，以增强训练动态性。</p>
<p>在标准 LoRA 中，权重更新为：</p>
<p>$$<br>
\Delta W = A B, \quad A \in \mathbb{R}^{d \times r}, ; B \in \mathbb{R}^{r \times d}<br>
$$</p>
<p>LoRA+ 设置独立的学习率：</p>
<p>$$<br>
A \leftarrow A - \eta_A \cdot \nabla_A \mathcal{L}, \quad B \leftarrow B - \eta_B \cdot \nabla_B \mathcal{L}<br>
$$</p>
<p>其中：</p>
<ul>
<li>$\eta_A$：A 的学习率</li>
<li>$\eta_B$：B 的学习率</li>
<li>通常设置 $\eta_B = \lambda \cdot \eta_A$，$\lambda \in [4, 16]$</li>
</ul>
<br>
<h5 id="DoRA">DoRA</h5>
<p><strong>核心思想</strong>：引入对残差结构的重构机制，更有效地利用参数空间。</p>
<p>标准 LoRA 更新为：<br>
$$<br>
W = W_0 + \Delta W = W_0 + A B<br>
$$<br>
而 DoRA 将残差部分进一步分解为：</p>
<p>$$<br>
W = U \cdot S \cdot V^T<br>
$$</p>
<p>其中：</p>
<ul>
<li>为低秩基</li>
<li>为可学习的对角矩阵或全连接矩阵（增强表达能力）</li>
</ul>
<p>若引入正则项，则完整目标函数为：<br>
$$<br>
\mathcal{L}<em>{\text{total}} = \mathcal{L}</em>{\text{task}} + \lambda |S|_F^2<br>
$$</p>
<br>
<h5 id="rsLoRA">rsLoRA</h5>
<p>rsLoRA 针对不同层设置不同的秩（rank），以便更灵活地分配参数量。<br>
<strong>核心思想</strong>：为每一层设置不同的秩 $r^{(l)}$，提高参数使用效率。</p>
<p>对于第 $l$ 层，有：<br>
$$<br>
\Delta W^{(l)} = A^{(l)} B^{(l)}, \quad A^{(l)} \in \mathbb{R}^{d \times r^{(l)}}, ; B^{(l)} \in \mathbb{R}^{r^{(l)} \times d}<br>
$$<br>
训练过程中可以手动设定 rank 或使用启发式函数自动选择：<br>
$$<br>
r^{(l)} = f\left( |W^{(l)}|, \sigma^{(l)} \right)<br>
$$<br>
其中 $\sigma^{(l)}$ 可为特征谱或梯度范数。</p>
<p><strong>优势</strong>：</p>
<ul>
<li>提高计算效率；</li>
<li>保持性能的同时减少冗余参数。</li>
</ul>
<br>
<h5 id="PiSSA">PiSSA</h5>
<p><strong>核心思想</strong>：用奇异值分解（SVD）初始化 $A$ 和 $B$，更好地保持原始权重结构。</p>
<p>将原始矩阵 $W$ 分解为：<br>
$$<br>
W \approx U_r \Sigma_r V_r^T<br>
$$<br>
其中：</p>
<ul>
<li>$U_r \in \mathbb{R}^{d \times r}, ; \Sigma_r \in \mathbb{R}^{r \times r}, ; V_r \in \mathbb{R}^{d \times r}$</li>
<li>保留前 $r$ 个奇异值（截断 SVD）</li>
</ul>
<p>初始化为：<br>
$$<br>
A = U_r \cdot \sqrt{\Sigma_r}, \quad B = \sqrt{\Sigma_r} \cdot V_r^T<br>
$$<br>
因此：<br>
$$<br>
\Delta W = A B = U_r \Sigma_r V_r^T \approx W_r<br>
$$<br>
<strong>优势</strong>：</p>
<ul>
<li>更接近原始参数空间；</li>
<li>避免随机初始化带来的不稳定性；</li>
<li>提升初期训练收敛速度。</li>
</ul>
<h4 id="LoRA相关论文">LoRA相关论文</h4>
<p>LoRA（论文：<strong>LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</strong>），该方法的核心思想就是通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。</p>
<br>
<p>AdaLoRA（论文：<strong>ADAPTIVE BUDGET ALLOCATION FOR PARAMETEREFFICIENT FINE-TUNING</strong>），是对LoRA的一种改进，它根据重要性评分动态分配参数预算给权重矩阵。</p>
<br>
<p>QLoRA（论文： <strong>QLORA: Efficient Finetuning of Quantized LLMs</strong>），使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。QLORA 有一种低精度存储数据类型（4 bit），还有一种计算数据类型（BFloat16）。实际上，这意味着无论何时使用 QLoRA 权重张量，我们都会将张量反量化为 BFloat16，然后执行 16 位矩阵乘法。QLoRA提出了两种技术实现高保真 4 bit微调——4 bit NormalFloat(NF4) 量化和双量化。此外，还引入了分页优化器，以防止梯度检查点期间的内存峰值，从而导致内存不足的错误，这些错误在过去使得大型模型难以在单台机器上进行微调。</p>
<br>
<h4 id="调参技巧">调参技巧</h4>
<ul>
<li>从低秩开始：对于绝大多数任务，可以从 $r=8$ 和 $r = 16$ 开始调整，评估性能后再决定是否需要更高的秩</li>
<li>数据集大小与秩的关系：小数据集（&lt;5k 样本）用低秩（$r=8$）；大数据集（&gt;50k样本）可以尝试更大秩（r=32+）</li>
<li>复杂任务策略：对于复杂推理任务，可以结合使用：（1）增大r到32或64；（2）启用rsLoRA；（3）添加更多目标层；</li>
</ul>
<br>
<h3 id="MAM-Adapter-UniPELT">MAM Adapter &amp; UniPELT</h3>
<p>MAM Adapter（论文：TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING），一个在Adapter、Prefix Tuning和LoRA之间建立联系的统一方法。</p>
<br>
<p>UniPELT（论文： UNIPELT: A Unified Framework for Parameter-Efficient Language Model Tuning）是 LoRA、Prefix Tuning和Adapter的门控组合。</p>
<br>
<h2 id="Reinforce-Learning-on-LLM">Reinforce Learning on LLM</h2>
<h3 id="LLM-DPO">LLM-DPO</h3>
<p>Direct Preference Optimization</p>
<p>DPO的核心思想是：<strong>跳过显式奖励建模和复杂的强化学习步骤，直接通过一个简单的分类损失函数来优化语言模型，使其符合人类偏好。</strong></p>
<p>它将“让模型生成高奖励的回答”这个目标，等价地转换为了“直接增大模型对‘更优回答’的生成概率，同时减小对‘较差回答’的生成概率”。</p>
<p>整个训练流程比PPO更简洁，通常只有两步：</p>
<ol>
<li><strong>监督式微调（SFT）：</strong> 与PPO的第一步完全相同。训练一个基础模型来理解指令和生成基本回答。</li>
<li><strong>直接偏好优化（DPO）：</strong> 这一步直接取代了PPO流程中的“奖励模型训练”和“PPO强化学习”两个阶段。</li>
</ol>
<hr>
<h4 id="流程"><strong>流程</strong></h4>
<p>在DPO阶段，我们不再需要Critic模型，也不需要在线采样生成数据。我们使用的是一个<strong>静态的偏好数据集</strong>。</p>
<h5 id="第一阶段：定义参与者（两大模型）"><strong>第一阶段：定义参与者（两大模型）</strong></h5>
<p>在DPO训练开始前，我们只需要两个模型：</p>
<ol>
<li>
<p><strong>策略模型（Policy Model, <code>π_θ</code>）：</strong></p>
<ul>
<li><strong>角色:</strong> 主角，即我们正在微调的语言模型。</li>
<li><strong>来源:</strong> 经过第一步SFT训练后的模型副本。</li>
<li><strong>任务:</strong> 在DPO训练中，它的参数<code>θ</code>会被更新。</li>
</ul>
</li>
<li>
<p><strong>参考模型（Reference Model, <code>π_ref</code>）：</strong></p>
<ul>
<li><strong>角色:</strong> “锚点”或“基准”。</li>
<li><strong>来源:</strong> 同样是第一步SFT模型的<strong>一个固定、不更新的副本</strong>。</li>
<li><strong>任务:</strong> 提供一个基准概率。它的作用与PPO中的参考模型完全相同：防止策略模型<code>π_θ</code>为了迎合偏好数据而偏离其原始的语言能力太远，这是一种隐式的KL散度约束。</li>
</ul>
</li>
</ol>
<h5 id="第二阶段：准备数据集"><strong>第二阶段：准备数据集</strong></h5>
<p>DPO使用的数据集格式非常关键。它不是单个的“好回答”，而是一个偏好对的集合。每一条数据包含：</p>
<ul>
<li><strong>提示（Prompt, <code>x</code>）</strong></li>
<li><strong>更优的回答（Chosen Response, <code>y_w</code>）</strong></li>
<li><strong>较差的回答（Rejected Response, <code>y_l</code>）</strong></li>
</ul>
<p>这个数据集 <code>D = &#123; (x, y_w, y_l) &#125;</code> 通常就是用来训练PPO流程中奖励模型的那个数据集。</p>
<h5 id="第三阶段：DPO训练循环"><strong>第三阶段：DPO训练循环</strong></h5>
<p>DPO的训练过程更像一个标准的监督学习循环，而不是RL的“生成-评估-更新”循环。</p>
<p>对于从偏好数据集中采样的每一个<code>(x, y_w, y_l)</code>三元组：</p>
<ol>
<li>
<p><strong>计算策略模型概率：</strong></p>
<ul>
<li>将 <code>(x, y_w)</code> 输入到<strong>策略模型 <code>π_θ</code></strong> 中，计算模型生成 <code>y_w</code> 的总对数概率：<code>log π_θ(y_w | x)</code>。</li>
<li>将 <code>(x, y_l)</code> 输入到<strong>策略模型 <code>π_θ</code></strong> 中，计算模型生成 <code>y_l</code> 的总对数概率：<code>log π_θ(y_l | x)</code>。</li>
<li><em>（这是通过对回答中的每个token的条件概率取对数再求和得到的）</em></li>
</ul>
</li>
<li>
<p><strong>计算参考模型概率：</strong></p>
<ul>
<li>将 <code>(x, y_w)</code> 输入到<strong>固定的参考模型 <code>π_ref</code></strong> 中，计算其生成 <code>y_w</code> 的总对数概率：<code>log π_ref(y_w | x)</code>。</li>
<li>将 <code>(x, y_l)</code> 输入到<strong>固定的参考模型 <code>π_ref</code></strong> 中，计算其生成 <code>y_l</code> 的总对数概率：<code>log π_ref(y_l | x)</code>。</li>
</ul>
</li>
<li>
<p><strong>计算隐式奖励的差异：</strong></p>
<ul>
<li>DPO理论证明，模型的对数概率与参考模型的对数概率之差，正比于一个隐式的奖励函数。</li>
<li>计算<code>y_w</code>的隐式奖励（或称为“偏好得分”）:<br>
<code>r_θ(x, y_w) = β * (log π_θ(y_w | x) - log π_ref(y_w | x))</code></li>
<li>计算<code>y_l</code>的隐式奖励:<br>
<code>r_θ(x, y_l) = β * (log π_θ(y_l | x) - log π_ref(y_l | x))</code></li>
<li><code>β</code> 是一个超参数，通常设为0.1到0.5之间，它控制着模型对参考模型的偏离程度。</li>
</ul>
</li>
<li>
<p><strong>计算DPO损失函数：</strong></p>
<ul>
<li>DPO的目标是让 <code>y_w</code> 的奖励远高于 <code>y_l</code> 的奖励。它使用一个类似于<strong>二元分类的逻辑损失（Logistic Loss）</strong> 来实现这个目标。</li>
<li><code>Loss_DPO = -log(σ(r_θ(x, y_w) - r_θ(x, y_l)))</code></li>
<li>其中 <code>σ</code> 是 Sigmoid 函数。</li>
<li><strong>直观理解：</strong> 这个损失函数的目标是最大化 <code>r_θ(x, y_w)</code> 和 <code>r_θ(x, y_l)</code> 之间的差值。当策略模型赋予<code>y_w</code>的相对概率（相对于参考模型）远高于<code>y_l</code>时，损失就会变小。</li>
</ul>
</li>
<li>
<p><strong>反向传播与优化：</strong></p>
<ul>
<li>计算 <code>Loss_DPO</code> 相对于<strong>策略模型 <code>π_θ</code></strong> 参数的梯度。</li>
<li>使用AdamW等优化器更新<strong>策略模型 <code>π_θ</code></strong> 的参数。</li>
<li><strong>注意：参考模型 <code>π_ref</code> 的参数始终不更新。</strong></li>
</ul>
</li>
</ol>
<p>通过在整个偏好数据集上重复这个过程，策略模型 <code>π_θ</code> 会被直接优化，使其倾向于生成更符合人类偏好的回答。</p>
<br>
<h3 id="LLM-PPO">LLM-PPO</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a></p>
<img src="/2025/03/19/LLM-Rela/image-20250325131654259.png" class="" title="image-20250325131654259">
<br>
<h4 id="流程-2"><strong>流程</strong></h4>
<p>我们可以将整个流程更清晰地划分为**“角色定义”<strong>、</strong>“数据生成（Rollout）”<strong>和</strong>“模型学习（Learning）”**三个阶段。</p>
<h5 id="第一阶段：定义参与者（四大模型）"><strong>第一阶段：定义参与者（四大模型）</strong></h5>
<p>在PPO训练循环开始前，我们有四个关键的模型，其中三个是神经网络：</p>
<ol>
<li>
<p><strong>Actor (策略模型 / Policy):</strong></p>
<ul>
<li><strong>角色:</strong> 主角，即我们正在微调的语言模型。</li>
<li><strong>来源:</strong> 经过第一步SFT（监督式微调）训练后的模型副本。</li>
<li><strong>任务:</strong> 根据当前状态 <code>s_t</code>（已生成的文本），生成下一个token <code>a_t</code>。</li>
</ul>
</li>
<li>
<p><strong>Reference Model (参考模型):</strong></p>
<ul>
<li><strong>角色:</strong> “锚点”或“约束器”。</li>
<li><strong>来源:</strong> 同样是第一步SFT模型的<strong>一个固定、不更新的副本</strong>。</li>
<li><strong>任务:</strong> 提供一个基准的概率分布 <code>π_ref(a|s_t)</code>，用于计算KL散度，防止Actor为了追求奖励而“走火入魔”，生成不自然或语法混乱的文本。</li>
</ul>
</li>
<li>
<p><strong>Critic (价值模型 / Value Network):</strong></p>
<ul>
<li><strong>角色:</strong> “评估员”或“裁判”。</li>
<li><strong>来源:</strong> 通常用RM的头部或SFT模型的头部初始化，然后与Actor一起在PPO阶段训练。</li>
<li><strong>任务:</strong> 评估当前状态 <code>s_t</code> 的<strong>潜在价值</strong> <code>V(s_t)</code>，即从这个状态开始，预期未来能获得多少总奖励。它的存在是为了减少策略梯度更新的方差（通过计算优势函数）。</li>
</ul>
</li>
<li>
<p><strong>Reward Model (奖励模型 / RM):</strong></p>
<ul>
<li><strong>角色:</strong> “最终裁判”，定义了优化的最终目标。</li>
<li><strong>来源:</strong> 第二步训练好的人类偏好模型，在PPO阶段<strong>保持固定，不更新</strong>。</li>
<li><strong>任务:</strong> 对一个<strong>完整的</strong>生成序列（prompt + response）给出一个标量分数 <code>R_final</code>，代表人类对这个回答的偏好程度。</li>
</ul>
</li>
</ol>
<hr>
<h5 id="第二阶段：数据生成（Rollout-Phase）"><strong>第二阶段：数据生成（Rollout Phase）</strong></h5>
<p>对于一个从数据集中采样的 <code>prompt</code>，我们执行一次完整的序列生成来收集训练数据。</p>
<ol>
<li>
<p><strong>初始化:</strong> 从一个 <code>prompt</code> 开始，得到初始状态 <code>s_0</code>。</p>
</li>
<li>
<p><strong>逐Token生成循环 (For t = 0, 1, …, T-1):</strong></p>
<ul>
<li><strong>动作 (Action):</strong> Actor模型接收当前状态 <code>s_t</code>，输出概率分布 <code>π_actor(·|s_t)</code>，并从中<strong>采样</strong>一个token <code>a_t</code>。</li>
<li><strong>记录 (Log):</strong> 记录Actor输出该token的对数概率 <code>log π_actor(a_t|s_t)</code>。</li>
<li><strong>价值评估 (Value Estimation):</strong> Critic模型评估当前状态的价值 <code>V(s_t)</code>。</li>
<li><strong>计算即时奖励 (Immediate Reward):</strong> 这里的奖励<strong>主要不是来自RM</strong>。
<ul>
<li><strong>KL惩罚:</strong> 计算Actor策略与Reference策略在当前步的KL散度，作为惩罚项。<br>
<code>r_t = -β * log(π_actor(a_t|s_t) / π_ref(a_t|s_t))</code></li>
<li><code>β</code> 是一个控制KL惩罚力度的超参数。这个 <code>r_t</code> 是每一步都会计算的。</li>
</ul>
</li>
<li><strong>状态转移:</strong> 将新生成的token <code>a_t</code> 添加到序列中，得到新状态 <code>s_&#123;t+1&#125;</code>。</li>
<li><strong>存储经验:</strong> 将元组 <code>(s_t, a_t, r_t, V(s_t), log π_actor(a_t|s_t))</code> 存储起来。</li>
</ul>
</li>
<li>
<p><strong>最终奖励计算 (Final Reward):</strong></p>
<ul>
<li>当生成结束（例如遇到EOS token或达到最大长度）时，我们得到了一个完整的回答。</li>
<li>将完整的（<code>prompt</code>, <code>response</code>）输入到<strong>Reward Model (RM)</strong> 中，获得最终的标量奖励 <code>R_final</code>。</li>
<li>将这个最终奖励 <strong>加到最后一步的即时奖励 <code>r_T</code> 上</strong>。所以，<code>r_T</code> 变为 <code>r_T (KL惩罚) + R_final</code>。</li>
<li><strong>关键点:</strong> 只有在序列结束时，RM才提供一次性的、整体的奖励信号。而KL惩罚是贯穿于每一步的。</li>
</ul>
</li>
</ol>
<p>至此，我们收集到了一条完整的轨迹（trajectory）。</p>
<hr>
<h5 id="第三阶段：模型学习（Learning-Phase）"><strong>第三阶段：模型学习（Learning Phase）</strong></h5>
<p>利用收集到的轨迹数据，我们来更新Actor和Critic模型。</p>
<ol>
<li>
<p><strong>计算优势函数 (Advantage Estimation):</strong></p>
<ul>
<li>使用收集到的奖励 <code>r_t</code> 和价值估计 <code>V(s_t)</code>，通过**广义优势估计（GAE）**算法计算每一步的优势 <code>A_t</code>。</li>
<li>GAE能有效平衡偏差和方差，稳定训练过程。<code>A_t</code> 直观地表示了在状态 <code>s_t</code> 选择动作 <code>a_t</code> 相对于平均水平有多好。</li>
</ul>
</li>
<li>
<p><strong>更新Actor (策略模型):</strong></p>
<ul>
<li>使用计算出的优势 <code>A_t</code> 和之前记录的 <code>log π_actor(a_t|s_t)</code>，构造PPO的<strong>裁剪代理目标函数（Clipped Surrogate Objective）</strong>。</li>
<li>通过梯度上升（最大化目标函数）来更新Actor模型的参数。PPO的裁剪机制确保了每次更新的步子不会太大，从而保证了训练的稳定性。</li>
</ul>
</li>
<li>
<p><strong>更新Critic (价值模型):</strong></p>
<ul>
<li>Critic的目标是更准确地预测未来的回报。</li>
<li>构造一个损失函数，通常是<strong>均方误差（MSE）</strong>，使其预测的价值 <code>V(s_t)</code> 尽可能接近于在该步之后实际观察到的累积回报（也称为Returns）。</li>
<li>通过梯度下降来更新Critic模型的参数。</li>
</ul>
</li>
</ol>
<br>
<h4 id="相关内容">相关内容</h4>
<p><strong>SFT LLM</strong></p>
<p>train Reward Model</p>
<img src="/2025/03/19/LLM-Rela/image-20250325151928459.png" class="" title="image-20250325151928459">
<p>use LLM_sft to be the Actor(reference model)</p>
<img src="/2025/03/19/LLM-Rela/image-20250325153214542.png" class="" title="image-20250325153214542">
<img src="/2025/03/19/LLM-Rela/image-20250325153304648.png" class="" title="image-20250325153304648">
<br>
<p><strong>PPO架构</strong></p>
<img src="/2025/03/19/LLM-Rela/image-20250325153337812.png" class="" title="image-20250325153337812">
<blockquote>
<p>广义优势GAE为多步时序差分的指数加权平均，详见reinforce-learning-record，参数含Value值和Reward值</p>
</blockquote>
<p><strong>为什么引入KL散度项？</strong></p>
<p>这个KL项有两个作用。首先，它作为一种熵奖励，鼓励策略进行探索，防止其坍缩到单一模式。其次，它确保策略不会学习生成与奖励模型在训练期间见过的输出差异过大的结果。</p>
<ul>
<li>在 RLHF 中，PPO 策略在优化过程中可能会偏离初始 SFT 模型太远，导致“灾难性遗忘” (catastrophic forgetting)，即模型忘记了 SFT 阶段学到的通用语言能力或指令遵循能力，过度追求奖励信号。PPO 通常会加入一个 KL 散度惩罚项来约束当前策略与 SFT 模型的距离。</li>
</ul>
<br>
<h3 id="LLM-GRPO">LLM-GRPO</h3>
<p>Group Relative Policy Optimization</p>
<ul>
<li>
<p>一个问题，多个回答</p>
</li>
<li>
<p>reward = Model + Rules -&gt; reward (仍然是对每句话给出)</p>
</li>
<li>
<p>advantage = $\frac{r - mean}{std}$</p>
</li>
<li>
<p>优化公式变化</p>
</li>
</ul>
<br>
<h4 id="相关内容-2">相关内容</h4>
<img src="/2025/03/19/LLM-Rela/image-20250325195333997.png" class="" title="image-20250325195333997">
<p>由于PPO算法中使用的价值函数通常是与策略模型规模相当的另一个模型，这会带来巨大的内存和计算负担。此外，在强化学习训练过程中，价值函数被作为计算优势函数（advantage）的基线以实现方差缩减。然而在大型语言模型（LLM）场景中，通常只有最后一个token会被奖励模型分配奖励分数，这可能导致对每个token都精确建模价值函数的训练变得复杂。为解决这一问题，如图4所示，我们提出了组相对策略优化（Group Relative Policy Optimization, GRPO）</p>
<p>每一个 $o_g $为策略模型对于输入 $q$ 的输出，每一个 $r_g$ 为 RM 对 每一个 $o_g$ 的评分</p>
<img src="/2025/03/19/LLM-Rela/image-20250325201341810.png" class="" title="image-20250325201341810">
<p>不同点：</p>
<ol>
<li>
<p>优势函数：过程监督与结果监督</p>
<img src="/2025/03/19/LLM-Rela/image-20250325202638658.png" class="" title="image-20250325202638658">
</li>
<li>
<p>超参数ε控制策略更新幅度，β调节KL散度约束</p>
</li>
</ol>
<img src="/2025/03/19/LLM-Rela/image-20250325201533207.png" class="" title="image-20250325201533207">
<br>
<h2 id="Sparse-Dense-Model">Sparse &amp; Dense Model</h2>
<p>pic ref: <a target="_blank" rel="noopener" href="https://github.com/jingyaogong/minimind">https://github.com/jingyaogong/minimind</a></p>
<img src="/2025/03/19/LLM-Rela/LLM-structure.png" class="" title="structure">
<img src="/2025/03/19/LLM-Rela/LLM-structure-moe.png" class="" title="structure-moe">

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python-Pytorch/" rel="tag"># Python, Pytorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/02/19/Computer-Network/" rel="prev" title="Computer-Network">
      <i class="fa fa-chevron-left"></i> Computer-Network
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/03/28/Netty/" rel="next" title="Netty">
      Netty <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">LLM&amp;Rela</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-base"><span class="nav-text">LLM-base</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%9F%E6%9C%9B%E4%B8%8E%E6%96%B9%E5%B7%AE"><span class="nav-text">期望与方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%8F%E6%96%B9%E5%B7%AE"><span class="nav-text">协方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-text">优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">随机梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">小批量梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adam"><span class="nav-text">Adam</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1-%E7%88%86%E7%82%B8"><span class="nav-text">梯度消失&amp;爆炸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96-%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">正则化&amp;归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#batch"><span class="nav-text">batch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E9%81%93"><span class="nav-text">通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN-LSTM"><span class="nav-text">RNN&amp;LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RNN"><span class="nav-text">RNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LSTM"><span class="nav-text">LSTM</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-struc"><span class="nav-text">LLM-struc</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder-Decoder"><span class="nav-text">Encoder&amp;Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder-Only"><span class="nav-text">Encoder Only</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder-Only"><span class="nav-text">Decoder Only</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tokenizer"><span class="nav-text">Tokenizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Embedding"><span class="nav-text">Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Positional-Encoding"><span class="nav-text">Positional Encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Base"><span class="nav-text">Base</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RoPE"><span class="nav-text">RoPE</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention"><span class="nav-text">Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-text">注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B-decoder"><span class="nav-text">自注意力 - decoder</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B-encoder"><span class="nav-text">交叉注意力 - encoder</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-text">多头注意力</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A9%E7%A0%81%E6%B3%A8%E6%84%8F%E5%8A%9B-decoder"><span class="nav-text">掩码注意力 - decoder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MHA%E3%80%81MQA%E3%80%81GQA-MLA"><span class="nav-text">MHA、MQA、GQA&amp;MLA</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Layer-Norm-Residual-Network"><span class="nav-text">Layer Norm &amp; Residual Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MLP-FFN"><span class="nav-text">MLP&amp;FFN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98"><span class="nav-text">相关问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-Train"><span class="nav-text">LLM-Train</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Base-2"><span class="nav-text">Base</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parallelism"><span class="nav-text">Parallelism</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DataParallel"><span class="nav-text">DataParallel</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TensorParallel"><span class="nav-text">TensorParallel</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PipelineParallel"><span class="nav-text">PipelineParallel</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deepspeed"><span class="nav-text">Deepspeed</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FSDP"><span class="nav-text">FSDP</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-Inference"><span class="nav-text">LLM-Inference</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Parameters"><span class="nav-text">Parameters</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Temperature"><span class="nav-text">Temperature</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sampling"><span class="nav-text">Sampling</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fine-Tuning"><span class="nav-text">Fine-Tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BitFit-Prefix-Tuning-Prompt-Tuning"><span class="nav-text">BitFit, Prefix Tuning &amp; Prompt Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#P-Tuning"><span class="nav-text">P-Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adapter-Tuning"><span class="nav-text">Adapter Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LoRA"><span class="nav-text">LoRA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E4%B8%8E%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3"><span class="nav-text">奇异值分解与低秩分解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LoRA%E5%BA%94%E7%94%A8%E4%BD%8D%E7%BD%AE"><span class="nav-text">LoRA应用位置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LoRA%E6%94%B9%E8%BF%9B"><span class="nav-text">LoRA改进</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#LoRA-2"><span class="nav-text">LoRA+</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DoRA"><span class="nav-text">DoRA</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#rsLoRA"><span class="nav-text">rsLoRA</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#PiSSA"><span class="nav-text">PiSSA</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LoRA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87"><span class="nav-text">LoRA相关论文</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7"><span class="nav-text">调参技巧</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MAM-Adapter-UniPELT"><span class="nav-text">MAM Adapter &amp; UniPELT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reinforce-Learning-on-LLM"><span class="nav-text">Reinforce Learning on LLM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LLM-DPO"><span class="nav-text">LLM-DPO</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%81%E7%A8%8B"><span class="nav-text">流程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%9A%E5%AE%9A%E4%B9%89%E5%8F%82%E4%B8%8E%E8%80%85%EF%BC%88%E4%B8%A4%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%89"><span class="nav-text">第一阶段：定义参与者（两大模型）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%EF%BC%9A%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-text">第二阶段：准备数据集</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5%EF%BC%9ADPO%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="nav-text">第三阶段：DPO训练循环</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LLM-PPO"><span class="nav-text">LLM-PPO</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%81%E7%A8%8B-2"><span class="nav-text">流程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%9A%E5%AE%9A%E4%B9%89%E5%8F%82%E4%B8%8E%E8%80%85%EF%BC%88%E5%9B%9B%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%89"><span class="nav-text">第一阶段：定义参与者（四大模型）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%EF%BC%9A%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%EF%BC%88Rollout-Phase%EF%BC%89"><span class="nav-text">第二阶段：数据生成（Rollout Phase）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5%EF%BC%9A%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%EF%BC%88Learning-Phase%EF%BC%89"><span class="nav-text">第三阶段：模型学习（Learning Phase）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9"><span class="nav-text">相关内容</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LLM-GRPO"><span class="nav-text">LLM-GRPO</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9-2"><span class="nav-text">相关内容</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sparse-Dense-Model"><span class="nav-text">Sparse &amp; Dense Model</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="marigo1d"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">marigo1d</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/marigo1d" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;marigo1d" rel="noopener" target="_blank">GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">marigo1d</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">528k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">16:01</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
