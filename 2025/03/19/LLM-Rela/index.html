<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"marigo1d.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="LLM相关">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM&amp;Rela">
<meta property="og:url" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/index.html">
<meta property="og:site_name" content="Marigold">
<meta property="og:description" content="LLM相关">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250830142432793.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/8361e16bac5ee3235ef89c78b1a1cf6b.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/e38fac064524158e493a66adb2caed6e.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250402161828313.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250402162106999.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/v2-ee9b5d4a0761d2d1d10acb37cebefba3_1440w.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250706100412469.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250706105452257.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/LLM-structure.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/LLM-structure-moe.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/1.webp">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250830112148964.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250404192612286.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325195443397.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9F%A9%E9%98%B5%E5%9B%BE.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325195459862.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/multi-head-%E6%8B%BC%E6%8E%A5.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/mask-attention-map.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/v2-b257d8660af7678f9c9bdc14d095b6d3_1440w.jpg">
<meta property="og:image" content="https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/4.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250901152222060.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250404100511037.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250405101512560.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250613213945162.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/post-training.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250810092205813.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/0.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250915155839854.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/6.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/6.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/13.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250901103525674.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250901102212626.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250901103305314.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250901103056683.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/v2-eaaf1c00d0c4ea350cd3a79b47de26d3_1440w.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250810092440215.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250929141016096.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250929141029131.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325131654259.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250907172731668.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325151928459.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325153214542.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325153304648.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325153337812.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325195333997.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325201341810.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250901113222981.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325201533207.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250325202638658.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250901105613345.png">
<meta property="article:published_time" content="2025-03-19T02:50:14.000Z">
<meta property="article:modified_time" content="2025-09-29T06:11:25.828Z">
<meta property="article:author" content="marigo1d">
<meta property="article:tag" content="Python, Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://marigo1d.github.io/2025/03/19/LLM-Rela/image-20250830142432793.png">

<link rel="canonical" href="https://marigo1d.github.io/2025/03/19/LLM-Rela/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>LLM&Rela | Marigold</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Marigold</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Salt, Pepper and Birds~</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://marigo1d.github.io/2025/03/19/LLM-Rela/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="marigo1d">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Marigold">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LLM&Rela
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-03-19 10:50:14" itemprop="dateCreated datePublished" datetime="2025-03-19T10:50:14+08:00">2025-03-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-09-29 14:11:25" itemprop="dateModified" datetime="2025-09-29T14:11:25+08:00">2025-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>42k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1:17</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>LLM相关</p>
<span id="more"></span>
<h1>LLM&amp;Rela</h1>
<h2 id="LLM-base">LLM-base</h2>
<h3 id="期望与方差">期望与方差</h3>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>性质</strong></th>
<th style="text-align:center"><strong>期望 E[⋅]</strong></th>
<th style="text-align:center"><strong>方差 Var(⋅)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>线性变换</strong></td>
<td style="text-align:center">$E[aX+b]=aE[X]+b$</td>
<td style="text-align:center">$Var(aX+b)=a^2Var(X)$</td>
</tr>
<tr>
<td style="text-align:center"><strong>独立性影响</strong></td>
<td style="text-align:center">$E[XY]=E[X]E[Y]$（若独立）</td>
<td style="text-align:center">$Var(X+Y)=Var(X)+Var(Y)$（若独立）</td>
</tr>
<tr>
<td style="text-align:center"><strong>与矩的关系</strong></td>
<td style="text-align:center">一阶原点矩</td>
<td style="text-align:center">二阶中心矩</td>
</tr>
</tbody>
</table>
<br>
<h3 id="协方差">协方差</h3>
<p>$$Cov(X, Y) = E[XY] - E[X]E[Y]$$</p>
<p>X, Y相互独立 -&gt; $Cov(X, Y) = 0$</p>
<p>协方差为零仅排除线性关系，而独立性排除所有形式的依赖，在联合正态分布中，两者等价，这是特例而非普遍规律。</p>
<blockquote>
<p>没有线性关系意味着 不能通过线性方程 （如 $Y = aX + b$）来描述变量间的关系，但是它们可能存在非线性关系（如 $Y = aX^2 + bX + c$ 或 $Y = sin(X)$）</p>
<p><strong>独立性</strong> 意味着两个随机变量 $X$ 和 $Y$ 的联合分布完全由它们的边缘分布决定，即 $P(X,Y)=P(X)P(Y)$，且<strong>无法通过用任何有统计意义的任何函数（无论是线性还是非线性）从其中一个变量预测另一个变量</strong>。</p>
</blockquote>
<br>
<h3 id="激活函数">激活函数</h3>
<p>激活函数在神经网络的每一层中引入非线性，使得神经网络能够拟合复杂的非线性模式。常见的激活函数有 Sigmoid、ReLU（Rectified Linear Unit）、Tanh、Leaky ReLU 等</p>
<p><strong>Sigmoid 函数</strong></p>
<blockquote>
<p>sigmoid 乙型形状 $(-\infty, 0) \to (0,\frac{1}{2})$ &amp; $(0, +\infty) \to (\frac{1}{2}, 1)$</p>
</blockquote>
<p><strong>公式：</strong></p>
<p>$\sigma(x) = \frac{1}{1 + e^{-x}}$</p>
<p><strong>特点：</strong></p>
<ul>
<li>输出范围在 (0, 1) 之间</li>
<li>常用于二分类问题的输出层</li>
<li>缺点：在极端值处梯度很小，容易导致梯度消失</li>
</ul>
<br>
<p><strong>Tanh（双曲正切）函数</strong></p>
<p><strong>公式：</strong></p>
<p>$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$</p>
<p><strong>特点：</strong></p>
<ul>
<li>输出范围在 (-1, 1) 之间</li>
<li>相比 Sigmoid 更适合隐藏层，因为它的均值为 0</li>
<li>同样存在梯度消失问题（但稍弱于 Sigmoid）</li>
</ul>
<br>
<p><strong>ReLU（Rectified Linear Unit）函数</strong></p>
<p><strong>公式：</strong></p>
<p>$\text{ReLU}(x) = \max(0, x)$</p>
<p><strong>特点：</strong></p>
<ul>
<li>简单、高效，收敛速度快</li>
<li>输出范围：[0, +∞)</li>
<li>缺点：负值部分梯度为 0，可能导致“神经元死亡”</li>
</ul>
<br>
<p><strong>Leaky ReLU 函数</strong></p>
<p><strong>公式：</strong></p>
<p>$\text{Leaky ReLU}(x) = \begin{cases} x &amp; \text{if } x \geq 0 \ \alpha x &amp; \text{if } x &lt; 0 \end{cases}$</p>
<p>其中，$\alpha$ 是一个很小的正数（如 0.01）</p>
<p><strong>特点：</strong></p>
<ul>
<li>改进了 ReLU 的“死亡神经元”问题</li>
<li>允许负方向有微小的梯度，避免完全失活</li>
</ul>
<br>
<h3 id="损失函数">损失函数</h3>
<p><strong>均方误差（Mean Squared Error, MSE）</strong></p>
<p><strong>公式：</strong></p>
<p>$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$</p>
<ul>
<li>$y_i$：真实值</li>
<li>$\hat{y}_i$：预测值</li>
<li>$n$：样本数</li>
</ul>
<p><strong>应用场景：</strong></p>
<ul>
<li>回归问题（如房价预测）</li>
<li>对异常值敏感，平方项放大误差</li>
</ul>
<br>
<p><strong>平均绝对误差（Mean Absolute Error, MAE）</strong></p>
<p><strong>公式：</strong></p>
<p>$\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$</p>
<p><strong>应用场景：</strong></p>
<ul>
<li>回归问题</li>
<li>更鲁棒，不像 MSE 那样对异常值敏感</li>
</ul>
<br>
<p><strong>Huber Loss（平滑的 MSE 和 MAE 的结合）</strong></p>
<p><strong>公式：</strong></p>
<p>$L_\delta(a) = \begin{cases} \frac{1}{2} a^2 &amp; \text{if } |a| \leq \delta \ \delta (|a| - \frac{1}{2} \delta) &amp; \text{otherwise} \end{cases}$</p>
<p>其中 $a = y - \hat{y}$</p>
<p><strong>应用场景：</strong></p>
<ul>
<li>回归问题</li>
<li>同时兼顾 MAE 的鲁棒性和 MSE 的可导性</li>
</ul>
<br>
<p><strong>交叉熵损失（Cross-Entropy Loss）</strong></p>
<p>▶ 二分类交叉熵（Binary Cross-Entropy）：</p>
<p><strong>公式：</strong></p>
<p>$\text{Loss} = - \left[ y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right]$</p>
<ul>
<li>$y \in {0, 1}$ 为真实标签</li>
<li>$\hat{y}$ 是预测概率</li>
</ul>
<p><strong>应用场景：</strong></p>
<ul>
<li>二分类问题（如猫 vs 狗）</li>
</ul>
<blockquote>
<p>预测值在log内</p>
<p>有负号</p>
</blockquote>
<br>
<p>▶ 多分类交叉熵（Categorical Cross-Entropy）：</p>
<p><strong>公式（softmax 输出）：</strong></p>
<p>$\text{Loss} = - \sum_{i=1}^{C} y_i \log(\hat{y}_i)$</p>
<ul>
<li>$C$ 是类别总数</li>
<li>$y_i$ 是 one-hot 编码的真实标签</li>
<li>$\hat{y}_i$ 是第 ii 类的预测概率</li>
</ul>
<p><strong>应用场景：</strong></p>
<ul>
<li>多分类问题（如数字识别）</li>
</ul>
<br>
<p><strong>KL 散度（Kullback–Leibler Divergence）</strong></p>
<p><strong>公式：</strong></p>
<p>$D_{KL}(P \parallel Q) = \sum_{i} P(i) \log\left(\frac{P(i)}{Q(i)}\right)$</p>
<ul>
<li>$P$：真实分布</li>
<li>$Q$：预测分布</li>
</ul>
<p><strong>应用场景：</strong></p>
<ul>
<li>分布之间的距离度量（如在生成模型中）</li>
</ul>
<blockquote>
<p>交叉熵 = KL散度 + 熵</p>
</blockquote>
<br>
<h3 id="优化算法">优化算法</h3>
<h4 id="梯度下降">梯度下降</h4>
<p>Gradient Descent</p>
<p><strong>公式：</strong></p>
<p>$\theta := \theta - \eta \cdot \nabla_\theta J(\theta)$</p>
<ul>
<li>$\theta$：模型参数</li>
<li>$\eta$：学习率（learning rate）</li>
<li>$\nabla_\theta J(\theta)$：损失函数 $J$ 关于参数 $\theta$ 的梯度</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>每次用<strong>全部数据</strong>计算梯度，更新参数</li>
<li>精度高但计算开销大，适合小数据集</li>
</ul>
<br>
<h4 id="随机梯度下降">随机梯度下降</h4>
<p>Stochastic Gradient Descent, SGD</p>
<p><strong>公式：</strong></p>
<p>$\theta := \theta - \eta \cdot \nabla_\theta J(\theta; x^{(i)}, y^{(i)})$</p>
<ul>
<li>每次仅用<strong>一个样本</strong> $(x(i),y(i))(x^{(i)}, y^{(i)})$ 更新一次参数</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>计算效率高、更新频繁</li>
<li>噪声大，有时不稳定，但有助于跳出局部最优</li>
</ul>
<br>
<h4 id="小批量梯度下降">小批量梯度下降</h4>
<p>Adaptive Moment Estimation</p>
<p><strong>公式：</strong></p>
<p>$\theta := \theta - \eta \cdot \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta J(\theta; x^{(i)}, y^{(i)})$</p>
<ul>
<li>每次用一小批（mini-batch）样本来计算梯度</li>
<li>折中效率与稳定性，是深度学习中<strong>最常用的方式</strong></li>
</ul>
<br>
<h4 id="Adam">Adam</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://distill.pub/2017/momentum/">https://distill.pub/2017/momentum/</a></p>
<img src="/2025/03/19/LLM-Rela/image-20250830142432793.png" class="" title="image-20250830142432793">
<p>Adaptive Moment Estimation</p>
<p>Adam = <strong>Momentum（动量）</strong> + <strong>RMSProp（自适应学习率）</strong> + <strong>偏差修正</strong></p>
<br>
<p><strong>动量法（Momentum）来源</strong></p>
<p>先看普通的梯度下降更新：</p>
<p>$\theta := \theta - \eta \cdot \nabla_\theta J(\theta)$</p>
<p>但这个更新方向容易“来回震荡”，所以引入动量的思想，让参数更新像“带惯性的小球”那样滑下去：</p>
<p>动量法公式：</p>
<p>$v_t = \beta_1 v_{t-1} + (1 - \beta_1) \nabla_\theta J(\theta)$</p>
<p>$\theta := \theta - \eta \cdot v_t$</p>
<p>这个动量 $v_t$ 相当于对梯度的指数加权平均。</p>
<p>Adam 的第一部分：</p>
<p>$m_t = \beta_1 m_{t-1} + (1 - \beta_1) \cdot g_t$</p>
<p>就是来自这个思路，$m_t$ ≈ 平滑梯度（momentum）</p>
<br>
<p><strong>RMSProp 来源：自适应学习率</strong></p>
<p>RMSProp 想解决的问题是：<strong>不同参数的梯度尺度不同时，用相同学习率不合适</strong>。</p>
<p>所以它引入一个“平方梯度的指数平均”：</p>
<p>$s_t = \beta_2 s_{t-1} + (1 - \beta_2) g_t^2$</p>
<p>$\theta := \theta - \frac{\eta}{\sqrt{s_t} + \epsilon} \cdot g_t$</p>
<p>Adam 的第二部分：</p>
<p>$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$</p>
<p>就是借鉴了 RMSProp，让学习率根据梯度历史自适应调整。</p>
<br>
<p><strong>偏差修正的来由</strong></p>
<p>Adam 一开始 m_1, v_1 都是从 0 开始的，但因为是指数加权平均，会导致初始几步偏小，<strong>“有偏估计”</strong>。</p>
<p>所以做了修正：</p>
<p>$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$</p>
<p>这个公式来源于“期望的无偏估计推导”，用数学方法校正初期偏小的问题。</p>
<blockquote>
<p>Q：为什么要做偏差修正？</p>
<p>A：因为初始时刻 $m_t$、$v_t$ 都从 0 开始，用指数加权会造成“低估”真实值（特别在前几步）。</p>
<p>所以 Adam 中引入了：</p>
<p>$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$</p>
<p>下面<strong>推导</strong>这个修正项是怎么来的。</p>
<p>推导一阶动量偏差修正项（以 $m_t$ 为例）</p>
<p>我们从递推公式出发（假设 $m_0$ = 0）：</p>
<p>$m_1 = (1 - \beta_1) g_1  $</p>
<p>$m_2 = \beta_1 m_1 + (1 - \beta_1) g_2 = \beta_1 (1 - \beta_1) g_1 + (1 - \beta_1) g_2$</p>
<p>$m_3 = \beta_1^2 (1 - \beta_1) g_1 + \beta_1 (1 - \beta_1) g_2 + (1 - \beta_1) g_3$</p>
<p>可推广为：</p>
<p>$m_t = (1 - \beta_1) \sum_{i=1}^{t} \beta_1^{t-i} g_i$</p>
<p>期望分析：$m_t$ 是一个<strong>有偏估计</strong></p>
<p>我们希望的是：</p>
<p>$\mathbb{E}[m_t] = \mathbb{E}[g_t]$</p>
<p>但是上面推导的形式中：</p>
<p>$\mathbb{E}[m_t] = (1 - \beta_1) \sum_{i=1}^{t} \beta_1^{t-i} \mathbb{E}[g_i]$</p>
<p>如果我们假设：</p>
<ul>
<li>
<p>梯度是平稳的（即各时刻期望相同）：</p>
<p>$\mathbb{E}[g_1] = \mathbb{E}[g_2] = \dots = \mathbb{E}[g_t] = \mu$</p>
</li>
</ul>
<p>那么就有：</p>
<p>$\mathbb{E}[m_t] = (1 - \beta_1) \cdot \mu \sum_{i=1}^{t} \beta_1^{t - i} = \mu \cdot (1 - \beta_1) \cdot \sum_{k=0}^{t-1} \beta_1^k$</p>
<p>这是一个等比数列，求和后得到：</p>
<p>$\mathbb{E}[m_t] = \mu \cdot (1 - \beta_1) \cdot \frac{1 - \beta_1^t}{1 - \beta_1} = \mu \cdot (1 - \beta_1^t)$</p>
<p>所以：</p>
<p>$\boxed{\mathbb{E}[m_t] = \mu \cdot (1 - \beta_1^t)} \quad \text{有偏！}$</p>
<p>如何修正？</p>
<p>为了得到无偏估计，我们让：</p>
<p>$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$</p>
<p>那么：</p>
<p>$\mathbb{E}[\hat{m}_t] = \frac{\mathbb{E}[m_t]}{1 - \beta_1^t} = \mu$</p>
<p>成功修正偏差 🎉！</p>
<p>同理：</p>
<p>对于二阶动量 $v_t$，完全一样的推理过程也可以得到：</p>
<p>$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$</p>
</blockquote>
<br>
<p><strong>Adam公式：</strong></p>
<ol>
<li>
<p>一阶矩估计（类似动量）：</p>
<p>$m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla_\theta J(\theta)$</p>
</li>
<li>
<p>二阶矩估计（平方梯度）：</p>
<p>$v_t = \beta_2 v_{t-1} + (1 - \beta_2)(\nabla_\theta J(\theta))^2$</p>
</li>
<li>
<p>偏差校正：</p>
<p>$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$</p>
</li>
<li>
<p>参数更新：</p>
<p>$\theta := \theta - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$</p>
</li>
</ol>
<ul>
<li>通常 $\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$</li>
</ul>
<p><strong>特点：</strong></p>
<ul>
<li>自适应调整每个参数的学习率</li>
<li>适用于大规模数据和参数模型，深度学习中的默认选择之一</li>
</ul>
<br>
<h3 id="梯度消失-爆炸">梯度消失&amp;爆炸</h3>
<p>梯度消失是指，损失函数对网络中某参数计算梯度，由于链式法则，计算链过长，得到的梯度计算结果接近0</p>
<p>可能出现：</p>
<ol>
<li>参数初始化存在问题，权重参数w_i过小导致梯度计算值接近0</li>
<li>参数/激活函数存在非线性运算，值被快速缩小导致梯度计算值接近0</li>
</ol>
<p>参数的更新依赖于梯度，当出现梯度消失时，参数无法得到更新</p>
<br>
<p><strong>解决梯度消失问题的方法：</strong></p>
<ol>
<li>使用恰当的激活函数：某些激活函数（如ReLU和Leaky ReLU）在反向传播过程中更不容易出现梯度消失问题，可以考虑使用它们替代Sigmoid和Tanh。</li>
<li>批量归一化（Batch Normalization）：批量归一化可以加速训练过程，还可以缓解梯度消失问题，使得网络更稳定和更易训练。</li>
<li>使用残差连接（Residual Connections）：残差连接可以跳过某些层，将输入直接与输出相加，有助于信息的传递和梯度的流动，减少梯度消失问题。</li>
<li>调整网络架构：适当调整网络的深度，避免设计过深的网络结构，也有助于减少梯度消失的影响。</li>
</ol>
<p><strong>解决梯度爆炸问题的方法：</strong></p>
<ol>
<li>梯度截断（Gradient Clipping）：设置一个梯度阈值，在反向传播过程中，如果梯度超过该阈值，则将其裁剪为阈值以内的数值，避免梯度爆炸。</li>
<li>使用恰当的权重初始化：合适的权重初始化可以减少梯度爆炸问题。例如，Xavier/Glorot初始化针对Sigmoid和Tanh激活函数的网络效果较好，而He初始化针对ReLU激活函数的网络效果较好。</li>
<li>减少学习率：较小的学习率可以缓解梯度爆炸的影响，但要注意不要将学习率设置得过小，以免影响收敛速度。</li>
<li>批量归一化（Batch Normalization）：同样，批量归一化在训练过程中有助于控制梯度的大小，减少梯度爆炸问题。</li>
</ol>
<br>
<h3 id="正则化-归一化">正则化&amp;归一化</h3>
<p><strong>正则化（Regularization）：</strong> 正则化是通过在损失函数中添加一个额外的项，来限制模型参数的大小，从而避免过拟合问题。常见的正则化项有L1正则化和L2正则化。</p>
<ul>
<li>L1正则化：在损失函数中添加模型参数的绝对值之和。L1正则化有助于稀疏模型，即将一些参数的值压缩为0，从而减少模型的复杂度。</li>
<li>L2正则化：在损失函数中添加模型参数的平方之和。L2正则化对参数的惩罚更加平滑，通常会让参数接近于0，但不会严格地等于0。</li>
</ul>
<p>正则化的目的是防止模型在训练集上过度拟合，使得模型能够更好地泛化到未见过的新数据上。</p>
<p>假设我们在线性回归中训练一个模型：</p>
<p>原始损失函数（MSE）：</p>
<p>$Loss = (y - ŷ )²$</p>
<p>加入 L2 正则化后：</p>
<p>$Loss = (y - ŷ )² + λ * ||w||²$</p>
<p>其中：</p>
<ul>
<li><code>||w||²</code> 表示所有权重的平方和</li>
<li><code>λ</code> 是正则化强度（超参数）</li>
</ul>
<p>➡️ <strong>作用：</strong> 如果某些权重太大，会被惩罚，从而让模型更简单、泛化能力更强。</p>
<br>
<p><strong>归一化（Normalization）：</strong> 归一化是将数据按比例缩放，使其值落在特定范围内。在深度学习中，常见的归一化方法是将输入特征缩放到0和1之间，或者使其均值为0，方差为1。</p>
<ul>
<li>最小-最大归一化（Normalization）：将数据缩放到指定的最小值和最大值之间，公式为：$(x - x_{min}) / (x_{max} - x_{min})$</li>
<li>均值-方差归一化（Standardization）：将数据缩放为均值为0，方差为1的分布，公式为：$(x - mean) / std$</li>
</ul>
<p>归一化的目的是将特征的值统一到相似的范围内，加速模型的训练过程，同时有助于梯度的传播和优化算法的收敛。</p>
<p>常见方法：</p>
<ul>
<li>
<p><strong>Min-Max 归一化：</strong> 把数据压缩到 [0,1] 区间</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x&#x27; = (x - min) / (max - min)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong>Z-score 标准化（Standardization）：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x&#x27; = (x - mean) / std</span><br></pre></td></tr></table></figure>
</li>
</ul>
<br>
<p><strong>两者的作用：</strong></p>
<ul>
<li>正则化用于防止过拟合，通过约束模型参数的大小，减少模型的复杂度，提高模型的泛化能力。</li>
<li>归一化用于将数据的特征缩放到统一的范围内，使得训练过程更稳定，加速优化算法的收敛，并且有助于避免梯度消失或梯度爆炸问题。</li>
</ul>
<p>在实际应用中，正则化和归一化通常是一起使用的，以提高深度学习模型的性能和训练效果。</p>
<br>
<h3 id="batch">batch</h3>
<p>在机器学习，特别是深度学习训练中，batch_size 为 1 做两次训练（假设数据集相同）和 batch_size 为 2 做一次训练<strong>不等价</strong>。</p>
<p>这里涉及到的主要区别在于：</p>
<ol>
<li><strong>梯度计算：</strong>
<ul>
<li><strong>Batch Size = 1 (两次训练)：</strong> 每次训练迭代时，模型会计算<strong>一个样本</strong>的损失，并基于这一个样本的损失来计算梯度。然后，优化器会使用这个梯度来更新模型的权重。这意味着梯度更新的方差会非常大，因为每次更新都只依赖于一个非常小的“样本”信息。</li>
<li><strong>Batch Size = 2 (一次训练)：</strong> 每次训练迭代时，模型会计算<strong>两个样本</strong>的损失，然后将这两个样本的梯度<strong>平均</strong>（或求和后归一化）起来，得到一个更稳定的梯度。优化器会使用这个平均梯度来更新模型的权重。</li>
</ul>
</li>
<li><strong>梯度更新的频率与方向：</strong>
<ul>
<li>batch_size = 1 做两次训练：会发生两次<strong>独立的</strong>权重更新。两次更新的方向可能差异很大，因为它们基于不同的单个样本。</li>
<li>batch_size = 2 做一次训练：会发生一次权重更新，这个更新的方向是两个样本的梯度<strong>综合平均</strong>的结果。</li>
</ul>
</li>
<li><strong>损失函数评估：</strong>
<ul>
<li>batch_size = 1 做两次训练：损失是在单个样本上评估的。</li>
<li>batch_size = 2 做一次训练：损失是在两个样本的平均或总和上评估的。</li>
</ul>
</li>
<li><strong>收敛性与稳定性：</strong>
<ul>
<li><strong>Batch Size = 1 (随机梯度下降SGD)：</strong> 梯度方差大，导致训练过程更不稳定，路径更“抖动”。虽然理论上SGD在非凸优化中也能找到局部最优解，但收敛速度可能较慢，并且在实践中更容易陷入次优解或在训练过程中出现震荡。这种极端情况下的SGD通常被称为<strong>在线学习（Online Learning）</strong>。</li>
<li><strong>Batch Size = 2 (小批量梯度下降Mini-Batch GD)：</strong> 通过对多个样本的梯度进行平均，可以减少梯度的方差，使梯度方向更稳定、更准确地指向损失函数的下降方向。这通常能带来更稳定的训练过程和更快的收敛速度。</li>
</ul>
</li>
</ol>
<br>
<h3 id="通道">通道</h3>
<img src="/2025/03/19/LLM-Rela/8361e16bac5ee3235ef89c78b1a1cf6b.png" class="" title="channel">
<p>多通道卷积过程</p>
<p>输入一张三通道的图片，有多个卷积核进行卷积，并且每个卷积核都有三通道，分别对这张输入图片的三通道进行卷积操作。每个卷积核，分别输出三个通道，这三个通道进行求和，得到一个featuremap，有多少个卷积核，就有多少个featuremap</p>
<br>
<h3 id="RNN-LSTM">RNN&amp;LSTM</h3>
<h4 id="RNN">RNN</h4>
<img src="/2025/03/19/LLM-Rela/e38fac064524158e493a66adb2caed6e.png" class="" title="RNN">
<br>
<h4 id="LSTM">LSTM</h4>
<p><strong>输出</strong></p>
<p>短期记忆 $h_t$，长期记忆 $c_t$，input $x_t$<br>
$$<br>
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)<br>
$$</p>
<p>$$<br>
h_t = o_t \odot \tanh(c_t)<br>
$$</p>
<p><strong>遗忘门(蓝色)</strong><br>
$$<br>
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)<br>
$$</p>
<p><strong>输入门和候选记忆</strong></p>
<ul>
<li>输入门控制当前输入信息 $x_t$ 的写入程度：</li>
</ul>
<p>$$<br>
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)<br>
$$</p>
<ul>
<li>候选记忆生成新的候选信息 $\tilde{c}_t$：</li>
</ul>
<p>$$<br>
\tilde{c}<em>t = \tanh(W_c \cdot [h</em>{t-1}, x_t] + b_c)<br>
$$</p>
<br>
<p><strong>短期记忆更新</strong></p>
<p>结合遗忘门和输入门的结果更新短期记忆 $c_t$：<br>
$$<br>
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t<br>
$$</p>
<ul>
<li>$\odot$ 表示逐元素相乘。</li>
<li>第一项 $f_t \odot c_{t-1}$ 保留历史信息，第二项 $i_t \odot \tilde{c}_t$ 添加新信息。</li>
</ul>
<img src="/2025/03/19/LLM-Rela/image-20250402161828313.png" class="" title="image-20250402161828313">
<br>
<p><strong>LSTM Process</strong></p>
<img src="/2025/03/19/LLM-Rela/image-20250402162106999.png" class="" title="image-20250402162106999">
<br>
<h2 id="LLM-struc">LLM-struc</h2>
<p>大模型从模型架构上主要分为三种：Only-encoder, Only-Decoder, Encoder-Decoder三种模型架构</p>
<ul>
<li>Only-encoder：例如BERT，通过在大规模无标签文本上进行预训练，然后在下游任务上进行微调，具有强大的语言理解能力和表征能力。</li>
<li>Only-Decoder: 例如GPT，通过在大规模无标签文本上进行预训练，然后在特定任务上进行微调，具有很强的生成能力和语言理解能力。</li>
<li>Encoder-Decoder：例如T5（Text-to-Text Transfer Transformer）可以用于多种自然语言处理任务，如文本分类、机器翻译、问答等。</li>
</ul>
<img src="/2025/03/19/LLM-Rela/v2-ee9b5d4a0761d2d1d10acb37cebefba3_1440w.jpg" class="" title="img">
<br>
<h3 id="Encoder-Decoder">Encoder&amp;Decoder</h3>
<img src="/2025/03/19/LLM-Rela/image-20250706100412469.png" class="" title="image-20250706100412469">
<p>ref: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></p>
<br>
<br>
<h3 id="Encoder-Only">Encoder Only</h3>
<img src="/2025/03/19/LLM-Rela/image-20250706105452257.png" class="" title="image-20250706105452257">
<p>ref: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
<h3 id="Decoder-Only">Decoder Only</h3>
<img src="/2025/03/19/LLM-Rela/LLM-structure.png" class="" title="structure">
<img src="/2025/03/19/LLM-Rela/LLM-structure-moe.png" class="" title="structure-moe">
<p>ref: <a target="_blank" rel="noopener" href="https://github.com/jingyaogong/minimind">minimind</a></p>
<br>
<p>Comparsion</p>
<img src="/2025/03/19/LLM-Rela/1.webp" class="" title="img">
<p>ref: <a target="_blank" rel="noopener" href="https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html">https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html</a></p>
<br>
<h3 id="Tokenizer">Tokenizer</h3>
<p>分词表</p>
<h4 id="BPE">BPE</h4>
<p>案例：BPE (Byte Pair Encoding)  ref: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.16609">QWEN TECHNICAL REPORT</a></p>
<p>假设我们有以下非常小的文本语料库：</p>
<p><code>&quot;low low low low low low low low low low&quot;</code><br>
<code>&quot;lower lower lower lower lower lower lower lower&quot;</code><br>
<code>&quot;newest newest newest newest&quot;</code><br>
<code>&quot;widest widest widest widest&quot;</code></p>
<p>并且我们希望通过 BPE 来构建一个词汇表。最初，我们的“词汇表”可能只包含所有的独立字符。</p>
<p><strong>初始状态：</strong></p>
<ul>
<li><strong>语料中的独立字符（初始token）：</strong> <code>l, o, w, e, r, n, s, t, i, d</code> (以及一个空格字符，我们这里为了简化暂时忽略它，但实际BPE会处理所有字节)</li>
<li><strong>当前词汇表：</strong> <code>[l, o, w, e, r, n, s, t, i, d]</code></li>
</ul>
<p><strong>BPE 步骤：</strong></p>
<p>BPE 的核心是<strong>迭代地寻找语料中出现频率最高的相邻“字节对”（或字符对）并将其合并成一个新的单元。</strong></p>
<p><strong>第一次迭代：</strong></p>
<ol>
<li>
<p><strong>统计字符对频率：</strong></p>
<ul>
<li><code>lo</code>: 出现很多次（来自 “low”, “lower”）</li>
<li><code>ow</code>: 出现很多次（来自 “low”, “lower”）</li>
<li><code>er</code>: 出现很多次（来自 “lower”）</li>
<li><code>ne</code>: 出现多次（来自 “newest”）</li>
<li><code>es</code>: 出现多次（来自 “newest”）</li>
<li><code>st</code>: 出现多次（来自 “newest”, “widest”）</li>
<li><code>wi</code>: 出现多次（来自 “widest”）</li>
<li><code>id</code>: 出现多次（来自 “widest”）</li>
<li><code>de</code>: 出现多次（来自 “widest”）</li>
</ul>
</li>
<li>
<p><strong>找到频率最高的对：</strong> 假设 <code>(l, o)</code> 和 <code>(o, w)</code> 的频率非常高（因为 “low” 和 “lower” 都包含它们）。<br>
我们选择 <code>(l, o)</code> 和 <code>(o, w)</code> 进行合并。为了简化，我们先合并 <code>(l, o)</code>。</p>
</li>
<li>
<p><strong>合并 <code>lo</code>：</strong> 将所有 <code>l o</code> 替换为新的 token <code>lo</code>。</p>
<ul>
<li><code>&quot;low low ...&quot;</code> 变成 <code>&quot;low low ...&quot;</code> (lo 依然是 lo，但现在是一个整体token)</li>
<li><code>&quot;lower lower ...&quot;</code> 变成 <code>&quot;lower lower ...&quot;</code></li>
<li><strong>新的词汇表：</strong> <code>[l, o, w, e, r, n, s, t, i, d, lo]</code></li>
</ul>
</li>
</ol>
<p><strong>第二次迭代：</strong></p>
<ol>
<li>
<p><strong>统计新的字符/token 对频率：</strong></p>
<ul>
<li>现在 <code>(lo, w)</code> 出现的频率很高（来自 “low”, “lower”）</li>
<li><code>(e, r)</code> 频率很高</li>
<li><code>(n, e)</code> 频率很高</li>
<li><code>(e, s)</code> 频率很高</li>
<li><code>(s, t)</code> 频率很高</li>
</ul>
</li>
<li>
<p><strong>找到频率最高的对：</strong> 假设 <code>(lo, w)</code> 是频率最高的对。</p>
</li>
<li>
<p><strong>合并 <code>low</code>：</strong> 将所有 <code>lo w</code> 替换为新的 token <code>low</code>。</p>
<ul>
<li><code>&quot;low low ...&quot;</code> 变成 <code>&quot;low low ...&quot;</code> (现在 <code>low</code> 是一个整体 token)</li>
<li><code>&quot;lower lower ...&quot;</code> 中的 <code>low</code> 也变成一个整体 token。</li>
<li><strong>新的词汇表：</strong> <code>[l, o, w, e, r, n, s, t, i, d, lo, low]</code></li>
</ul>
</li>
</ol>
<p><strong>第三次迭代：</strong></p>
<ol>
<li>
<p><strong>统计新的字符/token 对频率：</strong></p>
<ul>
<li><code>(low, er)</code> 出现频率很高（来自 “lower”）</li>
<li><code>(n, ew)</code> (如果 <code>ew</code> 被合并了)</li>
<li><code>(ne, st)</code></li>
<li><code>(wi, de)</code> (如果 <code>de</code> 被合并了)</li>
</ul>
</li>
<li>
<p><strong>找到频率最高的对：</strong> 假设 <code>(low, er)</code> 是频率最高的对。</p>
</li>
<li>
<p><strong>合并 <code>lower</code>：</strong> 将所有 <code>low er</code> 替换为新的 token <code>lower</code>。</p>
<ul>
<li><code>&quot;lower lower ...&quot;</code> 变成 <code>&quot;lower lower ...&quot;</code> (现在 <code>lower</code> 是一个整体 token)</li>
<li><strong>新的词汇表：</strong> <code>[l, o, w, e, r, n, s, t, i, d, lo, low, lower]</code></li>
</ul>
</li>
</ol>
<p><strong>继续迭代…</strong></p>
<p>这个过程会一直重复：</p>
<ul>
<li>统计当前所有 token 序列中<strong>相邻 token 对</strong>的频率。</li>
<li>选择频率最高的对。</li>
<li>将该对合并成一个新的、更长的 token。</li>
<li>将新 token 加入词汇表。</li>
<li>更新语料中的表示（将旧的对替换为新的 token）。</li>
</ul>
<br>
<h4 id="WordPiece">WordPiece</h4>
<p>（Google BERT 使用）</p>
<ul>
<li>思路和 BPE 类似，但不是贪心地选 <strong>频率最高的符号对</strong>，而是选择能最大化 <strong>训练数据似然</strong> 的合并。</li>
<li>偏向生成更能提高语言模型概率的子词。</li>
<li>例子：<code>unaffable</code> → <code>[un, ##aff, ##able]</code>。</li>
</ul>
<p>案例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">unaffable</span><br><span class="line">unaffordable</span><br><span class="line">unable</span><br><span class="line">affable</span><br></pre></td></tr></table></figure>
<p>初始词表：所有<strong>字符</strong> <code>&#123;u,n,a,f,b,l,e,o,r,d&#125;</code>（以及特殊符号这里忽略）。</p>
<p>初始分词：全部按字符切分。</p>
<p>记号：语料被分成的所有 token 个数记为 <code>N</code>，每个 token 的出现次数为 <code>c(t)</code>。<br>
对于给定词表，极大似然下 token 概率是 <code>p(t)=c(t)/N</code>，训练目标是<strong>最大化对数似然</strong>：<br>
$$<br>
L = \sum_t c(t) · \log p(t) = \sum_t c(t) · \log(\frac{c(t)}{N})<br>
$$<br>
在本语料里，初始共有 <code>N = 34</code> 个字符 token（4 个词加起来的总字符数）。</p>
<p>与 BPE 一样，我们只在<strong>词内相邻</strong>位置取对（bigram）作为候选。但 WordPiece 不直接选“频数最高”的 pair，而是：</p>
<blockquote>
<p>对每个候选 AB，<strong>假设</strong>把 AB 作为一个新子词加入词表、并用“最长匹配”重新分词，计算新的对数似然 <code>L'</code>，取 <strong>ΔL = L’ - L</strong> 最大的那个。</p>
</blockquote>
<br>
<h4 id="SentencePiece">SentencePiece</h4>
<p>Google T5, ALBERT 使用</p>
<ul>
<li>直接把输入文本看作 <strong>不带空格的字符序列</strong>（空格也算符号）。</li>
<li>支持两种算法：
<ul>
<li><strong>BPE 模式</strong>（与经典 BPE 一致）。</li>
<li><strong>Unigram LM 模式</strong>（基于概率模型，从候选子词集中选择能最大化似然的子词集合）。</li>
</ul>
</li>
</ul>
<br>
<h4 id="词表">词表</h4>
<p>对应于项目中的 <code>tokenizer.json</code> 文件，它是一个至关重要的组件，其核心功能是将人类可读的文本转换为模型可理解的数字 ID 序列。可以将其理解为模型的“字典”或“编码器”。</p>
<p>具体来说，<code>tokenizer.json</code> 文件中包含了：</p>
<ol>
<li><strong>词表（Vocabulary）</strong>：这是最主要的部分，它列出了分词器识别的所有词元（tokens），每个词元都对应一个唯一的数字 ID。这些词元可以是单个字符、常用单词、词根、词缀甚至是不规则的子词单元（subword units），例如通过BPE（Byte Pair Encoding）或WordPiece等算法生成的。</li>
<li><strong>分词规则（Tokenization Rules）</strong>：除了词表本身，<code>tokenizer.json</code> 还定义了如何将原始文本拆分成这些词元序列的规则。这包括预处理步骤（如大小写转换、标点符号处理）、分词算法的配置以及如何处理未知词元（Out-Of-Vocabulary, OOV）等。</li>
</ol>
<p><strong>词表大小（Vocabulary Size）</strong> 指的是 <strong><code>tokenizer.json</code> 中定义的词元数量</strong>（即词表中词元ID的最大值加一），再加上 <strong>模型或分词器预设的特殊字符（Special Tokens）的数量</strong>。</p>
<p><strong>特殊字符</strong>通常包括：</p>
<ul>
<li><strong><code>[CLS]</code> (Classification Token)</strong>：在BERT等模型中用于表示句子的开头，其输出常用于分类任务。</li>
<li><strong><code>[SEP]</code> (Separation Token)</strong>：用于分隔不同的句子或文本片段。</li>
<li><strong><code>[PAD]</code> (Padding Token)</strong>：用于将不同长度的序列填充到相同的长度，以便于批处理。</li>
<li><strong><code>[UNK]</code> (Unknown Token)</strong>：当分词器遇到不在词表中的词元时，会用此标记替代。</li>
<li><strong><code>[MASK]</code> (Mask Token)</strong>：在预训练任务（如掩码语言模型）中用于替换被遮蔽的词元。</li>
</ul>
<p>因此，<strong>词表大小 = <code>tokenizer.json</code> 中唯一词元的数量 + 特殊字符的数量</strong>。这个大小直接决定了模型能够理解和表示的词元种类，也影响着模型的参数量和性能。</p>
<br>
<h3 id="Embedding">Embedding</h3>
<p>将 <strong>token ID 序列</strong> 转换为 <strong>连续的、稠密的向量表示</strong></p>
<p>例如，</p>
<ol>
<li>
<p><strong>原始文本:</strong> “The cat sat on the mat.”</p>
</li>
<li>
<p><strong>分词器 (Tokenizer):</strong></p>
<ul>
<li>将原始文本分解成一个个独立的单元，通常是词（word）或子词（subword）。</li>
<li>例如：<code>[&quot;The&quot;, &quot;cat&quot;, &quot;sat&quot;, &quot;on&quot;, &quot;the&quot;, &quot;mat&quot;, &quot;.&quot;]</code></li>
<li>分词器还会将这些词/子词映射到它们对应的<strong>整数ID (Integer ID)</strong>。</li>
<li>例如：<code>[101, 234, 567, 890, 101, 321, 999]</code> (这只是示例ID)</li>
</ul>
</li>
<li>
<p><strong>Embedding 层:</strong></p>
<ul>
<li>模型接收的输入是这些<strong>整数ID序列</strong>。</li>
<li>Embedding层本质上是一个<strong>查找表（lookup table）</strong>。</li>
<li>当接收到一个整数ID时，它会去这个查找表中找到该ID对应的<strong>预训练好的（或随机初始化后在训练中学习到的）低维、稠密的浮点数向量</strong>。</li>
<li>例如，ID <code>234</code> (对应“cat”) 可能被查找到一个像 <code>[0.1, -0.3, 0.8, ..., 0.5]</code> 这样的512维向量。</li>
<li>这些向量就是我们所说的<strong>词向量（Word Embeddings）<strong>或</strong>词嵌入</strong>。</li>
<li>这些词向量的特点是：
<ul>
<li><strong>低维：</strong> 相比独热编码的词汇表大小，词向量的维度通常是几十到几百（例如，50, 100, 300, 512, 768等）。</li>
<li><strong>稠密：</strong> 向量中的每个元素都是一个非零的浮点数。</li>
<li><strong>语义信息：</strong> 通过大量的语料库训练，这些向量能够捕捉词语的语义和语法信息，使得语义相似的词在向量空间中距离更近。</li>
</ul>
</li>
</ul>
</li>
</ol>
<br>
<h4 id="Word2Vec">Word2Vec</h4>
<p>案例：Word2Vec训练方法</p>
<p>Word2Vec的CBOW（Continuous Bag of Words）模型是一种通过上下文词预测目标词的神经网络模型。以下是其训练流程的详细说明，并结合具体例子进行解释：</p>
<ol>
<li>数据准备</li>
</ol>
<p>首先，需要准备训练数据，通常是大量的文本语料。文本数据需要进行分词等预处理，将文本转换为词语序列。例如，句子“I learn NLP everyday”会被分词为<code>[&quot;I&quot;, &quot;learn&quot;, &quot;NLP&quot;, &quot;everyday&quot;]</code>。</p>
<ol>
<li>
<p>创建上下文窗口</p>
<p>对于每个目标词，CBOW模型定义了一个上下文窗口。窗口大小由超参数<code>window</code>指定，表示目标词左右两侧的词语数目。例如，窗口大小为2时，目标词“NLP”的上下文词为<code>[&quot;I&quot;, &quot;learn&quot;, &quot;everyday&quot;]</code>。</p>
</li>
<li>
<p>构建训练样本</p>
<p>对于每个目标词，CBOW模型从其上下文窗口中收集上下文词。每个训练样本由上下文词构成，目标是预测目标词。例如，目标词“NLP”的训练样本为<code>&#123;&quot;context&quot;: [&quot;I&quot;, &quot;learn&quot;, &quot;everyday&quot;], &quot;target&quot;: &quot;NLP&quot;&#125;</code>。</p>
</li>
<li>
<p>模型结构</p>
<p>CBOW模型是一个简单的三层神经网络，包括输入层、隐藏层和输出层： - <strong>输入层</strong>：上下文词用one-hot向量表示。例如，词汇表大小为10,000，单词“I”可能表示为<code>[1, 0, 0, ..., 0]</code>。 - <strong>隐藏层</strong>：通过词向量矩阵（Embedding Matrix）将输入的one-hot向量转换为低维词向量（通常是100～300维）。然后将所有上下文词的词向量相加取平均，作为隐藏层向量。 - <strong>输出层</strong>：隐藏层向量乘以输出权重矩阵，得到输出向量。使用Softmax函数计算目标词的概率分布。</p>
</li>
<li>
<p>训练目标</p>
<p>CBOW模型的训练目标是最大化给定上下文词时目标词的条件概率，即最大化$P(w_t | w_{t-c}, w_{t-c+1}, …, w_{t+c})$，其中$w_t$是目标词，$w_{t-c}$到$w_{t+c}$是上下文词。</p>
</li>
<li>
<p>梯度下降</p>
<p>使用梯度下降或其变种，通过反向传播算法调整嵌入层的权重，使得模型的预测更接近实际的目标词。</p>
</li>
<li>
<p>重复迭代</p>
<p>重复以上步骤多次，直到模型收敛到一个合适的状态。每一轮迭代都遍历整个训练数据。</p>
</li>
</ol>
<br>
<p>例子</p>
<p>假设语料为“I learn NLP everyday”，目标词为“NLP”，上下文词为<code>[&quot;I&quot;, &quot;learn&quot;, &quot;everyday&quot;]</code>：</p>
<ol>
<li>将上下文词转换为one-hot向量。</li>
<li>将one-hot向量乘以输入权重矩阵，得到词向量。</li>
<li>将所有上下文词的词向量相加取平均，得到隐藏层向量。</li>
<li>将隐藏层向量乘以输出权重矩阵，得到输出向量。</li>
<li>使用Softmax函数计算目标词“NLP”的概率分布。</li>
<li>通过损失函数（如负对数似然）计算预测误差，并使用梯度下降更新模型参数。</li>
</ol>
<p>以上便是CBOW模型的完整训练流程。</p>
<br>
<h3 id="Positional-Encoding">Positional Encoding</h3>
<p>词向量 → 含位置信息词向量，加入位置信息（多个正弦函数）到词向量</p>
<h4 id="Sinusoidal">Sinusoidal</h4>
<p>$$<br>
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})<br>
$$</p>
<p>$$<br>
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})<br>
$$</p>
<p>其中，$pos$ 表示该token在token序列中的位置，$i$ 表示 $d_{model}$ 中的第 $i$ 个维度</p>
<p>ref: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></p>
<p>核心：由于attention score的计算有特性</p>
<p>$f(…,x_m,…,x_n,…) = f(…,x_n,…,x_m,…)$</p>
<p>我们希望</p>
<p>$\tilde{f}(…,x_m,…,x_n,…) = f(…,x_m + p_m,…,x_n + p_n,…)$</p>
<p>移除可交换性/无位置信息性</p>
<p>同时拥有特性：对于 $\tilde{f}$ 的泰勒展开，有项系数 $p_m H p_n$ ，包含相对位置信息；即当假设 $H = I$ 时，有</p>
<p>$&lt;p_m, p_n&gt; = Re[p_mp_n^*]=q_{m-n}$</p>
<p>可求解得到</p>
<p>$p_m = (cos(m\theta), sin(m\theta))$</p>
<blockquote>
<p>sinusoidal通过<strong>加</strong>的方式向原始向量添加位置信息</p>
</blockquote>
<br>
<h4 id="RoPE">RoPE</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/8231">https://spaces.ac.cn/archives/8231</a></p>
<p><a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/8265">https://spaces.ac.cn/archives/8265</a></p>
<img src="/2025/03/19/LLM-Rela/image-20250830112148964.png" class="" title="image-20250830112148964">
<p>其中，$q_i$ 为向量 $q$ 的第 $i$ 维，$\theta_i = 10000^{-2i/d}$，$d$ 为向量 $q$ 的总维数</p>
<blockquote>
<p>rope通过<strong>乘</strong>的方式向原始向量添加位置信息</p>
<p>相当于对于每个 $i$, $i + 1$ 维度进行旋转</p>
<p>当 $i$ 为奇数时， $q_i = cos m \theta_i q_i - sin m \theta_i q_{i + 1}$；</p>
<p>当 $i$ 为偶数时， $q_i = sin m \theta_i q_{i-1} + cos m \theta_i q_{i}$；</p>
</blockquote>
<br>
<h3 id="Attention">Attention</h3>
<p>Transformer模型中隐藏层（hidden layer）的维度大小，也就是模型宽度（model width）</p>
<p>在Transformer架构中，这通常对应于：</p>
<ul>
<li>每个Transformer层的输入/输出维度（即隐藏状态的维度d_model）</li>
<li>注意力机制中Q/K/V向量的行维度（当使用标准实现时）</li>
<li>前馈网络层的输入/输出维度</li>
</ul>
<h4 id="注意力">注意力</h4>
<h5 id="自注意力-decoder">自注意力 - decoder</h5>
<img src="/2025/03/19/LLM-Rela/image-20250404192612286.png" class="" title="image-20250404192612286">
<img src="/2025/03/19/LLM-Rela/image-20250325195443397.png" class="" title="image-20250325195443397">
<p>ref: <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1XH4y1T76e/">从编解码和词嵌入开始，一步一步理解Transformer，注意力机制(Attention)的本质是卷积神经网络(CNN)</a><br>
$$<br>
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{\top}}{\sqrt{d}}\right)V<br>
$$<br>
$QK^T$ 相当于计算 $token_i$ 与 $token_j$ 对应词向量的相似度</p>
<img src="/2025/03/19/LLM-Rela/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9F%A9%E9%98%B5%E5%9B%BE.jpg" class="" title="img">
<p>softmax <strong>是按行（row-wise）进行的</strong></p>
<p>Attention是对每个token <strong>value化的词向量矩阵</strong> 进行相关性修正，得到注意力矩阵；</p>
<p>其中，每行<strong>每个修正后词向量</strong>包含其他相似词词向量的加和</p>
<blockquote>
<p>维度变化</p>
<p>假设 $W_Q$ 的维度为 ($d_{model}$, $d_k$)，输入形状为 (batch_size, $L_{actual}$, $d_{model}$)，这里的 $L_{actual}$ 为输入token序列长度，则 $QK^{\top}$ 的维度为 (batch_size, $L_{actual}$, $L_{actual}$)，V的维度为(batch_size, $L_{actual}$, $d_v$)</p>
</blockquote>
<br>
<h5 id="交叉注意力-encoder">交叉注意力 - encoder</h5>
<img src="/2025/03/19/LLM-Rela/image-20250325195459862.png" class="" title="image-20250325195459862">
<br>
<h4 id="多头注意力">多头注意力</h4>
<img src="/2025/03/19/LLM-Rela/multi-head-%E6%8B%BC%E6%8E%A5.jpg" class="" title="img">
<blockquote>
<p>将 $W_K, W_Q, W_V$ 竖着切分为 N 个，这意味着长度为  $L_{actual}$的token序列要与每个头进行注意力计算，得到 ($L_{actual}$, $d_h$) 的矩阵，然后拼接得到 ($L_{actual}$, $d_K$) 的矩阵， 进入(可选) 最终线性投影 $W_O$，size 为 $\mathbb{R}^{d_K \times d_{model}}$</p>
</blockquote>
<p>1.拆分</p>
<p>原始的单头注意力中，$W_K, W_Q, W_V \in \mathbb{R}^{d_{model} \times d_K}$<br>
在多头机制中，每个头的参数矩阵被水平拆分为更小的矩阵：</p>
<ul>
<li>
<p><strong>第 $h$ 个头的参数</strong>：</p>
<p>$W_K^{(h)}, W_Q^{(h)}, W_V^{(h)} \in \mathbb{R}^{d_{model} \times d_h}$，其中 $d_h = d_K/N_h$。例如，若总维度 $d_K=512$，头数 $N_h=8$，则每个头的维度 $d_h=64$。</p>
</li>
</ul>
<p><strong>拆分方式</strong>：</p>
<ul>
<li><strong>水平拆分</strong>：将原始矩阵按列切分（如 $W_K$ 被拆为 $[W_K^{(1)}, W_K^{(2)}, …, W_K^{(N_h)}]$），每个子矩阵对应一个头的参数。</li>
</ul>
<p>2.独立计算注意力</p>
<p>每个头 $h$ 使用自己的参数矩阵独立计算注意力：</p>
<ul>
<li>
<p><strong>输入 $x$ 通过第 $h$ 个头</strong>：</p>
<p>$K^{(h)} = x W_K^{(h)}$，</p>
<p>$Q^{(h)} = x W_Q^{(h)}$，</p>
<p>$V^{(h)} = x W_V^{(h)}$。</p>
<p>注意，当前步得到的 $K^{(h)}, Q^{(h)}, V^{(h)}$ 等价于直接从原始 $K, Q, V$ 中拆分</p>
</li>
<li>
<p><strong>计算注意力输出</strong>：<br>
$\text{Attn}_h(x) = \text{softmax}\left(\frac{Q^{(h)} K^{(h)\top}}{\sqrt{d_h}}\right) V^{(h)}$。</p>
</li>
</ul>
<blockquote>
<p>$K^{(h)}Q^{(h)}$ size 为 (L_actual, L_actual), $V^{(h)}$ size 为 (L_actual, d_h)</p>
</blockquote>
<p>3.整合</p>
<p>所有头的输出通过**拼接（Concatenate）**整合：</p>
<ul>
<li>
<p><strong>拼接（标准Transformer）</strong>：</p>
<p>$\text{MHA}(x) = Concat(\text{Attn}<em>1(x), \text{Attn}<em>2(x), …, \text{Attn}</em>{N_h}(x)) W_O$，其中 $W_O \in \mathbb{R}^{d</em>{K} \times d_{model}}$ 是输出投影矩阵。</p>
</li>
</ul>
<br>
<p>测试</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">Wq = Wk = Wv = np.array([</span><br><span class="line">    [1, 0, 0, 0],</span><br><span class="line">    [0, 2, 0, 0],</span><br><span class="line">    [0, 0, 3, 0],</span><br><span class="line">    [0, 0, 0, 4]</span><br><span class="line">]) </span><br><span class="line"></span><br><span class="line">输入 X:</span><br><span class="line"> [[1 0 1 0]</span><br><span class="line"> [0 2 0 2]]</span><br><span class="line"></span><br><span class="line">--- 单头 Attention ---</span><br><span class="line">QK^T / sqrt(dk):</span><br><span class="line"> [[ 5.  0.]</span><br><span class="line"> [ 0. 40.]]</span><br><span class="line">Attention Weights:</span><br><span class="line"> [[9.93307149e-01 6.69285092e-03]</span><br><span class="line"> [4.24835426e-18 1.00000000e+00]]</span><br><span class="line">Single-Head Output:</span><br><span class="line"> [[9.93307149e-01 2.67714037e-02 2.97992145e+00 5.35428074e-02]</span><br><span class="line"> [4.24835426e-18 4.00000000e+00 1.27450628e-17 8.00000000e+00]]</span><br><span class="line"></span><br><span class="line">--- 多头 Attention（Head 1）---</span><br><span class="line">Q1K1^T / sqrt(dk):</span><br><span class="line"> [[ 0.70710678  0.        ]</span><br><span class="line"> [ 0.         11.3137085 ]]</span><br><span class="line">Attention Weights Head 1:</span><br><span class="line"> [[6.69761549e-01 3.30238451e-01]</span><br><span class="line"> [1.22043184e-05 9.99987796e-01]]</span><br><span class="line">Head 1 Output:</span><br><span class="line"> [[6.69761549e-01 1.32095380e+00]</span><br><span class="line"> [1.22043184e-05 3.99995118e+00]]</span><br><span class="line"></span><br><span class="line">--- 多头 Attention（Head 2）---</span><br><span class="line">Q2K2^T / sqrt(dk):</span><br><span class="line"> [[ 6.36396103  0.        ]</span><br><span class="line"> [ 0.         45.254834  ]]</span><br><span class="line">Attention Weights Head 2:</span><br><span class="line"> [[9.98280432e-01 1.71956818e-03]</span><br><span class="line"> [2.21858114e-20 1.00000000e+00]]</span><br><span class="line">Head 2 Output:</span><br><span class="line"> [[2.99484130e+00 1.37565454e-02]</span><br><span class="line"> [6.65574341e-20 8.00000000e+00]]</span><br><span class="line"></span><br><span class="line">Multi-Head Output:</span><br><span class="line"> [[6.69761549e-01 1.32095380e+00 2.99484130e+00 1.37565454e-02]</span><br><span class="line"> [1.22043184e-05 3.99995118e+00 6.65574341e-20 8.00000000e+00]]</span><br></pre></td></tr></table></figure>
<br>
<h4 id="掩码注意力">掩码注意力</h4>
<img src="/2025/03/19/LLM-Rela/mask-attention-map.jpg" class="" title="img">
<p><strong>掩码（Masking）在计算注意力权重（Attention Weights）时生效，注意力权重是通过Q（Query）和K（Key）的点积计算得出的。</strong> Value（V）是根据这些权重加权求和的。严格来说，掩码不是直接作用在Q、K、V的原始数值上，而是作用在<strong>Q和K计算得到的注意力分数（logits）上</strong>，以此来决定哪些K不能被Q关注到。</p>
<blockquote>
<p>需要说明的是，掩码通过作用于 $QK^T$ ，使得位置靠前的token 在 $softmax(\frac{QK^T}{\sqrt{d}})$ 后，其注意力权重向量（行向量）从结果上来说后位均为0，因此得到的修正词向量（注意力矩阵的每行），在原token序列越靠前的，其词向量变化越小（<strong>融入的上文信息量</strong>越少），越靠后的，在注意力机制下变化越大（融入的上文信息量越大）</p>
</blockquote>
<br>
<h4 id="MHA、MQA、GQA-MLA">MHA、MQA、GQA&amp;MLA</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/21151178690">https://zhuanlan.zhihu.com/p/21151178690</a></p>
<img src="/2025/03/19/LLM-Rela/v2-b257d8660af7678f9c9bdc14d095b6d3_1440w.jpg" class="" title="img">
<p>MQA(<strong>M</strong>ulti-<strong>Q</strong>uery <strong>A</strong>ttention): 每个 head 的 Query 共享K和V矩阵，KV cache的内存占用降为 $\frac{1}{n}$</p>
<p>GQA(<strong>G</strong>rouped-<strong>Q</strong>uery <strong>A</strong>ttention): 每个 head 的 Query 按组区分，共享K和V矩阵，$g = 1$ 为MQA，$g = n$ 为MHA</p>
<blockquote>
<p>GQA 相对于 MHA</p>
<p>MHA是对 $W_Q$ $W_K$ $W_V $按列划分为 num_heads 个头，每个头的维度为 $d_{head}$</p>
<p>GQA  是对 $W_Q$ 按列划分为 num_heads 个头，$W_K$ $W_V$ 按列划分为 num_heads / g 个头，每个头的维度为 $d_{head}$，头的维度一致</p>
<p>（拼起来的话K头和V头大小小于Q头</p>
</blockquote>
<p>MLA(<strong>M</strong>ulti-head <strong>L</strong>atent <strong>A</strong>ttention):</p>
<p><img src="https://sebastianraschka.com/images/blog/2025/the-big-llm-architecture-comparison/4.png" alt="img"></p>
<br>
<h4 id="Flash-Attention">Flash Attention</h4>
<p>前置知识</p>
<img src="/2025/03/19/LLM-Rela/image-20250901152222060.png" class="" title="image-20250901152222060">
<br>
<p>对Q，K，V进行tiling</p>
<p>假设：</p>
<ul>
<li>序列长度 L=1024，hidden dim $d=64$。</li>
<li>tile size = 128</li>
</ul>
<hr>
<p><strong>Step 1: 划分 Q, K, V</strong></p>
<ul>
<li>初始化输出块 $O_i$ 为零矩阵：$O_i = 0$</li>
<li>把 Q 按 row 方向划分成多个 <strong>query tile</strong>：
<ul>
<li>$Q = [Q_1, Q_2, \ldots, Q_{L/128}]$，每块 $128 \times d$。</li>
</ul>
</li>
<li>把 K, V 按 row 方向划分成多个 <strong>key-value tile</strong>：
<ul>
<li>$K = [K_1, K_2, \ldots, K_{L/128}]$，每块 $128 \times d$。</li>
<li>$V = [V_1, V_2, \ldots, V_{L/128}]$，每块 $128 \times d$。</li>
</ul>
</li>
</ul>
<p><strong>Step 2: 分块计算 Attention</strong></p>
<ul>
<li>
<p>对于 Q 块 $Q_i$，需要和所有 K 块计算相似度：</p>
<ul>
<li>
<p>先加载 $Q_i$ 和 $K_j$ 到 <strong>GPU SRAM</strong>，计算局部 score：</p>
<p>$S_{ij} = Q_i K_j^\top$</p>
<ul>
<li>$S_{ij}$ 的大小是 $128 \times 128$。</li>
<li>存在寄存器 / shared memory 中，不落显存。</li>
</ul>
</li>
<li>
<p>同时计算 softmax 的分母（逐步做归一化，避免溢出，见 Step 3）。</p>
</li>
</ul>
</li>
</ul>
<hr>
<p><strong>Step 3: 分块 Softmax 的数值稳定性</strong></p>
<ul>
<li>
<p>对每个 tile 维护一个 <strong>局部最大值 $m_{i}$</strong> ， <strong>归一化因子（softmax分母） $l_{i}$</strong>，<strong>当前加权V值（softmax分子）</strong> $O_i$</p>
</li>
<li>
<p>更新规则：</p>
<p>$m_i = \text{row_max}(m_i, \text{row_max}(S_{ij}))$</p>
<p>$l_i = l_i \cdot e^{(m_{old}-m_{new})} + \text{row_sum}  (e^{S_{ij} - m_i})$</p>
<p>$P_{ij}=e^{S_{ij} - m_i}$</p>
</li>
<li>
<p>$O_i = O_i \cdot e^{m_{old}-m_{new}} + P_{ij}V_{i}$</p>
</li>
</ul>
<hr>
<p><strong>Step 4: 完整输出</strong></p>
<p>循环结束。$O_i$ 中存储的是加权 $V_j$ 的分子部分，$l_i$ 中是分母部分。</p>
<ul>
<li>遍历完所有 $K_j, V_j$ 块后，$Q_i$ 对应的 attention output $O_i$ 就计算完成。</li>
<li>丢弃临时中间量，继续下一个 $Q_{i+1}$。</li>
</ul>
<br>
<h3 id="Layer-Norm-Residual-Network">Layer Norm &amp; Residual Network</h3>
<p><strong>Layer Norm</strong></p>
<p>Batch Normalization 是对 <strong>所有样本的同一特征维度</strong> 分别做归一化（按列操作）</p>
<p>Layer Normalization 是对 <strong>单个样本的所有特征维度</strong> 做归一化（按行操作）</p>
<p>例如：BN是对特征 $i$ 进行归一，LN是对样本 $x_i$ 进行归一</p>
<table>
<thead>
<tr>
<th style="text-align:center">样本</th>
<th style="text-align:center">特征1</th>
<th style="text-align:center">特征2</th>
<th style="text-align:center">特征3</th>
<th style="text-align:center">特征4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>x₁</strong></td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">4.0</td>
</tr>
<tr>
<td style="text-align:center"><strong>x₂</strong></td>
<td style="text-align:center">5.0</td>
<td style="text-align:center">6.0</td>
<td style="text-align:center">7.0</td>
<td style="text-align:center">8.0</td>
</tr>
<tr>
<td style="text-align:center"><strong>x₃</strong></td>
<td style="text-align:center">9.0</td>
<td style="text-align:center">10.0</td>
<td style="text-align:center">11.0</td>
<td style="text-align:center">12.0</td>
</tr>
</tbody>
</table>
<br>
<p><strong>Residual Network</strong></p>
<p>防止层数过深导致的梯度消失</p>
<br>
<h3 id="MLP-FFN">MLP&amp;FFN</h3>
<p>在Transformer的每个编码器和解码器层中，MLP（也称为<strong>Feed Forward Network, FFN</strong>）用于对自注意力层的输出进行非线性变换和特征映射。</p>
<p>MLP (Multi-Layer Perceptron) 是 Transformer 编码器中每个自注意力层之后的一个前馈网络模块。这个 MLP 通常包含两个线性层，中间有一个非线性激活函数（如 GELU）</p>
<br>
$$
FFN(x)=Linear_{2}(Activation(Linear_{1}(x)))
$$
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForwardNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model: <span class="built_in">int</span>, d_ff: <span class="built_in">int</span>, activation=<span class="string">&quot;relu&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear1 = nn.Linear(d_model, d_ff)  <span class="comment"># 扩展层</span></span><br><span class="line">        self.linear2 = nn.Linear(d_ff, d_model)  <span class="comment"># 收缩层</span></span><br><span class="line">        self.activation = nn.ReLU() <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span> <span class="keyword">else</span> nn.GELU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># x shape: [batch_size, seq_len, d_model]</span></span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> x  <span class="comment"># 输出维度保持 [batch_size, seq_len, d_model]</span></span><br></pre></td></tr></table></figure>
<br>
<p><strong>关于 MLP size</strong></p>
<p>“MLP size” 指的是<strong>这两个线性层中间的隐藏维度</strong>。</p>
<ul>
<li><strong>在 ViT 中的体现:</strong>
<ul>
<li>结构: <code>Linear(D -&gt; MLP_Size) -&gt; GELU -&gt; Linear(MLP_Size -&gt; D)</code></li>
<li>它是一个“瓶颈”结构，将 $D$ 维的输入先扩展到一个更大的维度 <code>MLP_Size</code>，再压缩回 $D$ 维。</li>
</ul>
</li>
<li><strong>作用:</strong>
<ul>
<li><strong>增加非线性表达能力:</strong> MLP 层是 Transformer 编码器中引入非线性的主要方式，使得模型能够学习更复杂的函数关系。</li>
<li><strong>提供“思考空间”:</strong> 扩展到更大的维度（MLP Size）可以被认为是给模型更多的“思考空间”来处理特征。</li>
<li><strong>影响计算量和参数量:</strong> MLP size 越大，这部分的计算量和参数量也会越大。</li>
</ul>
</li>
<li><strong>与 Hidden Size 的关系:</strong> MLP size 通常是 Hidden size $D$ 的一个倍数，例如 <strong>4 倍</strong>。</li>
<li><strong>示例:</strong> 在 ViT-Base 模型中，如果 $D = 768$，那么 MLP size 通常是 $768 \times 4 = 3072$。</li>
</ul>
<br>
<h3 id="相关问题">相关问题</h3>
<ol>
<li>
<p>为什么Attenion公式中要除以 $\sqrt d$（d为Q, K矩阵的输出维度）？</p>
<ul>
<li>当向量维度变大的时候，d变大， q 和 k 的点积的方差变大</li>
<li>由于要对 q 和 k 的点积的每一行进行softmax，过大的方差将导致softmax极端化，得到类似于 $[1,0,0,…]$ 的one-hot分布</li>
<li>当输出接近one-hot时，非最大值的梯度趋近于0，反向传播时，这些位置的参数无法得到更新</li>
<li>因此，设置 softmax 的 temperature 来缓解这个问题，这里 temperature 被设置为了 $\sqrt d$ .</li>
</ul>
<p>如下图所示，假设随机向量 $X$ 满足均值为 0，协方差矩阵为单位矩阵（即各变量独立且方差为 1）的<strong>多元标准正态分布</strong>，可计算得到 $XY^T$ 满足均值为 0，协方差矩阵为 $D_{out} I$ 的<strong>多元正态分布</strong>，通过除以 $\sqrt d$ 将 $XY^T$ 的方差缩放为1</p>
</li>
</ol>
<img src="/2025/03/19/LLM-Rela/image-20250404100511037.png" class="" title="image-20250404100511037">
<ol start="2">
<li>
<p>为什么选择多头注意力？</p>
<p>有说法认为，克服**「模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置」<strong>，或者</strong>表达能力提升**：多个低秩注意力头（$d_h &lt; d$）的集成，能捕捉更复杂的交互模式；</p>
<p>但是仍有相悖观点认为并非如此</p>
<p>ref: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.10650">https://arxiv.org/pdf/1905.10650</a></p>
<img src="/2025/03/19/LLM-Rela/image-20250405101512560.png" class="" title="image-20250405101512560">
</li>
<li>
<p>Q，K，V为什么名为Query，Key，Value？注意力机制的注意力体现在哪里？</p>
<p>在交叉注意力场景中，</p>
<ol>
<li>
<p><strong>解码器的任务：</strong> 解码器的目标是根据编码器输出的源语句信息，并结合已经生成的词，来预测下一个词。</p>
</li>
<li>
<p><strong>编码器的输出：</strong> 编码器处理完源语句后，会输出一系列的向量表示。这些向量捕获了源语句中每个词的上下文信息。在 Transformer 的原始设计中，这些输出向量就是解码器交叉注意力的 K 和 V。</p>
</li>
<li>
<p><strong>Q 来源解码器：</strong> 解码器在生成第 t 个词时，它已经生成了 t-1 个词（或者说，它知道要生成哪个位置的词）。解码器使用它自己当前的输入（通常是前一个生成的词的嵌入，或者一个特殊的表示当前位置的向量）来生成 <strong>查询向量 Q</strong>。这个 Q 代表了“我（解码器）现在想知道什么？我需要哪些信息来生成下一个词？”</p>
</li>
<li>
<p><strong>K，V 来源于编码器：</strong> 编码器处理完整个源语句后，会输出一系列上下文向量。这些向量被用作 <strong>键向量 K</strong> 和 <strong>值向量 V</strong>。</p>
<ul>
<li><strong>K（键）</strong>：代表了源语句中每个词的“身份”或“特征”。当解码器的 Q 去查询时，它会与这些 K 进行匹配，以判断源语句中哪些词与当前的 Q 更相关。</li>
<li><strong>V（值）</strong>：包含了源语句中每个词的实际“内容”或“信息”。一旦 Q 和 K 确定了相关性，V 就会提供这些相关词的实际信息，供解码器使用。</li>
</ul>
</li>
<li>
<p><strong>交叉注意力的过程：</strong></p>
<ul>
<li>解码器的 Q（来自当前生成词的表示）与编码器的 K 矩阵进行点积，并通过 softmax 得到注意力权重。这些权重表明了当前解码器关注点在源语句中各个词上的分布。</li>
<li>将这些权重应用于编码器的 V 矩阵，进行加权求和，得到一个 <strong>上下文向量</strong>。这个上下文向量浓缩了源语句中对当前生成词最重要的信息。</li>
<li>解码器将这个上下文向量与自己的内部状态（例如，通过自注意力获得的已生成词的信息）结合起来，用于预测下一个词。</li>
</ul>
<p>“通过已经生成的词和源语句做自注意力，就是确定源语句中哪些词对接下来的词的生成更有作用”正是 <strong>交叉注意力</strong> 的功能。解码器的 Q（代表当前生成词的意图）去查询编码器的 K/V（源语句信息），找到源语句中最重要的部分。</p>
</li>
</ol>
<p>对比：</p>
<ul>
<li><strong>LSTM Seq2Seq 的问题：</strong>
<ul>
<li><strong>信息瓶颈（Information Bottleneck）</strong>：在传统的 LSTM Encoder-Decoder 架构中，编码器会将整个源序列压缩成一个固定长度的 <strong>上下文向量 C</strong>。无论源序列多长，所有信息都必须挤进这个 C。这导致长序列的信息丢失，尤其在解码器生成后半段序列时，C 中关于源序列前半段的信息可能已经非常稀释。</li>
<li><strong>“每一次生成词，都是通过 C 的全部信息去生成”</strong>：解码器在每一步都依赖于这个固定的 C。这使得模型难以动态地关注源序列中与当前生成词最相关的部分。</li>
<li><strong>“很多信息对于当前生成词而言都是没有意义的”</strong>：没错，对于生成某个特定词，源序列中可能只有一两个词是真正相关的。LSTM 的 C 却包含了所有信息，无法做到“按需提取”。</li>
</ul>
</li>
<li><strong>Transformer 注意力机制的解决方案：</strong>
<ul>
<li><strong>动态聚焦：</strong> 通过交叉注意力，解码器在生成每一个词时，都能动态地计算源序列中不同词的注意力权重。这意味着它能根据当前生成词的需要，<strong>“按需”地从编码器输出中提取最相关的信息</strong>。</li>
<li><strong>避免信息瓶颈：</strong> 编码器不再需要将所有信息压缩成一个单一向量。它输出的是一系列的上下文向量（每个对应源序列中的一个词），解码器可以随时通过注意力机制访问这些向量。</li>
<li><strong>长距离依赖：</strong> 注意力机制可以直接连接源序列中的任意两个词，无论它们相距多远，有助于捕捉长距离依赖关系，这在 LSTM 中很难实现。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>为什么encoder需要多个注意力层？</p>
<p>我们可以把自注意力（Self-Attention）看作是一次信息交互的过程。在一个注意力层中，每个token会“看”到序列中所有其他的token，并根据相关性（Attention Score）从它们那里“借”一些信息来更新自己。</p>
<ul>
<li><strong>一层注意力 = 一次直接交互</strong>：<br>
在句子 “The cat that chased the dog was tired” 中，第一层注意力可以让 “tired” 直接关联到 “cat”。它能建立起直接的语法联系。<br>
但是，“tired” 与 “dog” 的关系是<em>间接</em>的（“tired” -&gt; “cat” -&gt; “chased” -&gt; “dog”）。在一层注意力中，“tired” 看到 “dog” 主要是因为它们同在一个句子里，但它们之间的深层逻辑关系（猫因为追狗而累）还很模糊。</li>
<li><strong>多层注意力 = 多次间接交互</strong>：
<ul>
<li><strong>第一层</strong>：每个词都与所有其他词进行了直接的信息交换。现在，每个词的向量表示（embedding）已经包含了它直接邻居的信息。比如，“cat” 的新向量里包含了 “chased” 的信息。</li>
<li><strong>第二层</strong>：当第二层注意力开始工作时，它的输入是第一层处理过的、已经“混合”了初步上下文的序列。现在，当 “tired” 再次审视 “cat” 时，它看到的 “cat” 已经不是最初的那个了，这个 “cat” 的向量里已经带有了 “chased” 和 “dog” 的“影子”。</li>
<li><strong>以此类推</strong>：每一层都建立在前一层的基础上，信息可以像涟漪一样，通过中间token一跳一跳地传播到更远的地方。经过多层堆叠，一个token的表示就能够聚合到跨越整个序列的、非常复杂的间接依赖关系。</li>
</ul>
</li>
</ul>
</li>
</ol>
<br>
<h2 id="LLM-Train">LLM-Train</h2>
<h3 id="Base">Base</h3>
<img src="/2025/03/19/LLM-Rela/image-20250613213945162.png" class="" title="image-20250613213945162">
<p>在Qwen3中</p>
<p>预训练包含了</p>
<p><strong>通用阶段（S1）</strong>：第一预训练阶段中，所有Qwen3模型在超过30T token上进行训练，使用4,096个token的序列长度。此阶段模型已在语言能力和通用世界知识方面得到全面训练，训练数据覆盖119种语言和方言。</p>
<p><strong>推理阶段（S2）</strong>：为增强推理能力，研究团队优化了此阶段的预训练语料库，提高了STEM、编程、推理和合成数据的比例。模型在约5T高质量token上进行进一步预训练，序列长度保持为4K token。在此阶段还加速了学习率衰减过程。</p>
<p><strong>长上下文阶段（S3）</strong>：最终预训练阶段，研究人员收集了高质量长上下文语料库以扩展Qwen3模型的上下文处理长度。所有模型在数百亿token上进行预训练，序列长度达32K token。长上下文语料库中，75%的文本长度在16K~32K token之间，25%的文本长度在4K~16K token之间。</p>
<p>后训练包含了</p>
<img src="/2025/03/19/LLM-Rela/post-training.png" class="" title="img">
<h3 id="Parallelism">Parallelism</h3>
<p>ref: <a target="_blank" rel="noopener" href="https://x.com/_avichawla/status/1943558634388197458">https://x.com/_avichawla/status/1943558634388197458</a></p>
<img src="/2025/03/19/LLM-Rela/image-20250810092205813.png" class="" title="image-20250810092205813">
<p>在超大模型（GPT-3、LLAMA2/3、GPT-4 等）训练时，往往 <strong>TP + PP + DP（数据并行）</strong> 混合使用：</p>
<ul>
<li><strong>TP</strong>：解决单层太大、单卡算不动的问题。</li>
<li><strong>PP</strong>：解决层数太多、单卡放不下的问题。</li>
<li><strong>DP</strong>：解决 batch size 扩展、数据吞吐量的问题。</li>
</ul>
<p>比如 Megatron-LM 的 <strong>3D 并行</strong> 就是 TP + PP + DP 结合。</p>
<img src="/2025/03/19/LLM-Rela/0.png" class="" title="alt text">
<p>图中对比了不同数据并行策略的资源消耗（以N=64个GPU为例）：</p>
<p><strong>Baseline（纯数据并行）</strong></p>
<ul>
<li><strong>内存消耗</strong>：120GB（最高）
<ul>
<li>原因：每个GPU需存储完整的模型参数（蓝色）、梯度（橙色）和优化器状态（绿色），无任何分区优化。</li>
</ul>
</li>
<li><strong>通信量</strong>：1x（基准）
<ul>
<li>需同步所有GPU的梯度（通信量随GPU数量线性增长）。</li>
</ul>
</li>
</ul>
<p><strong>优化策略（$P_{os}、P_{os+g}、P_{os+g+p}$）</strong></p>
<ul>
<li><strong>$P_{os}$</strong>：仅对优化器状态分区
<ul>
<li>内存降至16.6GB（优化器状态分到不同GPU）。</li>
</ul>
</li>
<li><strong>$P_{os+g}$</strong>：优化器状态+梯度分区
<ul>
<li>内存进一步降低（梯度不再全存储）。</li>
</ul>
</li>
<li><strong>$P_{os+g+p}$</strong>：参数、梯度、优化器状态全分区
<ul>
<li>内存最低（1.9GB），但通信量增至1.5x（需额外同步参数）。</li>
</ul>
</li>
</ul>
<br>
<h4 id="DataParallel-DP">DataParallel(DP)</h4>
<blockquote>
<p>这不联邦学习吗</p>
</blockquote>
<p>数据并行 DataParallel (DP) - 相同的设置被复制多次，每次都输入一部分数据。处理是并行进行的，所有设置在每个训练步骤结束时同步。</p>
<ol>
<li><strong>模型复制</strong>：将<strong>相同的模型</strong>（包括参数、优化器状态等）复制到多个GPU上。</li>
<li><strong>数据分片</strong>：将训练数据<strong>划分为多个子批次（mini-batch）</strong>，每个GPU处理一个子批次。</li>
<li><strong>并行计算</strong>：所有GPU<strong>并行执行前向传播和反向传播</strong>，计算各自子批次的梯度。</li>
<li><strong>梯度同步</strong>：通过<strong>全局通信</strong>（如AllReduce）收集所有梯度并求平均，更新一次全局模型参数。</li>
</ol>
<br>
<h4 id="TensorParallel-TP">TensorParallel(TP)</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">https://arxiv.org/abs/1909.08053</a></p>
<img src="/2025/03/19/LLM-Rela/image-20250915155839854.png" class="" title="image-20250915155839854">
<p>每个张量被分成多个块，因此不是将整个张量驻留在单个 gpu 上，而是将张量的每个分片驻留在其指定的 gpu 上。在处理过程中，每个分片在不同的 GPU 上单独并行处理，结果在步骤结束时同步。这就是所谓的水平并行，因为拆分发生在水平层面。</p>
<p>在层内拆张量 → 适合单层参数量太大，计算需要分摊。</p>
<br>
<h4 id="PipelineParallel-PP">PipelineParallel(PP)</h4>
<p>将模型的不同层分布在不同 GPU 上，每张 GPU 负责模型的一部分，<strong>输入数据按 micro-batch 流水处理</strong>。</p>
<p>在层间切 stage → 适合层数太多，内存需要分摊。</p>
<br>
<h3 id="Framework">Framework</h3>
<h4 id="Megatron">Megatron</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.08053">https://arxiv.org/abs/1909.08053</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/Megatron-LM">https://github.com/NVIDIA/Megatron-LM</a></p>
<br>
<h4 id="Deepspeed">Deepspeed</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://github.com/deepspeedai/DeepSpeed">https://github.com/deepspeedai/DeepSpeed</a></p>
<br>
<h2 id="LLM-Inference">LLM-Inference</h2>
<h3 id="Parameters">Parameters</h3>
<h4 id="Temperature">Temperature</h4>
<p><strong>温度参数控制输出随机性（多样性）的超参数。</strong></p>
<p>将模型输出的 logits（原始分数）除以温度值，然后再经过 softmax，计算出新的概率分布：<br>
$$<br>
P_i = \frac{e^{\frac{logit_i}{T}}}{\sum_j e^{\frac{logit_j}{T}}}<br>
$$</p>
<ul>
<li><strong>T &lt; 1</strong> → 增强高概率词，削弱低概率词</li>
<li><strong>T &gt; 1</strong> → 扁平化分布，低概率词获得更多机会</li>
<li><strong>T = 1</strong> → 原始 softmax 分布</li>
</ul>
<blockquote>
<p>温度对模型输出的影响相当于改进版的softmax层</p>
</blockquote>
<br>
<h4 id="Sampling">Sampling</h4>
<p>Top-K</p>
<p>Top-K控制的是“只在前K个最有可能的词中采样”。</p>
<ul>
<li><strong>K=1</strong> → 只选概率最大的词（等同于贪婪搜索）</li>
<li><strong>K=10</strong> → 从概率前10的词中进行随机选择</li>
<li><strong>K=100+</strong> → 越大，越接近全概率分布，输出更有创造性</li>
</ul>
<img src="/2025/03/19/LLM-Rela/6.png" class="" title="alt text">
<br>
<p>Top-p</p>
<p>使用随机策略选择一个输出，候选集为按概率排名靠前的连续结果，且累积概率&lt;=p</p>
<img src="/2025/03/19/LLM-Rela/6.png" class="" title="alt text">
<br>
<h3 id="推理加速技术">推理加速技术</h3>
<h4 id="Speculative-Decoding">Speculative Decoding</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.01318">https://arxiv.org/abs/2302.01318</a></p>
<br>
<h4 id="KV-cache-Page-Attention">KV cache&amp;Page Attention</h4>
<br>
<h4 id="GQA-MLA">GQA &amp; MLA</h4>
<p>见Attention部分</p>
<br>
<h4 id="Sliding-Window-Attention">Sliding Window Attention</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html#31-sliding-window-attention">https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html#31-sliding-window-attention</a></p>
<img src="/2025/03/19/LLM-Rela/13.png" class="" title="img">
<br>
<h3 id="上下文扩展">上下文扩展</h3>
<h4 id="Yarn">Yarn</h4>
<p>扩展上下文界限 $L$ 到 $L’$  $(L’ &gt; L)$，相关定义如下：</p>
<p>比例因子 $s = \frac{L’}{L}$</p>
<p>波长 $\lambda_i = 2\pi B^{\frac{2i}{d_{model}}}$</p>
<p>频率 $\theta_i = B^{\frac{2i}{d_{model}}}$</p>
<p>位置 $pos’ = \frac{pos}{s}$</p>
<p>位置函数 $g(m)=\frac{m}{s}$</p>
<p>频率函数 $h(\theta) = \theta$</p>
<p><strong>Position Interpolation</strong></p>
<img src="/2025/03/19/LLM-Rela/image-20250901103525674.png" class="" title="image-20250901103525674">
<br>
<p><strong>NTK-aware</strong></p>
<p><strong>高频分量编码的是“相对位置”的精细信息，而低频分量编码的是“绝对位置”的粗略信息。直接缩放严重损害了前者，但对后者影响较小。</strong></p>
<p>我们用一个更具体的例子来解释：假设我们要将模型的上下文窗口从 <strong>2048</strong> 扩展到 <strong>8192</strong>，缩放因子为 <strong>s = 8192 / 2048 = 4</strong>。</p>
<p>在进行推理时，为了让模型能够处理新位置（如位置 4096），我们不能直接将位置 <code>m = 4096</code> 输入，因为模型从未见过这么大的位置。位置插值的做法是，将新的位置索引“压缩”回原来的范围。即，我们将实际位置 <code>m</code> 替换为 <code>m' = m / s</code>。</p>
<p>所以，RoPE 的旋转角度计算就从 <code>m * ω_i</code> 变成了 <code>(m/s) * ω_i</code>。</p>
<p>现在我们来分析这对高频和低频部分的不同影响。</p>
<hr>
<p>高频部分（大 ω）：信息严重压缩和丢失</p>
<p>高频分量的设计初衷是让相邻 token 之间有显著的差异，从而让模型精确地感知到“就在旁边”这个概念。</p>
<ul>
<li>
<p><strong>原始情况 (未缩放)</strong>:</p>
<ul>
<li>假设最高频 <code>ω_0 = 1</code>。</li>
<li>位置 <code>m=0</code> 和 <code>m=1</code> 的旋转角度差是 <code>(1 * 1) - (0 * 1) = 1</code> 弧度（约 57.3°）。这是一个非常大的、清晰可辨的差异。</li>
<li>位置 <code>m=1</code> 和 <code>m=2</code> 的角度差同样是 <code>1</code> 弧度。</li>
<li>模型在训练中学会了：当这个维度的嵌入向量旋转了约 57.3° 时，就意味着 token 的相对位置移动了 1。</li>
</ul>
</li>
<li>
<p><strong>缩放后 (s=4)</strong>:</p>
<ul>
<li>现在我们看真实位置 <code>m=0</code> 和 <code>m=1</code>。它们被映射到插值位置 <code>m'=0</code> 和 <code>m'=1/4=0.25</code>。</li>
<li>它们之间的旋转角度差变成了 <code>(0.25 * 1) - (0 * 1) = 0.25</code> 弧度（约 14.3°）。</li>
<li>再看真实位置 <code>m=0, 1, 2, 3</code>，它们被映射到 <code>m'=0, 0.25, 0.5, 0.75</code>。这些位置在原始模型看来，都挤在了原来 <code>m=0</code> 和 <code>m=1</code> 的区间内！</li>
<li><strong>结论</strong>: 原本用于区分相邻 token 的清晰“刻度”被严重模糊了。模型看到一个微小的角度变化（比如 14.3°），它无法判断这究竟是原始训练数据中一个 token 内的细微抖动，还是一个全新的、代表“下一个”token 的位置。这种区分能力的丧失，就是您所说的“高频分量消失”，导致模型对局部、短距离的依赖关系变得不敏感。</li>
</ul>
</li>
</ul>
<hr>
<p>低频部分（小 ω）：信息保留较好</p>
<p>低频分量的作用是提供一个缓慢变化的信号，来标识 token 在长序列中的大致位置。它对微小的位置变化不敏感。</p>
<ul>
<li>
<p><strong>原始情况 (未缩放)</strong>:</p>
<ul>
<li>假设一个很低的频率 <code>ω_i = 0.001</code>。</li>
<li>位置 <code>m=1000</code> 和 <code>m=1001</code> 的旋转角度差是 <code>(1001 * 0.001) - (1000 * 0.001) = 0.001</code> 弧度。这个差异本身就微乎其微。</li>
<li>这个分量的主要作用是看长距离差异。比如，位置 <code>m=0</code> 和 <code>m=1000</code> 的角度差是 <code>1000 * 0.001 = 1</code> 弧度，这是一个很大的差异，清晰地表明了两个 token 相距很远。</li>
</ul>
</li>
<li>
<p><strong>缩放后 (s=4)</strong>:</p>
<ul>
<li>我们看真实位置 <code>m=1000</code> 和 <code>m=1001</code>。它们被映射到插值位置 <code>m'=1000/4=250</code> 和 <code>m'=1001/4=250.25</code>。</li>
<li>旋转角度差为 <code>(250.25 * 0.001) - (250 * 0.001) = 0.00025</code> 弧度。这个差异比原来更小了，但本来就已经很小，所以影响不大。</li>
<li>我们再看长距离。真实位置 <code>m=0</code> 和 <code>m=4000</code>。它们被映射到 <code>m'=0</code> 和 <code>m'=1000</code>。</li>
<li>它们之间的旋转角度差是 <code>(1000 * 0.001) - (0 * 0.001) = 1</code> 弧度。</li>
<li><strong>结论</strong>: 在缩放后，真实位置 4000 处的低频分量，其旋转状态和原始模型在位置 1000 处的状态是<strong>完全一样</strong>的。由于低频信号本身就是缓慢、近乎线性变化的，模型可以很好地进行“线性外推”（Linear Extrapolation）。它看到一个缓慢变化的信号持续了更长时间，这符合它在训练期间学到的规律。因此，长距离的绝对位置信息得以保留。</li>
</ul>
</li>
</ul>
<p>正是因为这种对高频信息的破坏，后续的上下文扩展方法，如 <strong>NTK-aware Scaling</strong>，才会提出修改基数 <code>θ</code> 而不是直接插值位置 <code>m</code>，以求在扩展上下文的同时，尽可能减小对高频信息的损害。</p>
<p>改变频率 $\theta_i = B’^{\frac{2i}{d_{model}}}$</p>
<p>where $B’ = B*s^{\frac{|d_{model}|}{|d_{model}| - 2}}$</p>
<p><strong>NTK-by-parts</strong></p>
<p>ref: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/683863159">https://zhuanlan.zhihu.com/p/683863159</a></p>
<img src="/2025/03/19/LLM-Rela/image-20250901102212626.png" class="" title="image-20250901102212626">
<img src="/2025/03/19/LLM-Rela/image-20250901103305314.png" class="" title="image-20250901103305314">
<blockquote>
<p>显然，</p>
<p>对于低维/高频/短波部分，基频不发生改变</p>
<p>对于高维/低频/长波部分，基频缩小/对应位置的旋转角加快，使其在长波长场景下的位置变化可区分度提升</p>
</blockquote>
<p><strong>Yarn</strong></p>
<img src="/2025/03/19/LLM-Rela/image-20250901103056683.png" class="" title="image-20250901103056683">
<br>
<h4 id="DCA">DCA</h4>
<br>
<h3 id="Framework-2">Framework</h3>
<h4 id="vllm">vllm</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a></p>
<br>
<h4 id="SGLang">SGLang</h4>
<p>ref:</p>
<br>
<h2 id="Fine-Tuning">Fine-Tuning</h2>
<p>高效微调技术分类：</p>
<ul>
<li>增加额外参数（A）
<ul>
<li>类适配器（Adapter-like）方法</li>
<li>软提示（Soft prompts）</li>
</ul>
</li>
<li>选取一部分参数更新（S）</li>
<li>引入重参数化（R）</li>
</ul>
<img src="/2025/03/19/LLM-Rela/v2-eaaf1c00d0c4ea350cd3a79b47de26d3_1440w.jpg" class="" title="img">
<br>
<h3 id="BitFit-Prefix-Tuning-Prompt-Tuning">BitFit, Prefix Tuning &amp; Prompt Tuning</h3>
<p>BitFit（论文：<strong>BitFit: Simple Parameter-efficient Fine-tuning or Transformer-based Masked Language-models</strong>）是一种稀疏的微调方法，它训练时只更新bias的参数或者部分bias参数。</p>
<p>涉及到的bias参数有attention模块中计算query,key,value跟合并多个attention结果时涉及到的bias，MLP层中的bias，Layernormalization层的bias参数。</p>
<br>
<p>Prefix Tuning（论文：<strong>Prefix-Tuning: Optimizing Continuous Prompts for Generation</strong>），在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而PLM(Pretrain LM)中的其他部分参数固定。</p>
<br>
<p>Prompt Tuning（论文：<strong>The Power of Scale for Parameter-Efficient Prompt Tuning</strong>），该方法可以看作是Prefix Tuning的简化版本，它给每个任务定义了自己的Prompt，然后拼接到数据上作为输入，但<strong>只在输入层加入prompt tokens</strong>，并且不需要加入 MLP 进行调整来解决难训练的问题。</p>
<br>
<h3 id="P-Tuning">P-Tuning</h3>
<p>P-Tuning（论文：<strong>GPT Understands, Too</strong>），该方法将Prompt转换为可以学习的Embedding层，并用MLP+LSTM的方式来对Prompt Embedding进行一层处理。</p>
<p>相比Prefix Tuning，P-Tuning加入的可微的virtual token，但仅限于输入层，没有在每一层都加；另外，virtual token的位置也不一定是前缀，插入的位置是可选的。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual token。</p>
<p>P-Tuning v2（论文： <strong>P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</strong>），该方法在每一层都加入了Prompts tokens作为输入，而不是仅仅加在输入层</p>
<br>
<h3 id="Adapter-Tuning">Adapter Tuning</h3>
<p>Adapter Tuning（论文：<strong>Parameter-Efficient Transfer Learning for NLP</strong>），该方法设计了Adapter结构，并将其嵌入Transformer的结构里面，针对每一个Transformer层，增加了两个Adapter结构(分别是多头注意力的投影之后和第二个feed-forward层之后)，在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构和 Layer Norm 层进行微调，从而保证了训练的高效性。</p>
<p>Adapter Fusion（论文：<strong>AdapterFusion:Non-Destructive Task Composition for Transfer Learning</strong>），一种融合多任务信息的Adapter的变体，在 Adapter 的基础上进行优化，通过将学习过程分为两阶段来提升下游任务表现。</p>
<p>AdapterDrop（论文：AdapterDrop: On the Efficiency of Adapters in Transformers），在不影响任务性能的情况下，对Adapter动态高效的移除，尽可能的减少模型的参数量，提高模型在反向传播（训练）和正向传播（推理）时的效率。</p>
<br>
<h3 id="LoRA">LoRA</h3>
<p>ref: <a target="_blank" rel="noopener" href="https://x.com/DailyDoseOfDS_/status/1942878914889826686">https://x.com/DailyDoseOfDS_/status/1942878914889826686</a></p>
<img src="/2025/03/19/LLM-Rela/image-20250810092440215.png" class="" title="image-20250810092440215">
<p>$$<br>
h = W_0 x + (\frac{\alpha}{r}) B A x<br>
$$<br>
其中</p>
<ul>
<li>$W_0$ 表示被冻结的原始权重矩阵</li>
<li>$x$ 为模块输入</li>
<li>$A$ $B$ 为低秩可训矩阵，秩为 $r$</li>
<li>$\alpha$ 为缩放因子</li>
</ul>
<br>
<h4 id="奇异值分解与低秩分解">奇异值分解与低秩分解</h4>
<p><strong>SVD</strong></p>
<p>对于任意一个 $m \times n$ 的实矩阵 $A$，可以分解成三个矩阵的乘积：<br>
$$<br>
A = U \Sigma V^T<br>
$$</p>
<ul>
<li>$U$：$m \times m$ 的正交矩阵（左奇异向量）</li>
<li>$\Sigma$：$m \times n$ 的对角矩阵，对角线上的值是奇异值（非负，按大小排列）</li>
<li>$V^T$：$n \times n$ 的正交矩阵（右奇异向量的转置）</li>
</ul>
<br>
<p><strong>低秩分解</strong></p>
<p>r是矩阵的秩，决定了分解后保留的信息量。如果只保留最大的几个奇异值（低秩近似），就能用更少的参数近似原矩阵</p>
<p>例如，存在矩阵<br>
$$<br>
S = \begin{bmatrix}<br>
1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \<br>
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br>
0 &amp; 3 &amp; 0 &amp; 0 &amp; 0 \<br>
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \<br>
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br>
\end{bmatrix}<br>
$$<br>
分解后的三个矩阵：<br>
$$<br>
U \approx \begin{bmatrix}<br>
0.3 &amp; 0 &amp; 0.34 &amp; -0.68 &amp; -0.58 \<br>
-0.22 &amp; 0 &amp; -0.76 &amp; 0.2 &amp; -0.58 \<br>
0 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \<br>
-0.77 &amp; 0 &amp; 0.36 &amp; 0.52 &amp; 0 \<br>
-0.52 &amp; 0 &amp; -0.42 &amp; -0.48 &amp; -0.58 \<br>
\end{bmatrix}, \quad<br>
\Sigma = \begin{bmatrix}<br>
7.03 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br>
0 &amp; 3 &amp; 0 &amp; 0 &amp; 0 \<br>
0 &amp; 0 &amp; 2.15 &amp; 0 &amp; 0 \<br>
0 &amp; 0 &amp; 0 &amp; 0.11 &amp; 0 \<br>
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \<br>
\end{bmatrix}, \quad<br>
V \approx \begin{bmatrix}<br>
0.34 &amp; -0.32 &amp; 0 &amp; -0.89 &amp; 0 \<br>
0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 \<br>
0.3 &amp; -0.93 &amp; 0 &amp; 0.22 &amp; 0 \<br>
-0.89 &amp; -0.19 &amp; 0 &amp; 0.41 &amp; 0 \<br>
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \<br>
\end{bmatrix}<br>
$$<br>
选择最大的三个奇异值重构，保留 $\sigma_{1} \approx 7.03$, $\sigma_{2} = 3$, $\sigma_{3}=2.15$，重构矩阵如下：</p>
<p>保留前三列：</p>
<p>$$<br>
U_{\text{trunc}} \approx<br>
\begin{bmatrix}<br>
0.3 &amp; 0 &amp; 0.34 \<br>
-0.22 &amp; 0 &amp; -0.76 \<br>
0 &amp; -1 &amp; 0 \<br>
-0.77 &amp; 0 &amp; 0.36 \<br>
-0.52 &amp; 0 &amp; -0.428<br>
\end{bmatrix}<br>
$$<br>
保留前三行和前三列：</p>
<p>$$<br>
\Sigma_{\text{trunc}} =<br>
\begin{bmatrix}<br>
7.03 &amp; 0 &amp; 0 \<br>
0 &amp; 3 &amp; 0 \<br>
0 &amp; 0 &amp; 2.15<br>
\end{bmatrix}<br>
$$<br>
保留前三行：</p>
<p>$$<br>
V_{\text{trunc}} \approx<br>
\begin{bmatrix}<br>
0.34 &amp; -0.32 &amp; 0 &amp; -0.89 &amp; 0 \<br>
0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 \<br>
0.3 &amp; -0.93 &amp; 0 &amp; 0.22 &amp; 0<br>
\end{bmatrix}<br>
$$<br>
根据，$S’ = U_{\text{trunc}} \times \Sigma_{\text{trunc}} \times V_{\text{trunc}}^T$，计算得到重构后：</p>
<p>$$<br>
S’ =<br>
\begin{bmatrix}<br>
0.93 &amp; -0.01 &amp; 0 &amp; 2.03 &amp; 0 \<br>
0.02 &amp; 2 &amp; 0 &amp; 0.99 &amp; 0 \<br>
0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 \<br>
2.05 &amp; 1.01 &amp; 0 &amp; 4.98 &amp; 0 \<br>
0.95 &amp; 1.99 &amp; 0 &amp; 3.02 &amp; 0<br>
\end{bmatrix}<br>
$$<br>
结果对比原始矩阵和重构矩阵，直观地看，基本保持一致。</p>
<p>事实上上面的结论：如果只保留最大的几个奇异值（低秩近似），就能用更少的参数近似 $W$。<br>
$$<br>
S =<br>
\begin{bmatrix}<br>
1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \<br>
0 &amp; 2 &amp; 0 &amp; 1 &amp; 0 \<br>
0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 \<br>
2 &amp; 1 &amp; 0 &amp; 5 &amp; 0 \<br>
1 &amp; 2 &amp; 0 &amp; 3 &amp; 0<br>
\end{bmatrix}<br>
\quad<br>
S’ =<br>
\begin{bmatrix}<br>
0.93 &amp; -0.01 &amp; 0 &amp; 2.03 &amp; 0 \<br>
0.02 &amp; 2 &amp; 0 &amp; 0.99 &amp; 0 \<br>
0 &amp; 0 &amp; 3 &amp; 0 &amp; 0 \<br>
2.05 &amp; 1.01 &amp; 0 &amp; 4.98 &amp; 0 \<br>
0.95 &amp; 1.99 &amp; 0 &amp; 3.02 &amp; 0<br>
\end{bmatrix}<br>
$$<br>
实际上，可以通过保留的奇异值，计算重构后的矩阵，保留了多少信息，如下：</p>
<p>$$<br>
|A|_F^2 = 7.03^2 + 3^2 + 2.15^2 + 0.11^2 + 0^2 \approx 63.06<br>
\quad<br>
|A’|_F^2 = 7.03^2 + 3^2 + 2.15^2 \approx 63.04<br>
$$</p>
<p>$$<br>
\text{信息保留比例} = \frac{63.04}{63.06} \approx 99.97%<br>
$$</p>
<blockquote>
<p>为什么LoRA可以进行低秩分解？</p>
<p>通过对微调后的权重变化 $\Delta W$ 的奇异值分解发现，大部分信息集中在少数几个奇异值上；在GPT-3上测试时发现，$\Delta W$ 的前 10 - 20 个奇异值占据了 90% 的信息</p>
<p>可以对原始权重 $W$ 进行分解吗？</p>
<p>不可以，$W$ 接近满秩</p>
</blockquote>
<p>假设对一个 $512 \times 512$ 的权重矩阵 $W$ 进行微调</p>
<ul>
<li>全微调：可能需要调整 262144 个参数</li>
<li>LoRA：假设 r = 8，只需要调整 $A(512 \times 8)$ 和 $B(8 \times 512)$，共 8192 个参数</li>
</ul>
<br>
<h4 id="LoRA应用位置">LoRA应用位置</h4>
<p><strong>注意力层</strong></p>
<p>多应用与 $W_q$ 和 $W_v$ 上</p>
<p><strong>FFN层</strong></p>
<p>$W_1$ (升维)和 $W_2$ (降维)</p>
<br>
<h4 id="LoRA改进">LoRA改进</h4>
<h5 id="LoRA-2">LoRA+</h5>
<p><strong>核心思想</strong>：对低秩矩阵 $A$ 和 $B$ 设置不同的学习率，以增强训练动态性。</p>
<p>在标准 LoRA 中，权重更新为：</p>
<p>$$<br>
\Delta W = A B, \quad A \in \mathbb{R}^{d \times r}, ; B \in \mathbb{R}^{r \times d}<br>
$$</p>
<p>LoRA+ 设置独立的学习率：</p>
<p>$$<br>
A \leftarrow A - \eta_A \cdot \nabla_A \mathcal{L}, \quad B \leftarrow B - \eta_B \cdot \nabla_B \mathcal{L}<br>
$$</p>
<p>其中：</p>
<ul>
<li>$\eta_A$：A 的学习率</li>
<li>$\eta_B$：B 的学习率</li>
<li>通常设置 $\eta_B = \lambda \cdot \eta_A$，$\lambda \in [4, 16]$</li>
</ul>
<br>
<h5 id="DoRA">DoRA</h5>
<p><strong>核心思想</strong>：LoRA的更新 $\Delta W$ 会同时改变原始权重 $W_0$ 的<strong>幅度</strong>（magnitude）和<strong>方向</strong>（direction）。DoRA认为，更有效的微调应该主要集中在改变权重的“方向”上，而其“幅度”应保持相对稳定或独立调整。因此，DoRA将权重矩阵 $W$ 分解为幅度和方向两个部分，然后只用LoRA来微调方向。</p>
<p><strong>数学公式</strong>：</p>
<ol>
<li>
<p><strong>分解</strong>：将原始权重 $W_0$ 分解为幅度和方向向量。<br>
$$ W_0 = m \cdot \frac{V}{||V||_F} $$</p>
<ul>
<li>$m = ||W_0||_F$ 是矩阵的弗罗贝尼乌斯范数，代表<strong>幅度</strong>（一个标量）。</li>
<li>$V$ 是方向矩阵，初始时 $V=W_0$。$\frac{V}{||V||_F}$ 是归一化的方向。</li>
</ul>
</li>
<li>
<p><strong>微调</strong>：使用LoRA更新方向矩阵 $V$，而不是直接更新 $W_0$。<br>
$$ V_{trained} = V + \Delta V = V + B A $$</p>
</li>
<li>
<p><strong>重构</strong>：用学习到的新方向 $V_{trained}$ 和<strong>可训练的</strong>幅度 $m$ 来重构最终的权重。<br>
$$ W_{new} = m \cdot \frac{V_{trained}}{||V_{trained}||_F} = m \cdot \frac{W_0 + BA}{||W_0 + BA||_F} $$<br>
这里的幅度 $m$ 变成了一个可训练的向量（每个输出通道一个），使得模型可以独立学习幅度的变化。</p>
</li>
</ol>
<br>
<h5 id="rsLoRA">rsLoRA</h5>
<p><strong>核心思想</strong>：这是一个非常简单但有效的技巧。标准的LoRA中，缩放因子 $\alpha$ 是一个固定的超参数。但研究发现，当秩 $r$ 变化时，$\Delta W = BA$ 的期望大小（方差）会随着 $r$ 的变化而变化。具体来说，它大致与 $\sqrt{r}$ 成正比。这导致在调整 $r$ 时，需要同时精调 $\alpha$ 才能获得最佳效果，非常麻烦。rsLoRA通过动态调整 $\alpha$ 来解决这个问题。</p>
<p><strong>数学公式</strong>：<br>
标准的LoRA：<br>
$$ W_{new} = W_0 + \alpha \cdot (B A) $$<br>
rsLoRA (Rank-Stabilized LoRA)：<br>
$$ W_{new} = W_0 + \frac{\alpha}{\sqrt{r}} \cdot (B A) $$<br>
就是把固定的 $\alpha$ 替换为 $\frac{\alpha}{\sqrt{r}}$。</p>
<br>
<h5 id="PiSSA">PiSSA</h5>
<p><strong>核心思想</strong>：传统的LoRA中，矩阵 $A$ 和 $B$ 是随机初始化的（A通常是高斯分布，B是0）。这意味着在训练开始时，$\Delta W=BA=0$，LoRA没有利用任何关于预训练权重 $W_0$ 的先验知识。PiSSA认为， $W_0$ 中最重要的信息包含在其最大的奇异值和对应的奇异向量中。因此，PiSSA利用这些信息来<strong>初始化</strong>LoRA矩阵，而不是从零开始学习。</p>
<p><strong>数学公式</strong>：</p>
<p><strong>SVD分解</strong>：对原始权重 $W_0$ 进行奇异值分解 (SVD)。<br>
$$ W_0 = U \Sigma V^T $$</p>
<ul>
<li>$U$ 和 $V$ 是正交矩阵，$\Sigma$ 是对角矩阵，对角线上的值是奇异值 $\sigma_i$。</li>
</ul>
<p><strong>权重重参数化</strong>：将 $W_0$ 分为两部分：最重要的“主成分”和“残差”。</p>
<ul>
<li>取最大的 $r$ 个奇异值和对应的向量，构成 $W_0$ 的低秩近似：$W_{principal} = U_r \Sigma_r V_r^T$。</li>
<li>残差部分：$W_{residual} = W_0 - W_{principal}$。</li>
</ul>
<p><strong>PiSSA的实现</strong>：</p>
<ul>
<li>
<p>将 $W_{residual}$ <strong>冻结</strong>。</p>
</li>
<li>
<p>将LoRA的 $A$ 和 $B$ 矩阵<strong>初始化</strong>为 $W_{principal}$ 的分解形式。<br>
$$ B_{init} = U_r \sqrt{\Sigma_r} $$<br>
$$ A_{init} = \sqrt{\Sigma_r} V_r^T $$<br>
这样 $B_{init} A_{init} = U_r \Sigma_r V_r^T = W_{principal}$。</p>
</li>
<li>
<p>训练时，模型学习的是对这个最优初始化的一个微小调整。我们引入新的、初始化为零的LoRA矩阵 $A’$ 和 $B’$，训练目标变为：<br>
$$ W_{new} = W_{residual} + (B_{init} + B’) (A_{init} + A’) $$</p>
</li>
<li>
<p>因为 $A’, B’$ 很小，所以训练开始时 $W_{new} \approx W_{residual} + B_{init}A_{init} = W_0$。模型从一个完美的起点开始微调。</p>
</li>
</ul>
<br>
<h4 id="LoRA相关论文">LoRA相关论文</h4>
<p>LoRA（论文：<strong>LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</strong>），该方法的核心思想就是通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。</p>
<br>
<p>AdaLoRA（论文：<strong>ADAPTIVE BUDGET ALLOCATION FOR PARAMETEREFFICIENT FINE-TUNING</strong>），是对LoRA的一种改进，它根据重要性评分动态分配参数预算给权重矩阵。</p>
<br>
<p>QLoRA（论文： <strong>QLORA: Efficient Finetuning of Quantized LLMs</strong>），使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。QLORA 有一种低精度存储数据类型（4 bit），还有一种计算数据类型（BFloat16）。实际上，这意味着无论何时使用 QLoRA 权重张量，我们都会将张量反量化为 BFloat16，然后执行 16 位矩阵乘法。QLoRA提出了两种技术实现高保真 4 bit微调——4 bit NormalFloat(NF4) 量化和双量化。此外，还引入了分页优化器，以防止梯度检查点期间的内存峰值，从而导致内存不足的错误，这些错误在过去使得大型模型难以在单台机器上进行微调。</p>
<br>
<h4 id="调参技巧">调参技巧</h4>
<p>通过固定 $\alpha$ 后，调整 rank（原始LoRA论文实现）； $\alpha$ 默认为8</p>
<ul>
<li>从低秩开始：对于绝大多数任务，可以从 $r=8$ 和 $r = 16$ 开始调整，评估性能后再决定是否需要更高的秩</li>
<li>数据集大小与秩的关系：小数据集（&lt;5k 样本）用低秩（$r=8$）；大数据集（&gt;50k样本）可以尝试更大秩（r=32+）</li>
<li>复杂任务策略：对于复杂推理任务，可以结合使用：（1）增大r到32或64；（2）启用rsLoRA；（3）添加更多目标层；</li>
</ul>
<img src="/2025/03/19/LLM-Rela/image-20250929141016096.png" class="" title="image-20250929141016096">
<img src="/2025/03/19/LLM-Rela/image-20250929141029131.png" class="" title="image-20250929141029131">
<br>
<h3 id="MAM-Adapter-UniPELT">MAM Adapter &amp; UniPELT</h3>
<p>MAM Adapter（论文：TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING），一个在Adapter、Prefix Tuning和LoRA之间建立联系的统一方法。</p>
<br>
<p>UniPELT（论文： UNIPELT: A Unified Framework for Parameter-Efficient Language Model Tuning）是 LoRA、Prefix Tuning和Adapter的门控组合。</p>
<br>
<h2 id="Reinforce-Learning-on-LLM">Reinforce Learning on LLM</h2>
<h3 id="Base-2">Base</h3>
<p>详见 Reinforce Learning Record</p>
<h4 id="Rollout-Sample">Rollout &amp; Sample</h4>
<p>在强化学习（RL）里：</p>
<ul>
<li>
<p><strong>rollout = 用当前策略 π 与环境交互，得到一条完整的状态—动作—奖励序列（trajectory）</strong>。</p>
</li>
<li>
<p>形式上就是：</p>
<p>$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots, s_T)$</p>
</li>
<li>
<p>这个过程通常会跑到 episode 结束（或设定的最大步数）。</p>
</li>
</ul>
<hr>
<p>🔹 和“采样”的关系</p>
<ul>
<li>在 RL 里，策略 π(a|s) 本身是一个<strong>概率分布</strong>，所以在某个状态 s 下选择动作 a 就是一次 <strong>采样</strong>。</li>
<li><strong>rollout = 多次采样动作 + 环境状态转移的累计结果</strong>。</li>
<li>换句话说：
<ul>
<li>“采样”是<strong>一步</strong>（从分布里抽一个动作）；</li>
<li>“rollout”是<strong>一条轨迹</strong>（把一系列采样拼在一起）。</li>
</ul>
</li>
</ul>
<h3 id="LLM-DPO">LLM-DPO</h3>
<p>Direct Preference Optimization</p>
<p>DPO的核心思想是：<strong>跳过显式奖励建模和复杂的强化学习步骤，直接通过一个简单的分类损失函数来优化语言模型，使其符合人类偏好。</strong></p>
<p>它将“让模型生成高奖励的回答”这个目标，等价地转换为了“直接增大模型对‘更优回答’的生成概率，同时减小对‘较差回答’的生成概率”。</p>
<p>整个训练流程比PPO更简洁，通常只有两步：</p>
<ol>
<li><strong>监督式微调（SFT）：</strong> 与PPO的第一步完全相同。训练一个基础模型来理解指令和生成基本回答。</li>
<li><strong>直接偏好优化（DPO）：</strong> 这一步直接取代了PPO流程中的“奖励模型训练”和“PPO强化学习”两个阶段。</li>
</ol>
<br>
<p>在DPO中，不需要Critic模型，不需要在线采样生成数据。使用<strong>静态的偏好数据集</strong>。</p>
<p><strong>参与者</strong></p>
<p>在DPO训练开始前，我们只需要两个模型：</p>
<ol>
<li>
<p><strong>策略模型（Policy Model, <code>π_θ</code>）：</strong></p>
<ul>
<li><strong>角色:</strong> 主角，即我们正在微调的语言模型。</li>
<li><strong>来源:</strong> 经过第一步SFT训练后的模型副本。</li>
<li><strong>任务:</strong> 在DPO训练中，它的参数<code>θ</code>会被更新。</li>
</ul>
</li>
<li>
<p><strong>参考模型（Reference Model, <code>π_ref</code>）：</strong></p>
<ul>
<li><strong>角色:</strong> “锚点”或“基准”。</li>
<li><strong>来源:</strong> 同样是第一步SFT模型的<strong>一个固定、不更新的副本</strong>。</li>
<li><strong>任务:</strong> 提供一个基准概率。它的作用与PPO中的参考模型完全相同：防止策略模型<code>π_θ</code>为了迎合偏好数据而偏离其原始的语言能力太远，这是一种隐式的KL散度约束。</li>
</ul>
</li>
</ol>
<p><strong>数据集</strong></p>
<p>DPO使用的数据集格式非常关键。它不是单个的“好回答”，而是一个偏好对的集合。每一条数据包含：</p>
<ul>
<li><strong>提示（Prompt, <code>x</code>）</strong></li>
<li><strong>更优的回答（Chosen Response, <code>y_w</code>）</strong></li>
<li><strong>较差的回答（Rejected Response, <code>y_l</code>）</strong></li>
</ul>
<p>这个数据集 <code>D = &#123; (x, y_w, y_l) &#125;</code> 通常就是用来训练PPO流程中奖励模型的那个数据集。</p>
<p><strong>DPO训练循环</strong></p>
<p>DPO的训练过程更像一个标准的监督学习循环，而不是RL的“生成-评估-更新”循环。</p>
<p>对于从偏好数据集中采样的每一个<code>(x, y_w, y_l)</code>三元组：</p>
<ol>
<li>
<p><strong>计算策略模型概率：</strong></p>
<ul>
<li>将 <code>(x, y_w)</code> 输入到<strong>策略模型 <code>π_θ</code></strong> 中，计算模型生成 <code>y_w</code> 的总对数概率：<code>log π_θ(y_w | x)</code>。</li>
<li>将 <code>(x, y_l)</code> 输入到<strong>策略模型 <code>π_θ</code></strong> 中，计算模型生成 <code>y_l</code> 的总对数概率：<code>log π_θ(y_l | x)</code>。</li>
<li><em>（这是通过对回答中的每个token的条件概率取对数再求和得到的）</em></li>
</ul>
</li>
<li>
<p><strong>计算参考模型概率：</strong></p>
<ul>
<li>将 <code>(x, y_w)</code> 输入到<strong>固定的参考模型 <code>π_ref</code></strong> 中，计算其生成 <code>y_w</code> 的总对数概率：<code>log π_ref(y_w | x)</code>。</li>
<li>将 <code>(x, y_l)</code> 输入到<strong>固定的参考模型 <code>π_ref</code></strong> 中，计算其生成 <code>y_l</code> 的总对数概率：<code>log π_ref(y_l | x)</code>。</li>
</ul>
</li>
<li>
<p><strong>计算隐式奖励的差异：</strong></p>
<ul>
<li>DPO理论证明，模型的对数概率与参考模型的对数概率之差，正比于一个隐式的奖励函数。</li>
<li>计算<code>y_w</code>的隐式奖励（或称为“偏好得分”）:<br>
<code>r_θ(x, y_w) = β * (log π_θ(y_w | x) - log π_ref(y_w | x))</code></li>
<li>计算<code>y_l</code>的隐式奖励:<br>
<code>r_θ(x, y_l) = β * (log π_θ(y_l | x) - log π_ref(y_l | x))</code></li>
<li><code>β</code> 是一个超参数，通常设为0.1到0.5之间，它控制着模型对参考模型的偏离程度。</li>
</ul>
</li>
<li>
<p><strong>计算DPO损失函数：</strong></p>
<ul>
<li>DPO的目标是让 <code>y_w</code> 的奖励远高于 <code>y_l</code> 的奖励。它使用一个类似于<strong>二元分类的逻辑损失（Logistic Loss）</strong> 来实现这个目标。</li>
<li><code>Loss_DPO = -log(σ(r_θ(x, y_w) - r_θ(x, y_l)))</code></li>
<li>其中 <code>σ</code> 是 Sigmoid 函数。</li>
<li><strong>直观理解：</strong> 这个损失函数的目标是最大化 <code>r_θ(x, y_w)</code> 和 <code>r_θ(x, y_l)</code> 之间的差值。当策略模型赋予<code>y_w</code>的相对概率（相对于参考模型）远高于<code>y_l</code>时，损失就会变小。</li>
</ul>
</li>
<li>
<p><strong>反向传播与优化：</strong></p>
<ul>
<li>计算 <code>Loss_DPO</code> 相对于<strong>策略模型 <code>π_θ</code></strong> 参数的梯度。</li>
<li>使用AdamW等优化器更新<strong>策略模型 <code>π_θ</code></strong> 的参数。</li>
<li><strong>注意：参考模型 <code>π_ref</code> 的参数始终不更新。</strong></li>
</ul>
</li>
</ol>
<p>通过在整个偏好数据集上重复这个过程，策略模型 <code>π_θ</code> 会被直接优化，使其倾向于生成更符合人类偏好的回答。</p>
<br>
<h3 id="LLM-PPO">LLM-PPO</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a></p>
<img src="/2025/03/19/LLM-Rela/image-20250325131654259.png" class="" title="image-20250325131654259">
<br>
<p><strong>参与者</strong></p>
<p>在PPO训练循环开始前，我们有四个关键的模型，其中三个是神经网络：</p>
<ol>
<li>
<p><strong>Actor (策略模型 / Policy):</strong></p>
<ul>
<li><strong>角色:</strong> 主角，即我们正在微调的语言模型。</li>
<li><strong>来源:</strong> 经过第一步SFT（监督式微调）训练后的模型副本。</li>
<li><strong>任务:</strong> 根据当前状态 <code>s_t</code>（已生成的文本），生成下一个token <code>a_t</code>。</li>
</ul>
</li>
<li>
<p><strong>Reference Model (参考模型):</strong></p>
<ul>
<li><strong>角色:</strong> “锚点”或“约束器”。</li>
<li><strong>来源:</strong> 同样是第一步SFT模型的<strong>一个固定、不更新的副本</strong>。</li>
<li><strong>任务:</strong> 提供一个基准的概率分布 <code>π_ref(a|s_t)</code>，用于计算KL散度，防止Actor为了追求奖励而“走火入魔”，生成不自然或语法混乱的文本。</li>
</ul>
</li>
<li>
<p><strong>Critic (价值模型 / Value Network):</strong></p>
<ul>
<li><strong>角色:</strong> “评估员”或“裁判”。</li>
<li><strong>来源:</strong> 通常用RM的头部或SFT模型的头部初始化，然后与Actor一起在PPO阶段训练。</li>
<li><strong>任务:</strong> 评估当前状态 <code>s_t</code> 的<strong>潜在价值</strong> <code>V(s_t)</code>，即从这个状态开始，预期未来能获得多少总奖励。它的存在是为了减少策略梯度更新的方差（通过计算优势函数）。</li>
</ul>
</li>
<li>
<p><strong>Reward Model (奖励模型 / RM):</strong></p>
<ul>
<li><strong>角色:</strong> “最终裁判”，定义了优化的最终目标。</li>
<li><strong>来源:</strong> 第二步训练好的人类偏好模型，在PPO阶段<strong>保持固定，不更新</strong>。</li>
<li><strong>任务:</strong> 对一个<strong>完整的</strong>生成序列（prompt + response）给出一个标量分数 <code>R_final</code>，代表人类对这个回答的偏好程度。</li>
</ul>
</li>
</ol>
<p><strong>数据生成（Rollout Phase）</strong></p>
<p>对于一个从数据集中采样的 <code>prompt</code>，我们执行一次完整的序列生成来收集训练数据。</p>
<ol>
<li>
<p><strong>初始化:</strong> 从一个 <code>prompt</code> 开始，得到初始状态 <code>s_0</code>。</p>
</li>
<li>
<p><strong>逐Token生成循环 (For t = 0, 1, …, T-1):</strong></p>
<ul>
<li><strong>动作 (Action):</strong> Actor模型接收当前状态 <code>s_t</code>，输出概率分布 <code>π_actor(·|s_t)</code>，并从中<strong>采样</strong>一个token <code>a_t</code>。</li>
<li><strong>记录 (Log):</strong> 记录Actor输出该token的对数概率 <code>log π_actor(a_t|s_t)</code>。</li>
<li><strong>价值评估 (Value Estimation):</strong> Critic模型评估当前状态的价值 <code>V(s_t)</code>。</li>
<li><strong>计算即时奖励 (Immediate Reward):</strong> 这里的奖励<strong>主要不是来自RM</strong>。
<ul>
<li><strong>KL惩罚:</strong> 计算Actor策略与Reference策略在当前步的KL散度，作为惩罚项。<br>
<code>r_t = -β * log(π_actor(a_t|s_t) / π_ref(a_t|s_t))</code></li>
<li><code>β</code> 是一个控制KL惩罚力度的超参数。这个 <code>r_t</code> 是每一步都会计算的。</li>
</ul>
</li>
<li><strong>状态转移:</strong> 将新生成的token <code>a_t</code> 添加到序列中，得到新状态 <code>s_&#123;t+1&#125;</code>。</li>
<li><strong>存储经验:</strong> 将元组 <code>(s_t, a_t, r_t, V(s_t), log π_actor(a_t|s_t))</code> 存储起来。</li>
</ul>
</li>
<li>
<p><strong>最终奖励计算 (Final Reward):</strong></p>
<ul>
<li>当生成结束（例如遇到EOS token或达到最大长度）时，我们得到了一个完整的回答。</li>
<li>将完整的（<code>prompt</code>, <code>response</code>）输入到<strong>Reward Model (RM)</strong> 中，获得最终的标量奖励 <code>R_final</code>。</li>
<li>将这个最终奖励 <strong>加到最后一步的即时奖励 <code>r_T</code> 上</strong>。所以，<code>r_T</code> 变为 <code>r_T (KL惩罚) + R_final</code>。</li>
<li><strong>关键点:</strong> 只有在序列结束时，RM才提供一次性的、整体的奖励信号。而KL惩罚是贯穿于每一步的。</li>
</ul>
</li>
</ol>
<p>至此，我们收集到了一条完整的轨迹（trajectory）。</p>
<p><strong>模型学习（Learning Phase）</strong></p>
<p>利用收集到的轨迹数据，我们来更新Actor和Critic模型。</p>
<ol>
<li>
<p><strong>计算优势函数 (Advantage Estimation):</strong></p>
<ul>
<li>使用收集到的奖励 <code>r_t</code> 和价值估计 <code>V(s_t)</code>，通过**广义优势估计（GAE）**算法计算每一步的优势 <code>A_t</code>。</li>
<li>GAE能有效平衡偏差和方差，稳定训练过程。<code>A_t</code> 直观地表示了在状态 <code>s_t</code> 选择动作 <code>a_t</code> 相对于平均水平有多好。</li>
</ul>
</li>
<li>
<p><strong>更新Actor (策略模型):</strong></p>
<ul>
<li>使用计算出的优势 <code>A_t</code> 和之前记录的 <code>log π_actor(a_t|s_t)</code>，构造PPO的<strong>裁剪代理目标函数（Clipped Surrogate Objective）</strong>。</li>
<li>通过梯度上升（最大化目标函数）来更新Actor模型的参数。PPO的裁剪机制确保了每次更新的步子不会太大，从而保证了训练的稳定性。</li>
</ul>
</li>
<li>
<p><strong>更新Critic (价值模型):</strong></p>
<ul>
<li>Critic的目标是更准确地预测未来的回报。</li>
<li>构造一个损失函数，通常是<strong>均方误差（MSE）</strong>，使其预测的价值 <code>V(s_t)</code> 尽可能接近于在该步之后实际观察到的累积回报（也称为Returns）。</li>
<li>通过梯度下降来更新Critic模型的参数。</li>
</ul>
</li>
</ol>
<img src="/2025/03/19/LLM-Rela/image-20250907172731668.png" class="" title="image-20250907172731668">
<br>
<p><strong>SFT LLM</strong></p>
<p>train Reward Model</p>
<img src="/2025/03/19/LLM-Rela/image-20250325151928459.png" class="" title="image-20250325151928459">
<p>use LLM_sft to be the Actor(reference model)</p>
<img src="/2025/03/19/LLM-Rela/image-20250325153214542.png" class="" title="image-20250325153214542">
<img src="/2025/03/19/LLM-Rela/image-20250325153304648.png" class="" title="image-20250325153304648">
<br>
<p><strong>PPO架构</strong></p>
<img src="/2025/03/19/LLM-Rela/image-20250325153337812.png" class="" title="image-20250325153337812">
<blockquote>
<p>广义优势GAE为多步时序差分的指数加权平均，详见reinforce-learning-record，参数含Value值和Reward值</p>
</blockquote>
<p><strong>为什么引入KL散度项？</strong></p>
<p>这个KL项有两个作用。首先，它作为一种熵奖励，鼓励策略进行探索，防止其坍缩到单一模式。其次，它确保策略不会学习生成与奖励模型在训练期间见过的输出差异过大的结果。</p>
<ul>
<li>在 RLHF 中，PPO 策略在优化过程中可能会偏离初始 SFT 模型太远，导致“灾难性遗忘” (catastrophic forgetting)，即模型忘记了 SFT 阶段学到的通用语言能力或指令遵循能力，过度追求奖励信号。PPO 通常会加入一个 KL 散度惩罚项来约束当前策略与 SFT 模型的距离。</li>
</ul>
<br>
<h3 id="LLM-GRPO">LLM-GRPO</h3>
<p>Group Relative Policy Optimization</p>
<ul>
<li>
<p>一个问题，多个回答</p>
</li>
<li>
<p>reward = Model + Rules -&gt; reward (仍然是对每句话给出)</p>
</li>
<li>
<p>advantage = $\frac{r - mean}{std}$</p>
</li>
<li>
<p>优化公式变化</p>
</li>
</ul>
<br>
<img src="/2025/03/19/LLM-Rela/image-20250325195333997.png" class="" title="image-20250325195333997">
<p>由于PPO算法中使用的价值函数通常是与策略模型规模相当的另一个模型，这会带来巨大的内存和计算负担。此外，在强化学习训练过程中，价值函数被作为计算优势函数（advantage）的基线以实现方差缩减。然而在大型语言模型（LLM）场景中，通常只有最后一个token会被奖励模型分配奖励分数，这可能导致对每个token都精确建模价值函数的训练变得复杂。为解决这一问题，如图4所示，我们提出了组相对策略优化（Group Relative Policy Optimization, GRPO）</p>
<p>每一个 $o_g $为策略模型对于输入 $q$ 的输出，每一个 $r_g$ 为 RM 对 每一个 $o_g$ 的评分</p>
<img src="/2025/03/19/LLM-Rela/image-20250325201341810.png" class="" title="image-20250325201341810">
<img src="/2025/03/19/LLM-Rela/image-20250901113222981.png" class="" title="image-20250901113222981">
<img src="/2025/03/19/LLM-Rela/image-20250325201533207.png" class="" title="image-20250325201533207">
<br>
<p>GRPO 的奖励（Reward）和优势（Advantage）是在序列层面（per-sequence）计算的，但在应用这个优势进行策略更新时，其目标函数是在词元层面（per-token）上定义的。</p>
<p><strong>奖励和优势的计算：序列级 (Per-Sequence)</strong></p>
<ul>
<li><strong>单一奖励</strong>：首先，模型针对一个问题 <code>q</code> 生成一个<strong>完整的</strong>输出序列 <code>o_i</code>。然后，一个奖励模型会评估这<strong>整个</strong>序列，并给出一个<strong>单一的、整体的</strong>奖励分数 <code>r_i</code>。</li>
<li><strong>单一优势</strong>：接着，算法会比较一组（G个）输出序列的奖励，计算出每个序列 <code>o_i</code> 的<strong>单一优势值 Â_i</strong>。这个优势值代表了序列 <code>o_i</code> 相对于同组其他序列的优劣程度。</li>
</ul>
<p>到此为止，所有的评估都是在序列（sequence）层面完成的。我们得到了一个适用于整个序列 <code>o_i</code> 的功劳/惩罚信号 <code>Â_i</code>。</p>
<p><strong>目标函数的计算：词元级 (Per-Token)</strong></p>
<p>现在，如何用这个序列级的优势 <code>Â_i</code> 来更新模型参数呢？这里就是词元级计算发挥作用的地方，也是第三张图中公式的核心。</p>
<ul>
<li>
<p><strong>广播优势 (Broadcast Advantage)</strong>：算法将序列级的优势值 <code>Â_i</code> <strong>“广播”或复制</strong>给该序列中的<strong>每一个词元 (token)</strong>。也就是说，对于序列 <code>o_i</code> 中的所有时间步 <code>t</code> (从1到序列长度 <code>|o_i|</code>)，我们设定 <code>Â_&#123;i,t&#125; = Â_i</code>。这意味着，如果一个序列是好的（<code>Â_i</code> &gt; 0），那么构成这个序列的<strong>每一个词元</strong>都被认为是“有功劳的”；反之亦然。</p>
</li>
<li>
<p><strong>词元级概率比 (Per-Token Policy Ratio)</strong>：在目标函数内部，计算的是<strong>每个词元</strong>的概率比：<code>π_θ(o_&#123;i,t&#125; | q, o_&#123;i,&lt;t&#125;) / π_θ_old(o_&#123;i,t&#125; | q, o_&#123;i,&lt;t&#125;)</code>。这衡量了新策略相比于旧策略，在特定上下文中生成特定词元 <code>o_&#123;i,t&#125;</code> 的概率变化。</p>
</li>
<li>
<p><strong>累加和平均 (Summation and Average)</strong>：如公式所示，最终的目标函数 <code>J_GRPO(θ)</code> 是对一个序列中<strong>所有词元的损失进行求和，然后再取平均</strong> (<code>1/|o_i| * Σ_&#123;t=1 to |o_i|&#125;</code>)。每个词元的损失都是用 PPO 的 <code>min-clip</code> 形式，乘以<strong>被广播过来的那个相同的优势值 <code>Â_&#123;i,t&#125;</code></strong> 来计算的。</p>
</li>
</ul>
<br>
<p>过程监督与结果监督</p>
<ol>
<li>结果监督 (Outcome Supervision)</li>
</ol>
<p>这部分描述的就是我们之前一直在讨论的标准 GRPO 模式。明确指出，优势值 <code>Â_&#123;i,t&#125;</code> 是为<strong>所有词元 (all tokens)</strong> 设置的。所有这些词元的优势值都被设置成了<strong>同一个值</strong>——即整个序列的归一化奖励 <code>r̃_i</code>。一个在<strong>序列层面</strong>计算出的单一评估值（这里的归一化奖励 <code>r̃_i</code>，也就是优势），被<strong>复制或广播</strong>给了构成该序列的<strong>每一个词元</strong>。</p>
<ol start="2">
<li>过程监督 (Process Supervision)</li>
</ol>
<p>这一节作为对比，描述了一种更复杂的、<strong>非广播</strong>的模式：</p>
<ul>
<li><strong>奖励</strong>：奖励是在<strong>每一步 (each reasoning step)</strong> 提供的，而不是只在最后。</li>
<li><strong>优势</strong>：每个词元的优势 <code>Â_&#123;i,t&#125;</code> 是通过计算<strong>其后所有步骤</strong>的奖励之和来得到的 (<code>Σ_&#123;index(j)≥t&#125;</code>…)。</li>
</ul>
<p>在这种模式下，一个序列中不同位置的词元 <code>t</code> 会有<strong>不同</strong>的优势值 <code>Â_&#123;i,t&#125;</code>。这与传统的强化学习中的回报（Return）计算方式更相似。</p>
<img src="/2025/03/19/LLM-Rela/image-20250325202638658.png" class="" title="image-20250325202638658">
<br>
<h3 id="LLM-GAPO">LLM-GAPO</h3>
<p>ref: <a target="_blank" rel="noopener" href="https://fcnisnh9uh54.feishu.cn/docx/JYETdd6SPoILyTxW443cwzv0nTc">https://fcnisnh9uh54.feishu.cn/docx/JYETdd6SPoILyTxW443cwzv0nTc</a></p>
<p><strong>Clip-Higher</strong></p>
<p>假设AI在写“猫坐在__上”时，有两个选择：</p>
<ul>
<li><strong>高概率词（利用 exploitation token）</strong>: “垫子”。它原来的概率是90% (0.9)。因为这个词很合理，AI得到了奖励。在 [0.8, 1.2] 的限制下，它的新概率最高可以被更新到 0.9 * 1.2 = 1.08（理论上，实际会归一化，但可以看出提升空间很大），很容易就从90%提升到99.9%。</li>
<li><strong>低概率词（探索 exploration token）</strong>: “王座”。这是一个非常有创意的词，原来的概率只有1% (0.01)。假设这次使用“王座”得到了超高的奖励，AI很想大幅提升它的概率。但遗憾的是，它同样受到1.2倍的上限限制，新概率最多只能提升到 0.01 * 1.2 = 0.012。</li>
</ul>
<p><strong>结论</strong>：传统的PPO-Clip机制严重压制了那些有潜力的“探索性”词汇。AI很难学会使用这些新词，因为它得到的奖励信号被这个对称的“剪刀”给剪掉了，导致其创造力无法被有效强化。</p>
<p><strong>方法</strong>：进行重要性采样比率裁剪时，解耦上下界（提高上界）</p>
<p><strong>Dynamic Sampling</strong></p>
<p>采样时过滤掉准确率为1或0的prompt</p>
<p><strong>Token-leven Policy Gradient Loss</strong></p>
<p>词元级别打分</p>
<p><strong>Overlong Reward Shaping</strong></p>
<br>
<h3 id="LLM-GSPO">LLM-GSPO</h3>
<p><strong>单一的重要性权重 (Single Importance Weight)</strong>: <code>s_i(θ)</code> 是为<strong>一整个序列 <code>y_i</code></strong> 计算出来的一个<strong>标量值</strong>。它首先计算整个序列的新旧策略概率比，然后进行长度归一化。</p>
<p><strong>没有词元级求和 (No Per-Token Summation)</strong>: 在 <code>min(...)</code> 这个核心计算部分，<strong>没有任何对序列内部词元 <code>t</code> 的求和符号 <code>Σ_t</code></strong>。目标函数直接将序列级的权重 <code>s_i(θ)</code> 与序列级的优势 <code>Â_i</code> 相乘。</p>
<p><strong>将序列视为一个整体</strong>: 整个优化过程将每个输出序列 <code>y_i</code> 视为一个不可分割的整体数据点。优化目标是提高那些具有正优势值的<strong>完整序列</strong>的生成概率（由 <code>s_i(θ)</code> 体现），同时通过 <code>clip</code> 函数加以限制。</p>
<img src="/2025/03/19/LLM-Rela/image-20250901105613345.png" class="" title="image-20250901105613345">
<p>以优秀的回复 <strong>y₁</strong> 为例：</p>
<p><code>s₁(θ)</code> = (新模型 <code>π_θ</code> 生成 y₁ 的概率 / 旧模型 <code>π_θold</code> 生成 y₁ 的概率) ^ (1 / y₁的长度)</p>
<ul>
<li><strong>概率比值</strong>：假设我们的新模型 <code>π_θ</code> 经过初步学习，现在生成 y₁ 的概率比旧模型高了一点。比如，概率比值是 1.15。</li>
<li><strong>长度归一化</strong>：公式右上角的 <code>1/|y_i|</code> 是长度归一化。 这么做的目的是为了消除句子长度对结果的影响，使得长句子和短句子的比例值可以在一个统一的范围内比较，从而降低方差，让训练更稳定。</li>
<li>假设经过长度归一化后，我们得到 <code>s₁(θ) = 1.1</code>。</li>
</ul>
<p>同样，对于糟糕的回复 <strong>y₃</strong>，新模型可能更不倾向于生成它，所以 <code>s₃(θ)</code> 可能是一个小于1的数字，比如 0.9。</p>
<br>
<h2 id="附录">附录</h2>
<h3 id="为什么Qwen3-0725放弃了thinking-fusion">为什么Qwen3 0725放弃了thinking fusion?</h3>
<p>以下是结合相关技术分析，对这种“单一模型+特殊令牌”实现方式存在问题的深入探讨：</p>
<p><strong>性能稀释与参数冲突 (Performance Dilution &amp; Parameter Conflict)</strong></p>
<p>这是最根本的问题。一个模型的参数（权重）需要同时学习两种截然不同的任务模式：</p>
<ul>
<li><strong>“指令”模式 (Non-thinking):</strong> 目标是<strong>简洁、直接、高效</strong>。它需要模型学习遵循指令、提取信息、保持风格，并快速生成符合人类交流习惯的答案。这部分训练会惩罚冗余和啰嗦。</li>
<li><strong>“思考”模式 (Thinking):</strong> 目标是<strong>严谨、深入、逻辑自洽</strong>。它需要模型学习逐步分解问题、进行符号推理、验证中间步骤，并最终得出结论。这部分训练会奖励详细的、展示推理过程的输出。</li>
</ul>
<p>让同一组神经网络参数去同时精通这两种互有冲突的目标，就像要求一位短跑冠军同时也是一位马拉松冠军。虽然可能做到“两边都还不错”，但很难在任何一个项目上都达到世界顶尖水平。模型有限的“能力带宽”（parameter capacity）会被分散，导致：</p>
<ul>
<li>在需要简洁回答时，可能仍然会残留一些推理的痕迹，不够直接。</li>
<li>在需要复杂推理时，为了兼顾简洁性，其推理的深度和严谨性可能会受到限制。</li>
</ul>
<p>最终结果是两种模式的性能都无法达到极致，是一种“折衷”而非“最优”。</p>
<p><strong>训练目标与优化难题 (Conflicting Training Objectives)</strong></p>
<p>在模型的训练阶段，尤其是RLHF（基于人类反馈的强化学习）阶段，这种冲突会变得更加尖锐。</p>
<ul>
<li>对于一个需要快速回答的问题，人类标注员会给简洁、直接的答案打高分。</li>
<li>对于一个复杂的数学题，标注员会给详细、步骤正确的“思考链”答案打高fen。</li>
</ul>
<p>当模型面对这些混合在一起的反馈信号时，其优化过程会感到“困惑”。优化算法（如PPO）很难找到一个能同时满足这两个矛盾目标的“最优策略”。这可能导致训练不稳定，或者模型最终学到一种“四不像”的中间策略。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python-Pytorch/" rel="tag"># Python, Pytorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/02/19/Computer-Network/" rel="prev" title="Computer-Network">
      <i class="fa fa-chevron-left"></i> Computer-Network
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/03/28/Netty/" rel="next" title="Netty">
      Netty <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">LLM&amp;Rela</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-base"><span class="nav-text">LLM-base</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%9F%E6%9C%9B%E4%B8%8E%E6%96%B9%E5%B7%AE"><span class="nav-text">期望与方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%8F%E6%96%B9%E5%B7%AE"><span class="nav-text">协方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-text">优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">随机梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">小批量梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adam"><span class="nav-text">Adam</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1-%E7%88%86%E7%82%B8"><span class="nav-text">梯度消失&amp;爆炸</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96-%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-text">正则化&amp;归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#batch"><span class="nav-text">batch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E9%81%93"><span class="nav-text">通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNN-LSTM"><span class="nav-text">RNN&amp;LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RNN"><span class="nav-text">RNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LSTM"><span class="nav-text">LSTM</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-struc"><span class="nav-text">LLM-struc</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder-Decoder"><span class="nav-text">Encoder&amp;Decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder-Only"><span class="nav-text">Encoder Only</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Decoder-Only"><span class="nav-text">Decoder Only</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tokenizer"><span class="nav-text">Tokenizer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#BPE"><span class="nav-text">BPE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#WordPiece"><span class="nav-text">WordPiece</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SentencePiece"><span class="nav-text">SentencePiece</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%8D%E8%A1%A8"><span class="nav-text">词表</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Embedding"><span class="nav-text">Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Word2Vec"><span class="nav-text">Word2Vec</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Positional-Encoding"><span class="nav-text">Positional Encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Sinusoidal"><span class="nav-text">Sinusoidal</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RoPE"><span class="nav-text">RoPE</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention"><span class="nav-text">Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-text">注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B-decoder"><span class="nav-text">自注意力 - decoder</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B-encoder"><span class="nav-text">交叉注意力 - encoder</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-text">多头注意力</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A9%E7%A0%81%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-text">掩码注意力</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MHA%E3%80%81MQA%E3%80%81GQA-MLA"><span class="nav-text">MHA、MQA、GQA&amp;MLA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Flash-Attention"><span class="nav-text">Flash Attention</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Layer-Norm-Residual-Network"><span class="nav-text">Layer Norm &amp; Residual Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MLP-FFN"><span class="nav-text">MLP&amp;FFN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98"><span class="nav-text">相关问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-Train"><span class="nav-text">LLM-Train</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Base"><span class="nav-text">Base</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parallelism"><span class="nav-text">Parallelism</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DataParallel-DP"><span class="nav-text">DataParallel(DP)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TensorParallel-TP"><span class="nav-text">TensorParallel(TP)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PipelineParallel-PP"><span class="nav-text">PipelineParallel(PP)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Framework"><span class="nav-text">Framework</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Megatron"><span class="nav-text">Megatron</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Deepspeed"><span class="nav-text">Deepspeed</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-Inference"><span class="nav-text">LLM-Inference</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Parameters"><span class="nav-text">Parameters</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Temperature"><span class="nav-text">Temperature</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sampling"><span class="nav-text">Sampling</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E6%8A%80%E6%9C%AF"><span class="nav-text">推理加速技术</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Speculative-Decoding"><span class="nav-text">Speculative Decoding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KV-cache-Page-Attention"><span class="nav-text">KV cache&amp;Page Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GQA-MLA"><span class="nav-text">GQA &amp; MLA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sliding-Window-Attention"><span class="nav-text">Sliding Window Attention</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E6%89%A9%E5%B1%95"><span class="nav-text">上下文扩展</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Yarn"><span class="nav-text">Yarn</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DCA"><span class="nav-text">DCA</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Framework-2"><span class="nav-text">Framework</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#vllm"><span class="nav-text">vllm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SGLang"><span class="nav-text">SGLang</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fine-Tuning"><span class="nav-text">Fine-Tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BitFit-Prefix-Tuning-Prompt-Tuning"><span class="nav-text">BitFit, Prefix Tuning &amp; Prompt Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#P-Tuning"><span class="nav-text">P-Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adapter-Tuning"><span class="nav-text">Adapter Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LoRA"><span class="nav-text">LoRA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E4%B8%8E%E4%BD%8E%E7%A7%A9%E5%88%86%E8%A7%A3"><span class="nav-text">奇异值分解与低秩分解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LoRA%E5%BA%94%E7%94%A8%E4%BD%8D%E7%BD%AE"><span class="nav-text">LoRA应用位置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LoRA%E6%94%B9%E8%BF%9B"><span class="nav-text">LoRA改进</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#LoRA-2"><span class="nav-text">LoRA+</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DoRA"><span class="nav-text">DoRA</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#rsLoRA"><span class="nav-text">rsLoRA</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#PiSSA"><span class="nav-text">PiSSA</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LoRA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87"><span class="nav-text">LoRA相关论文</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7"><span class="nav-text">调参技巧</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MAM-Adapter-UniPELT"><span class="nav-text">MAM Adapter &amp; UniPELT</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reinforce-Learning-on-LLM"><span class="nav-text">Reinforce Learning on LLM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Base-2"><span class="nav-text">Base</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Rollout-Sample"><span class="nav-text">Rollout &amp; Sample</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LLM-DPO"><span class="nav-text">LLM-DPO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LLM-PPO"><span class="nav-text">LLM-PPO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LLM-GRPO"><span class="nav-text">LLM-GRPO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LLM-GAPO"><span class="nav-text">LLM-GAPO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LLM-GSPO"><span class="nav-text">LLM-GSPO</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88Qwen3-0725%E6%94%BE%E5%BC%83%E4%BA%86thinking-fusion"><span class="nav-text">为什么Qwen3 0725放弃了thinking fusion?</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="marigo1d"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">marigo1d</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/marigo1d" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;marigo1d" rel="noopener" target="_blank">GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">marigo1d</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">555k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">16:49</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
