<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"marigo1d.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="AI-Agent设计相关">
<meta property="og:type" content="article">
<meta property="og:title" content="AI-Agent">
<meta property="og:url" content="https://marigo1d.github.io/2025/04/21/AI-Agent/index.html">
<meta property="og:site_name" content="Marigold">
<meta property="og:description" content="AI-Agent设计相关">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/Gu-NqzLb0AAG0lX">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/tool_calling_components-bef9d2bcb9d3706c2fe58b57bf8ccb60.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/structured_output-2c42953cee807dedd6e96f3e1db17f69.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/short-vs-long.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/filter.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/summary.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250810091717337.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/2d00628-Untitled_2.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250909141156823.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250909141213294.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250909141229088.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250909141244188.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250909141257247.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250810092757168.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/5.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/v2-2bcd98f6541da0b6f14dc9082ee2dcda_1440w.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250810092707204.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250604165407470.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250604170724823.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250604173158175.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250604173209083.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/b2aaf634151b4706892693ffb43d9093.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/prompt_components.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/Medqa-comp.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/black_box_graphic.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/white_box_graphic.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/judges_graphic.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/uqensemble_generate_score.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/GsmyMNLasAQ_DY6">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/GsmySEdaQAACdBZ">
<meta property="article:published_time" content="2025-04-21T01:19:56.000Z">
<meta property="article:modified_time" content="2025-09-09T06:12:58.471Z">
<meta property="article:author" content="marigo1d">
<meta property="article:tag" content="Python, Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/Gu-NqzLb0AAG0lX">

<link rel="canonical" href="https://marigo1d.github.io/2025/04/21/AI-Agent/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AI-Agent | Marigold</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Marigold</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Salt, Pepper and Birds~</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://marigo1d.github.io/2025/04/21/AI-Agent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="marigo1d">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Marigold">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AI-Agent
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-04-21 09:19:56" itemprop="dateCreated datePublished" datetime="2025-04-21T09:19:56+08:00">2025-04-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-09-09 14:12:58" itemprop="dateModified" datetime="2025-09-09T14:12:58+08:00">2025-09-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>30k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>55 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>AI-Agent设计相关</p>
<span id="more"></span>
<h1>AI-Agent</h1>
<img src="/2025/04/21/AI-Agent/Gu-NqzLb0AAG0lX" class="" title="Agent_workflow">
<p>AI-Agent侧区分</p>
<ul>
<li>LLM部分，包含对基座LLM的特化，如SFT和RLHF</li>
<li>Agent部分，包含执行链，如上下文管理，MCP（工具管理与使用）</li>
<li>数据部分，包含训练数据和检索数据，如RAG检索数据，SFT训练数据，FAISS库</li>
</ul>
<blockquote>
<p>LLM部分详见 LLM&amp;Rela</p>
</blockquote>
<br>
<h2 id="Base">Base</h2>
<h3 id="Model-rela">Model rela</h3>
<h4 id="Key-methods">Key methods</h4>
<ol>
<li>invoke：与聊天模型交互的主要方法。它接受一个消息列表作为输入，并返回一个消息列表作为输出。</li>
<li>stream：允许您以流的形式接收聊天模型生成的输出的方法。</li>
<li>batch：允许您将多个请求批量发送到聊天模型，以提高处理效率的方法。</li>
<li>bind_tools：一种允许您将工具绑定到聊天模型，以便在模型的执行上下文中使用的方法。</li>
<li>with_structured_output：针对原生支持结构化输出的模型的 <code>invoke</code> 方法的包装器。</li>
</ol>
<br>
<h4 id="Tool-calling">Tool calling</h4>
<img src="/2025/04/21/AI-Agent/tool_calling_components-bef9d2bcb9d3706c2fe58b57bf8ccb60.png" class="" title="Conceptual parts of tool calling">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tool creation</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.tools <span class="keyword">import</span> tool</span><br><span class="line"></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiply a and b.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tool binding</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiply a and b.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        a: first int</span></span><br><span class="line"><span class="string">        b: second int</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line">llm_with_tools = tool_calling_model.bind_tools([multiply])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tool calls</span></span><br><span class="line">query = <span class="string">&quot;What is 3 * 12? Also, what is 11 + 49?&quot;</span></span><br><span class="line"></span><br><span class="line">llm_with_tools.invoke(query).tool_calls</span><br></pre></td></tr></table></figure>
<br>
<h4 id="Structured-Output">Structured Output</h4>
<img src="/2025/04/21/AI-Agent/structured_output-2c42953cee807dedd6e96f3e1db17f69.png" class="" title="Structured output">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define schema</span></span><br><span class="line">schema = &#123;<span class="string">&quot;foo&quot;</span>: <span class="string">&quot;bar&quot;</span>&#125;</span><br><span class="line"><span class="comment"># Bind schema to model</span></span><br><span class="line">model_with_structure = model.with_structured_output(schema)</span><br><span class="line"><span class="comment"># Invoke the model to produce structured output that matches the schema</span></span><br><span class="line">structured_output = model_with_structure.invoke(user_input)</span><br></pre></td></tr></table></figure>
<br>
<h4 id="Multimodality">Multimodality</h4>
<p>多模态指的是处理不同形式数据的能力，例如文本、音频、图像和视频。多模态可以出现在各种组件中，使模型和系统能够无缝地处理这些数据类型的混合。</p>
<ul>
<li>聊天模型：理论上，这些模型可以接受和生成多模态输入和输出，处理各种数据类型，如文本、图像、音频和视频。</li>
<li>嵌入模型：嵌入模型可以表示多模态内容，将各种形式的数据（如文本、图像和音频）嵌入到向量空间中。</li>
<li>向量存储：向量存储可以搜索表示多模态数据的嵌入，实现不同类型信息的检索。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage</span><br><span class="line"></span><br><span class="line">message = HumanMessage(</span><br><span class="line">    content=[</span><br><span class="line">        &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>, <span class="string">&quot;text&quot;</span>: <span class="string">&quot;Describe the weather in this image:&quot;</span>&#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;image&quot;</span>,</span><br><span class="line">            <span class="string">&quot;source_type&quot;</span>: <span class="string">&quot;base64&quot;</span>,</span><br><span class="line">            <span class="string">&quot;data&quot;</span>: <span class="string">&quot;&lt;base64 string&gt;&quot;</span>,</span><br><span class="line">            <span class="string">&quot;mime_type&quot;</span>: <span class="string">&quot;image/jpeg&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line">response = model.invoke([message])</span><br><span class="line"></span><br><span class="line">message = HumanMessage(</span><br><span class="line">    content=[</span><br><span class="line">        &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>, <span class="string">&quot;text&quot;</span>: <span class="string">&quot;Describe the weather in this image:&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;image_url&quot;</span>, <span class="string">&quot;image_url&quot;</span>: &#123;<span class="string">&quot;url&quot;</span>: image_url&#125;&#125;,</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line">response = model.invoke([message])</span><br></pre></td></tr></table></figure>
<br>
<h3 id="Memory">Memory</h3>
<img src="/2025/04/21/AI-Agent/short-vs-long.png" class="" title="img">
<h4 id="短期记忆">短期记忆</h4>
<p>会话历史管理</p>
<h5 id="编辑消息列表">编辑消息列表</h5>
<p>聊天模型通过消息接受上下文，这些消息包括开发者提供的指令（系统消息）和用户输入（人类消息）。在聊天应用中，消息在人类输入和模型响应之间交替，导致消息列表随着时间的推移而变长。由于上下文窗口有限且富标记的消息列表可能成本高昂，许多应用可以从使用手动删除或忘记过时信息的技术中受益。</p>
<img src="/2025/04/21/AI-Agent/filter.png" class="" title="img">
<p>最直接的方法是从列表中删除旧消息（类似于最近最少使用缓存）</p>
<br>
<h5 id="总结过往对话">总结过往对话</h5>
<p>问题在于修剪或删除消息，如上所示，我们可能会从消息队列的筛选中丢失信息。因此，一些应用程序从使用聊天模型对消息历史进行更复杂的总结方法中受益。</p>
<img src="/2025/04/21/AI-Agent/summary.png" class="" title="img">
<br>
<h4 id="长期记忆">长期记忆</h4>
<h5 id="记忆存储">记忆存储</h5>
<br>
<h5 id="记忆类型">记忆类型</h5>
<ul>
<li>
<p>Semantic Memory：</p>
<p>语义记忆通常用于通过记住过去交互中的事实或概念来个性化应用程序。个性化的记忆</p>
<p>例如个人资料和收藏</p>
</li>
<li>
<p>Episodic Memory：</p>
<p>情景记忆通常用于帮助代理记住如何完成任务。</p>
</li>
<li>
<p>Procedural Memory：</p>
<p>程序性记忆是模型权重、代理代码和代理提示的组合；</p>
<p>一般仅修改系统提示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个节点函数，该函数使用存储中的指令来调用模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">call_model</span>(<span class="params">state: State, store: BaseStore</span>):</span><br><span class="line">    <span class="comment"># 定义命名空间，这里使用&quot;agent_instructions&quot;作为命名空间</span></span><br><span class="line">    namespace = (<span class="string">&quot;agent_instructions&quot;</span>, )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从存储中获取键为&quot;agent_a&quot;的指令数据</span></span><br><span class="line">    <span class="comment"># [0]表示获取搜索结果的第一项(假设search返回列表)</span></span><br><span class="line">    instructions = store.get(namespace, key=<span class="string">&quot;agent_a&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 应用逻辑：使用获取的指令构建提示词</span></span><br><span class="line">    <span class="comment"># prompt_template是一个格式化字符串模板，包含&#123;instructions&#125;占位符</span></span><br><span class="line">    prompt = prompt_template.<span class="built_in">format</span>(instructions=instructions.value[<span class="string">&quot;instructions&quot;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 这里应该有调用模型等后续操作...</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个节点函数，该函数更新存储中的指令</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_instructions</span>(<span class="params">state: State, store: BaseStore</span>):</span><br><span class="line">    <span class="comment"># 定义命名空间，这里使用&quot;instructions&quot;作为命名空间</span></span><br><span class="line">    namespace = (<span class="string">&quot;instructions&quot;</span>,)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 搜索当前命名空间下的所有指令，并获取第一个结果</span></span><br><span class="line">    current_instructions = store.search(namespace)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 记忆逻辑：使用当前指令和对话状态构建提示词</span></span><br><span class="line">    prompt = prompt_template.<span class="built_in">format</span>(</span><br><span class="line">        instructions=instructions.value[<span class="string">&quot;instructions&quot;</span>], </span><br><span class="line">        conversation=state[<span class="string">&quot;messages&quot;</span>]</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调用语言模型获取输出</span></span><br><span class="line">    output = llm.invoke(prompt)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从模型输出中提取新的指令</span></span><br><span class="line">    new_instructions = output[<span class="string">&#x27;new_instructions&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将新指令存储到&quot;agent_instructions&quot;命名空间下，键为&quot;agent_a&quot;</span></span><br><span class="line">    store.put(</span><br><span class="line">        (<span class="string">&quot;agent_instructions&quot;</span>,),  <span class="comment"># 命名空间</span></span><br><span class="line">        <span class="string">&quot;agent_a&quot;</span>,                <span class="comment"># 键名</span></span><br><span class="line">        &#123;<span class="string">&quot;instructions&quot;</span>: new_instructions&#125;  <span class="comment"># 值(新指令)</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 这里可能有其他后续操作...</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
</li>
</ul>
<br>
<h3 id="Chunking">Chunking</h3>
<p>ref: <a target="_blank" rel="noopener" href="https://x.com/i/status/1952300998942396582">https://x.com/i/status/1952300998942396582</a></p>
<img src="/2025/04/21/AI-Agent/image-20250810091717337.png" class="" title="image-20250810091717337">
<p>将长文档按段落或 token 限制拆分为小块（chunk），以避免输入超长。</p>
<p><code>RecursiveCharacterTextSplitter</code> 通过<strong>递归尝试不同的分隔符层级</strong>，逐步将文本分割为语义连贯的块。它优先尝试更高级别的分隔符（如段落），如果分割后的块仍过大，则递归使用更低级别的分隔符（如句子、单词）进行二次分割。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line">splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">500</span>, chunk_overlap=<span class="number">50</span>)</span><br><span class="line">chunks = splitter.split_text(document_text)</span><br></pre></td></tr></table></figure>
<br>
<h4 id="Doc-structure-Chunking">Doc structure Chunking</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://github.com/ConardLi/easy-dataset">https://github.com/ConardLi/easy-dataset</a></p>
<ol>
<li>首先需要设定文本块的最小，最大分割长度</li>
<li>自动对章节（Markdown文件中的 <code>#, ##, ###</code>）进行识别</li>
<li>对已识别到的章节字数进行计数，在恰好位于 &gt; 最小分割长度 和 &lt; 最大分割长度的前提下进行分段</li>
<li>遇到长段落（超出最大分割长度）时，执行递归分段算法 <code>RecursiveCharacterTextSplitter</code>；</li>
</ol>
<blockquote>
<p>其实就是加了一个对章节的识别分割</p>
</blockquote>
<br>
<h3 id="Embedding">Embedding</h3>
<p>使用 Embedding 模型（如 OpenAI、HuggingFace）将文本向量化，然后存入向量数据库（如 FAISS、Chroma、Pinecone）。</p>
<p>E5、BGE、T5、DPR等多种预训练模型进行Embedding</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-large&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FAISS存储</span></span><br><span class="line">embedding_dim = <span class="built_in">len</span>(embeddings.embed_query(<span class="string">&quot;hello world&quot;</span>))</span><br><span class="line">index = faiss.IndexFlatL2(embedding_dim)</span><br><span class="line"></span><br><span class="line">vector_store = FAISS(</span><br><span class="line">    embedding_function=embeddings,</span><br><span class="line">    index=index,</span><br><span class="line">    docstore=InMemoryDocstore(),</span><br><span class="line">    index_to_docstore_id=&#123;&#125;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">ids = vector_store.add_documents(documents=all_splits)</span><br></pre></td></tr></table></figure>
<br>
<h3 id="Retrieval">Retrieval</h3>
<h4 id="BM25">BM25</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyserini.search.lucene <span class="keyword">import</span> LuceneSearcher</span><br><span class="line">self.searcher = LuceneSearcher(self.index_path)</span><br></pre></td></tr></table></figure>
<br>
<h4 id="Dense">Dense</h4>
<p>k-NN</p>
<p>案例：使用向量检索获取相关 chunk，输入给 LLM，生成答案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> chain</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@chain</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">retriever</span>(<span class="params">query: <span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[Document]:</span><br><span class="line">    <span class="keyword">return</span> vector_store.similarity_search(query, k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">retriever.batch(</span><br><span class="line">    [</span><br><span class="line">        <span class="string">&quot;How many distribution centers does Nike have in the US?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;When was Nike incorporated?&quot;</span>,</span><br><span class="line">    ],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<br>
<h3 id="Reranker">Reranker</h3>
<p><a target="_blank" rel="noopener" href="https://www.pinecone.io/learn/series/rag/rerankers/">https://www.pinecone.io/learn/series/rag/rerankers/</a></p>
<h4 id="RRF">RRF</h4>
<p>Reciprocal Rank Fusion</p>
<p>RRF_Score(d) = $\sum (1 / (k + rank_i(d)))$</p>
<br>
<h4 id="Model-based-Reranking">Model-based Reranking</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://haystack.deepset.ai/tutorials/33_hybrid_retrieval#creating-a-pipeline-for-hybrid-retrieval">https://haystack.deepset.ai/tutorials/33_hybrid_retrieval#creating-a-pipeline-for-hybrid-retrieval</a></p>
<p>使用 TransformersSimilarityRanker，通过交叉编码模型对给定搜索查询中检索到的所有文档的相关性进行评分。在本示例中，您将使用 BAAI/bge-reranker-base 模型对检索到的文档进行排序，但您可以将其替换为 Hugging Face 上的其他交叉编码模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> haystack.components.rankers <span class="keyword">import</span> TransformersSimilarityRanker</span><br><span class="line"></span><br><span class="line">ranker = TransformersSimilarityRanker(model=<span class="string">&quot;BAAI/bge-reranker-base&quot;</span>)</span><br></pre></td></tr></table></figure>
<br>
<h3 id="Extended-Method">Extended Method</h3>
<h4 id="假设性文档嵌入-Hypothetical-Document-Embedding-HDE">假设性文档嵌入 (Hypothetical Document Embedding - HDE)</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://docs.haystack.deepset.ai/v2.0/docs/hypothetical-document-embeddings-hyde">https://docs.haystack.deepset.ai/v2.0/docs/hypothetical-document-embeddings-hyde</a></p>
<p>许多嵌入检索器泛化到新的、未见过的领域效果不佳。这种方法试图解决这个问题。给定一个查询，假设文档嵌入（HyDE）首先使用指令跟随语言模型进行零样本提示，生成一个“假设”文档，该文档捕获初始查询中的相关文本模式——在实践中，这会重复五次。然后，它将每个假设文档编码为嵌入向量并取平均值。得到的单个嵌入可以用来在文档嵌入空间中识别一个邻域，根据向量相似性检索相似的实际文档。与其他检索器一样，这些检索到的文档可以随后在管道中下游使用（例如，在用于 RAG 的生成器中）。</p>
<p>Refer to the paper “<a target="_blank" rel="noopener" href="https://aclanthology.org/2023.acl-long.99/">Precise Zero-Shot Dense Retrieval without Relevance Labels</a>” for more details.</p>
<img src="/2025/04/21/AI-Agent/2d00628-Untitled_2.png" class="" title="Source: Original Paper, Gao et al, &lt;https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.acl-long.99&#x2F;&gt;">
<br>
<h4 id="迭代式检索生成-Iterative-Retrieval-Generation-IRG">迭代式检索生成 (Iterative Retrieval Generation - IRG)</h4>
<p><strong>核心思想：</strong><br>
将检索和生成过程从“一次性完成”的线性流程，转变为一个循环、迭代、逐步求精的过程。系统会根据初步生成的答案或中间思考，主动发现知识缺口，并生成新的查询以获取更多信息，直到能够形成一个完整、准确的答案为止。</p>
<p><strong>解决的问题：</strong><br>
它主要解决<strong>复杂、多步骤或需要整合多方面知识的查询 (Complex, Multi-hop Questions)</strong>。对于这类问题，单次检索往往无法获取所有必要的信息。例如，“比较一下《指环王》和《冰与火之歌》在世界构建和角色塑造上的异同？”</p>
<p><strong>工作原理：</strong></p>
<ol>
<li><strong>初始检索 (Initial Retrieval):</strong> 系统接收初始查询，并进行第一次检索，获取一批相关文档。</li>
<li><strong>初步生成与分析 (Initial Generation &amp; Analysis):</strong> LLM 基于第一次检索到的文档，尝试生成答案。在生成过程中，它会识别出当前信息中的不足之处。例如，它可能找到了关于《指环王》世界构建的资料，但缺少《冰与火之歌》角色塑造的细节。</li>
<li><strong>生成新查询 (Generate New Queries):</strong> 基于识别出的知识缺口，系统（或 LLM 本身）会生成一个或多个新的、更具针对性的查询。例如：“《冰与火之歌》中的主要角色塑造技巧”、“《指环王》的魔法系统设定”。</li>
<li><strong>迭代检索 (Iterative Retrieval):</strong> 系统使用这些新查询去知识库中进行新一轮的检索，获取补充信息。</li>
<li><strong>整合与精炼 (Integration &amp; Refinement):</strong> 将所有轮次检索到的文档进行整合，并交由 LLM 进行最终的综合性回答。</li>
<li><strong>循环终止：</strong> 当 LLM 判断信息已经足够全面，或者达到预设的迭代次数时，循环结束。</li>
</ol>
<p><strong>优势：</strong></p>
<ul>
<li><strong>处理复杂问题：</strong> 能够像人类研究员一样，逐步深入，解决需要多方面信息支持的复杂问题。</li>
<li><strong>提高答案的全面性和深度：</strong> 通过补充检索，确保答案覆盖了问题的所有方面。</li>
</ul>
<p><strong>挑战与考量：</strong></p>
<ul>
<li><strong>高延迟和高成本：</strong> 多次检索和生成步骤显著增加了响应时间和计算成本。</li>
<li><strong>逻辑复杂性：</strong> 实现一个有效的迭代循环、判断何时停止以及如何生成有效的后续查询，技术上更具挑战性。</li>
<li><strong>可能陷入无效循环：</strong> 如果后续查询生成得不好，可能会导致检索不到有用信息，陷入低效循环。</li>
</ul>
<br>
<h4 id="优化稠密加稀疏检索-Optimized-Dense-Sparse-Retrieval">优化稠密加稀疏检索 (Optimized Dense + Sparse Retrieval)</h4>
<p><strong>核心思想：</strong><br>
这是一种混合检索（Hybrid Retrieval）策略，它结合了两种主流检索技术的优点：<strong>稠密检索 (Dense Retrieval)</strong> 和 <strong>稀疏检索 (Sparse Retrieval)</strong>，以实现比单一技术更强大、更鲁棒的检索效果。</p>
<ul>
<li>
<p><strong>稠密检索 (Dense Retrieval):</strong></p>
<ul>
<li><strong>技术:</strong> 基于向量嵌入（Embeddings）和向量相似性搜索（如余弦相似度）。</li>
<li><strong>优点:</strong> 擅长理解<strong>语义和上下文</strong>。能够匹配意思相近但用词不同的查询和文档（例如，查询“美国总统官邸”能匹配到包含“白宫”的文档）。</li>
<li><strong>缺点:</strong> 对于<strong>关键词、专业术语、ID 或缩写</strong>的精确匹配能力较弱。</li>
</ul>
</li>
<li>
<p><strong>稀疏检索 (Sparse Retrieval):</strong></p>
<ul>
<li><strong>技术:</strong> 基于关键词频率和分布的传统信息检索方法（如 BM25、TF-IDF）。</li>
<li><strong>优点:</strong> 擅长<strong>精确匹配关键词</strong>。对于包含特定术语、产品型号（如“iPhone 15 Pro”）或代码标识符的查询，效果极佳。计算速度快，资源消耗低。</li>
<li><strong>缺点:</strong> 无法理解语义。无法匹配同义词或近义词（查询“汽车”无法匹配到包含“automobile”的文档）。</li>
</ul>
</li>
</ul>
<p><strong>工作原理 (优化结合):</strong></p>
<ol>
<li><strong>并行检索 (Parallel Retrieval):</strong> 当用户输入查询时，系统会<strong>同时</strong>使用两种方法进行检索：
<ul>
<li><strong>稠密路径：</strong> 将查询嵌入为向量，在向量数据库中搜索最相似的文档。</li>
<li><strong>稀疏路径：</strong> 对查询进行分词，在倒排索引（如 Elasticsearch）中使用 BM25 算法搜索最相关的文档。</li>
</ul>
</li>
<li><strong>结果融合 (Result Fusion):</strong> 系统会得到两个独立的、按相关性排序的文档列表。下一步是智能地将这两个列表融合成一个最终列表。最常用的融合算法是 <strong>“倒数排序融合” (Reciprocal Rank Fusion - RRF)</strong>。
<ul>
<li><strong>RRF 算法：</strong> 该算法不关心每个检索系统的原始分数（因为它们的量纲不同），只关心文档在各自列表中的<strong>排名 (Rank)</strong>。一个文档的最终分数是它在每个列表中排名的倒数之和。排名越靠前，倒数越大，最终分数也越高。</li>
<li><strong>公式:</strong> <code>RRF_Score(doc) = Σ (1 / (k + rank_i(doc)))</code>，其中 <code>rank_i(doc)</code> 是文档在第 <code>i</code> 个检索结果列表中的排名，<code>k</code> 是一个常数（通常设为60），用于降低低排名文档的影响。</li>
</ul>
</li>
<li><strong>最终排序与截断 (Final Ranking &amp; Truncation):</strong> 根据 RRF 分数对所有文档进行重新排序，并选取排名最高的 Top-K 个文档，用于后续的生成环节。</li>
</ol>
<p><strong>优势：</strong></p>
<ul>
<li><strong>取长补短：</strong> 结合了语义理解和关键词匹配的能力，是目前最有效的通用检索策略之一。</li>
<li><strong>鲁棒性强：</strong> 即使一种检索方法效果不佳，另一种方法仍可能召回相关的文档，大大降低了检索失败的风险。</li>
<li><strong>显著提升相关性：</strong> 在多数场景下，混合检索召回的文档质量远高于任何单一检索方法。</li>
</ul>
<p><strong>挑战与考量：</strong></p>
<ul>
<li><strong>系统复杂性：</strong> 需要同时维护向量数据库和关键词索引两种基础设施。</li>
<li><strong>调优挑战：</strong> 融合策略（如 RRF中的 <code>k</code> 值）或不同检索结果的权重需要进行实验和调优，以达到最佳效果。</li>
</ul>
<br>
<h3 id="Agent">Agent</h3>
<p>Agent 是 LangChain 中实现 LLM 推理和决策的核心模块，支持动态选择工具（Tool）来完成多步任务。</p>
<p>核心组件：</p>
<ul>
<li><strong>Agent</strong>：负责推理和调用工具</li>
<li><strong>Tool</strong>：工具函数，如 Web 搜索、数据库查询、Python 计算等</li>
<li><strong>LLM</strong>：驱动 Agent 的大脑</li>
<li><strong>AgentExecutor</strong>：执行器，负责管理 Agent 的执行逻辑</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import relevant functionality</span></span><br><span class="line"><span class="keyword">from</span> langchain_anthropic <span class="keyword">import</span> ChatAnthropic</span><br><span class="line"><span class="keyword">from</span> langchain_community.tools.tavily_search <span class="keyword">import</span> TavilySearchResults</span><br><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage</span><br><span class="line"><span class="keyword">from</span> langgraph.checkpoint.memory <span class="keyword">import</span> MemorySaver</span><br><span class="line"><span class="keyword">from</span> langgraph.prebuilt <span class="keyword">import</span> create_react_agent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the agent</span></span><br><span class="line">memory = MemorySaver()</span><br><span class="line">model = ChatAnthropic(model_name=<span class="string">&quot;claude-3-sonnet-20240229&quot;</span>)</span><br><span class="line">search = TavilySearchResults(max_results=<span class="number">2</span>)</span><br><span class="line">tools = [search]</span><br><span class="line">agent_executor = create_react_agent(model, tools, checkpointer=memory)</span><br></pre></td></tr></table></figure>
<br>
<h3 id="Tool">Tool</h3>
<p>ref: <a target="_blank" rel="noopener" href="https://www.anthropic.com/engineering/building-effective-agents">https://www.anthropic.com/engineering/building-effective-agents</a><br>
无论你正在构建哪种代理系统，工具很可能都是你代理的重要组成部分。通过在我们的 API 中指定其精确的结构和定义，工具使 Claude 能够与外部服务和 API 进行交互。当 Claude 响应时，如果它计划调用工具，API 响应中将包含一个工具使用块。工具定义和规范应该像你的整体提示一样得到同样多的提示工程关注。在这个简短的附录中，我们描述了如何对工具进行提示工程。</p>
<p>通常有几种方法可以指定同一操作。例如，你可以通过编写差异文件来指定文件编辑，或者通过重写整个文件。对于结构化输出，你可以在 markdown 中或 JSON 中返回代码。在软件工程中，这些差异是表面的，可以无损地从一种格式转换为另一种格式。然而，某些格式对 LLM 来说比其他格式更难编写。编写差异文件需要知道在新代码写入之前，块头中要更改多少行。在 JSON 中编写代码（与 markdown 相比）需要额外转义换行符和引号。</p>
<p>我们关于决定工具格式的建议如下：</p>
<ul>
<li>给模型足够的 token，让它有足够的时间思考，避免自己陷入困境。</li>
<li>保持模型在互联网上自然出现的文本格式。</li>
<li>确保没有格式化的“开销”，例如需要准确统计数千行代码，或对写入的任何代码进行字符串转义。</li>
</ul>
<p>一个经验法则是思考在人类计算机界面（HCI）上投入了多少精力，并计划在创建良好的代理计算机界面（ACI）上投入同等精力的投资。以下是一些关于如何做到这一点的想法：</p>
<ul>
<li>将自己代入模型的视角。根据描述和参数，使用这个工具是否显而易见，还是需要仔细思考？如果是这样，那么模型可能也是同样的情况。一个好的工具定义通常包括示例用法、边缘情况、输入格式要求以及与其他工具的清晰界限。</li>
<li>如何更改参数名称或描述，使事情更加清晰？把这看作是为团队里初级开发者编写出色的文档字符串。当使用许多类似的工具时，这一点尤其重要。</li>
<li>测试模型如何使用你的工具：在我们的工作台中运行许多示例输入，看看模型会犯什么错误，并进行迭代。</li>
<li>为你的工具设置防错机制。更改参数，使其更难出错。</li>
</ul>
<p>在为 SWE-bench 构建我们的智能体时，我们实际上花在优化工具上的时间比整体提示语更多。例如，我们发现当智能体离开根目录后，使用相对文件路径的工具会导致模型出错。为了解决这个问题，我们将工具改为始终要求绝对文件路径——我们发现模型使用这种方法非常完美。</p>
<br>
<h3 id="Graph">Graph</h3>
<p>Graph 由 nodes 和 edge 构成</p>
<p>state 为每个图的 context，在定义图时传入；</p>
<p>每个 <code>node</code> 都可以接收当前的 <code>State</code> 作为输入，并输出状态更新。</p>
<p>The <code>State</code> includes the graph’s schema and <a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers">reducer functions</a> that handle state updates.</p>
<br>
<p><strong>常用方法</strong></p>
<p><code>add_node</code></p>
<ul>
<li>
<p>通过 <code>.add_node(str, node)</code> 添加节点到图，第一个参数为唯一节点名，第二个参数为节点被使用时所调用的函数或对象；</p>
<blockquote>
<p>每个node相当于一个可执行函数，可以在函数内做任何事情</p>
</blockquote>
</li>
</ul>
<br>
<p><code>add_edge</code></p>
<ul>
<li>通过 <code>.add_edge(START, str)</code> 和 <code>.add_edge(node, str)</code> 添加 <code>entry</code> point 和 <code>finish</code> point，指名图的起点和终点，str 为节点名</li>
</ul>
<br>
<p><code>add_conditional_edges</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">add_conditional_edges(</span><br><span class="line">    source: <span class="built_in">str</span>,</span><br><span class="line">    path: <span class="type">Union</span>[</span><br><span class="line">        <span class="type">Callable</span>[..., <span class="type">Union</span>[Hashable, <span class="built_in">list</span>[Hashable]]],</span><br><span class="line">        <span class="type">Callable</span>[</span><br><span class="line">            ..., Awaitable[<span class="type">Union</span>[Hashable, <span class="built_in">list</span>[Hashable]]]</span><br><span class="line">        ],</span><br><span class="line">        Runnable[<span class="type">Any</span>, <span class="type">Union</span>[Hashable, <span class="built_in">list</span>[Hashable]]],</span><br><span class="line">    ],</span><br><span class="line">    path_map: <span class="type">Optional</span>[</span><br><span class="line">        <span class="type">Union</span>[<span class="built_in">dict</span>[Hashable, <span class="built_in">str</span>], <span class="built_in">list</span>[<span class="built_in">str</span>]]</span><br><span class="line">    ] = <span class="literal">None</span>,</span><br><span class="line">    then: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>,</span><br><span class="line">) -&gt; Self</span><br></pre></td></tr></table></figure>
<p>从起始节点添加条件边到任意数量的目标节点。</p>
<p><strong>Parameters: 参数：</strong></p>
<ul>
<li><code>source</code> （ <code>str</code> ）- 起始节点。当退出此节点时，将运行此条件边。</li>
<li><code>path</code> （ <code>Union[Callable, Runnable]</code> ）- 确定下一个节点或节点的可调用对象。如果不指定 <code>path_map</code> ，它应该返回一个或多个节点。如果返回 END，则图形将停止执行。</li>
<li><code>path_map</code> （ <code>Optional[dict[Hashable, str]]</code> ，默认： <code>None</code> ）- 可选的路径到节点名称的映射。如果省略，则 <code>path</code> 返回的路径应该是节点名称。</li>
<li><code>then</code> （ <code>Optional[str]</code> ，默认： <code>None</code> ）- 在 <code>path</code> 选择的节点之后执行节点的名称。</li>
</ul>
<p><strong>Returns: 返回值：</strong></p>
<ul>
<li><code>Self</code> （ <code>Self</code> ）- 图的实例，允许方法链式调用。</li>
</ul>
<br>
<p><code>interrupt</code></p>
<p>LangGraph 中的 <code>interrupt</code> 函数通过在特定节点暂停图、向人类展示信息，并使用他们的输入继续图来启用人类在循环中的工作流程。此功能适用于审批、编辑或收集额外输入等任务。 <code>interrupt</code> 函数与 <code>Command</code> 对象一起使用，以人类提供的值继续图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">human_approval</span>(<span class="params">state: State</span>) -&gt; Command[<span class="type">Literal</span>[<span class="string">&quot;some_node&quot;</span>, <span class="string">&quot;another_node&quot;</span>]]:</span><br><span class="line">    is_approved = interrupt(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;question&quot;</span>: <span class="string">&quot;Is this correct?&quot;</span>,</span><br><span class="line">            <span class="comment"># Surface the output that should be</span></span><br><span class="line">            <span class="comment"># reviewed and approved by the human.</span></span><br><span class="line">            <span class="string">&quot;llm_output&quot;</span>: state[<span class="string">&quot;llm_output&quot;</span>]</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_approved:</span><br><span class="line">        <span class="keyword">return</span> Command(goto=<span class="string">&quot;some_node&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> Command(goto=<span class="string">&quot;another_node&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the node to the graph in an appropriate location</span></span><br><span class="line"><span class="comment"># and connect it to the relevant nodes.</span></span><br><span class="line">graph_builder.add_node(<span class="string">&quot;human_approval&quot;</span>, human_approval)</span><br><span class="line">graph = graph_builder.<span class="built_in">compile</span>(checkpointer=checkpointer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># After running the graph and hitting the interrupt, the graph will pause.</span></span><br><span class="line"><span class="comment"># Resume it with either an approval or rejection.</span></span><br><span class="line">thread_config = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;thread_id&quot;</span>: <span class="string">&quot;some_id&quot;</span>&#125;&#125;</span><br><span class="line">graph.invoke(Command(resume=<span class="literal">True</span>), config=thread_config)</span><br></pre></td></tr></table></figure>
<br>
<p><strong>案例代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    messages: Annotated[<span class="built_in">list</span>, add_messages]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 工具：通过interrupt与人类交互</span></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="comment"># Note that because we are generating a ToolMessage for a state update, we</span></span><br><span class="line"><span class="comment"># generally require the ID of the corresponding tool call. We can use</span></span><br><span class="line"><span class="comment"># LangChain&#x27;s InjectedToolCallId to signal that this argument should not</span></span><br><span class="line"><span class="comment"># be revealed to the model in the tool&#x27;s schema.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">human_assistance</span>(<span class="params"></span></span><br><span class="line"><span class="params">    name: <span class="built_in">str</span>, birthday: <span class="built_in">str</span>, tool_call_id: Annotated[<span class="built_in">str</span>, InjectedToolCallId]</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Request assistance from a human.&quot;&quot;&quot;</span></span><br><span class="line">    human_response = interrupt(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;question&quot;</span>: <span class="string">&quot;Is this correct?&quot;</span>,</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: name,</span><br><span class="line">            <span class="string">&quot;birthday&quot;</span>: birthday,</span><br><span class="line">        &#125;,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># If the information is correct, update the state as-is.</span></span><br><span class="line">    <span class="keyword">if</span> human_response.get(<span class="string">&quot;correct&quot;</span>, <span class="string">&quot;&quot;</span>).lower().startswith(<span class="string">&quot;y&quot;</span>):</span><br><span class="line">        verified_name = name</span><br><span class="line">        verified_birthday = birthday</span><br><span class="line">        response = <span class="string">&quot;Correct&quot;</span></span><br><span class="line">    <span class="comment"># Otherwise, receive information from the human reviewer.</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        verified_name = human_response.get(<span class="string">&quot;name&quot;</span>, name)</span><br><span class="line">        verified_birthday = human_response.get(<span class="string">&quot;birthday&quot;</span>, birthday)</span><br><span class="line">        response = <span class="string">f&quot;Made a correction: <span class="subst">&#123;human_response&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># This time we explicitly update the state with a ToolMessage inside</span></span><br><span class="line">    <span class="comment"># the tool.</span></span><br><span class="line">    state_update = &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: verified_name,</span><br><span class="line">        <span class="string">&quot;birthday&quot;</span>: verified_birthday,</span><br><span class="line">        <span class="string">&quot;messages&quot;</span>: [ToolMessage(response, tool_call_id=tool_call_id)],</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># We return a Command object in the tool to update our state.</span></span><br><span class="line">    <span class="keyword">return</span> Command(update=state_update)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Modification: tell the LLM which tools it can call</span></span><br><span class="line">tool = TavilySearchResults(max_results=<span class="number">2</span>)</span><br><span class="line">tools = [tool, human_assistance]</span><br><span class="line"></span><br><span class="line">llm = ChatAnthropic(model=<span class="string">&quot;claude-3-5-sonnet-20240620&quot;</span>)</span><br><span class="line">llm_with_tools = llm.bind_tools(tools)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chatbot</span>(<span class="params">state: State</span>):</span><br><span class="line">    message = llm_with_tools.invoke(state[<span class="string">&quot;messages&quot;</span>])</span><br><span class="line">    <span class="comment"># Because we will be interrupting during tool execution,</span></span><br><span class="line">    <span class="comment"># we disable parallel tool calling to avoid repeating any</span></span><br><span class="line">    <span class="comment"># tool invocations when we resume.</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(message.tool_calls) &lt;= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: [llm_with_tools.invoke(state[<span class="string">&quot;messages&quot;</span>])]&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">graph_builder = StateGraph(State)</span><br><span class="line">graph_builder.add_node(<span class="string">&quot;chatbot&quot;</span>, chatbot)</span><br><span class="line"></span><br><span class="line">tool_node = ToolNode(tools=[tool])</span><br><span class="line">graph_builder.add_node(<span class="string">&quot;tools&quot;</span>, tool_node)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">route_tools</span>(<span class="params"></span></span><br><span class="line"><span class="params">    state: State,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Use in the conditional_edge to route to the ToolNode if the last message</span></span><br><span class="line"><span class="string">    has tool calls. Otherwise, route to the end.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(state, <span class="built_in">list</span>):</span><br><span class="line">        ai_message = state[-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">elif</span> messages := state.get(<span class="string">&quot;messages&quot;</span>, []):</span><br><span class="line">        ai_message = messages[-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">f&quot;No messages found in input state to tool_edge: <span class="subst">&#123;state&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(ai_message, <span class="string">&quot;tool_calls&quot;</span>) <span class="keyword">and</span> <span class="built_in">len</span>(ai_message.tool_calls) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;tools&quot;</span>  <span class="comment"># path 函数返回的为节点名称</span></span><br><span class="line">    <span class="keyword">return</span> END  <span class="comment"># path 函数返回的为节点名称</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># The `tools_condition` function returns &quot;tools&quot; if the chatbot asks to use a tool, and &quot;END&quot; if</span></span><br><span class="line"><span class="comment"># it is fine directly responding. This conditional routing defines the main agent loop.</span></span><br><span class="line">graph_builder.add_conditional_edges(</span><br><span class="line">    <span class="string">&quot;chatbot&quot;</span>,</span><br><span class="line">    route_tools,</span><br><span class="line">    <span class="comment"># The following dictionary lets you tell the graph to interpret the condition&#x27;s outputs as a specific node</span></span><br><span class="line">    <span class="comment"># It defaults to the identity function, but if you</span></span><br><span class="line">    <span class="comment"># want to use a node named something else apart from &quot;tools&quot;,</span></span><br><span class="line">    <span class="comment"># You can update the value of the dictionary to something else</span></span><br><span class="line">    <span class="comment"># e.g., &quot;tools&quot;: &quot;my_tools&quot;</span></span><br><span class="line">    <span class="comment"># 可选的路径到节点名称的映射。如果省略，则 `path` 返回的路径应该是节点名称。</span></span><br><span class="line">    &#123;<span class="string">&quot;tools&quot;</span>: <span class="string">&quot;tools&quot;</span>, END: END&#125;,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># Any time a tool is called, we return to the chatbot to decide the next step</span></span><br><span class="line">graph_builder.add_edge(<span class="string">&quot;tools&quot;</span>, <span class="string">&quot;chatbot&quot;</span>)</span><br><span class="line">graph_builder.add_edge(START, <span class="string">&quot;chatbot&quot;</span>)</span><br><span class="line"><span class="comment"># graph = graph_builder.compile()</span></span><br><span class="line">memory = MemorySaver()  <span class="comment"># 添加检查点</span></span><br><span class="line">graph = graph_builder.<span class="built_in">compile</span>(checkpointer=memory)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">config = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;thread_id&quot;</span>: <span class="string">&quot;1&quot;</span>&#125;&#125;</span><br><span class="line">events = graph.stream(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;messages&quot;</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">                <span class="string">&quot;content&quot;</span>: (</span><br><span class="line">                    <span class="string">&quot;I&#x27;m learning LangGraph. &quot;</span></span><br><span class="line">                    <span class="string">&quot;Could you do some research on it for me?&quot;</span></span><br><span class="line">                ),</span><br><span class="line">            &#125;,</span><br><span class="line">        ],</span><br><span class="line">    &#125;,</span><br><span class="line">    config,</span><br><span class="line">    stream_mode=<span class="string">&quot;values&quot;</span>,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">for</span> event <span class="keyword">in</span> events:</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;messages&quot;</span> <span class="keyword">in</span> event:</span><br><span class="line">        event[<span class="string">&quot;messages&quot;</span>][-<span class="number">1</span>].pretty_print()</span><br></pre></td></tr></table></figure>
<br>
<h2 id="Workflow">Workflow</h2>
<p>ref: <a target="_blank" rel="noopener" href="https://www.anthropic.com/engineering/building-effective-agents">https://www.anthropic.com/engineering/building-effective-agents</a></p>
<ul>
<li><strong>Workflows</strong> are systems where LLMs and tools are orchestrated through predefined code paths.<br>
工作流是 LLMs 和工具通过预定义的代码路径进行编排的系统。</li>
<li><strong>Agents</strong>, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.<br>
另一方面，智能体是 LLMs 能够动态指导自身进程和工具使用，并保持对完成任务方式的控制的系统。</li>
</ul>
<h3 id="Gate">Gate</h3>
<img src="/2025/04/21/AI-Agent/image-20250909141156823.png" class="" title="image-20250909141156823">
<br>
<h3 id="Routing">Routing</h3>
<img src="/2025/04/21/AI-Agent/image-20250909141213294.png" class="" title="image-20250909141213294">
<br>
<h3 id="Parallelization">Parallelization</h3>
<img src="/2025/04/21/AI-Agent/image-20250909141229088.png" class="" title="image-20250909141229088">
<br>
<h3 id="Orchestrator-workers">Orchestrator-workers</h3>
<img src="/2025/04/21/AI-Agent/image-20250909141244188.png" class="" title="image-20250909141244188">
<br>
<h3 id="Evaluator-optimizer">Evaluator-optimizer</h3>
<img src="/2025/04/21/AI-Agent/image-20250909141257247.png" class="" title="image-20250909141257247">
<br>
<h2 id="Agent-2">Agent</h2>
<p>ref: <a target="_blank" rel="noopener" href="https://x.com/DailyDoseOfDS_/status/1940342200216482211">https://x.com/DailyDoseOfDS_/status/1940342200216482211</a></p>
<img src="/2025/04/21/AI-Agent/image-20250810092757168.png" class="" title="image-20250810092757168">
<h3 id="Design-Pattern">Design Pattern</h3>
<img src="/2025/04/21/AI-Agent/5.png" class="" title="alt text">
<ul>
<li>Short-term memory: 上下文Context</li>
<li>Long-term memory: 外挂数据库，例如RAG技术</li>
<li>Tool: MCP(function call…)</li>
<li>Planning: Reflection，Self-critics…</li>
</ul>
<br>
<h3 id="Frame">Frame</h3>
<p>ref: <a target="_blank" rel="noopener" href="https://fcnisnh9uh54.feishu.cn/wiki/ATmlw7H4OiXwMBkJ0ZTc2cyWnph">https://fcnisnh9uh54.feishu.cn/wiki/ATmlw7H4OiXwMBkJ0ZTc2cyWnph</a></p>
<h4 id="ReAct">ReAct</h4>
<br>
<h4 id="Multi-Agent">Multi-Agent</h4>
<br>
<h4 id="Tool-Integrated-Reasoning">Tool-Integrated Reasoning</h4>
<br>
<h3 id="Paper-Rela">Paper Rela</h3>
<h4 id="Reflection">Reflection</h4>
<ul>
<li>“[Self-Refine: Iterative Refinement with Self-Feedback](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.17651?utm_campaign=The">https://arxiv.org/abs/2303.17651?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B),” Madaan et al. (2023)</li>
<li>“[Reflexion: Language Agents with Verbal Reinforcement Learning](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.11366?utm_campaign=The">https://arxiv.org/abs/2303.11366?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B),” Shinn et al. (2023)</li>
<li>“[CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.11738?utm_campaign=The">https://arxiv.org/abs/2305.11738?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B),” Gou et al. (2024)</li>
</ul>
<br>
<h4 id="Tool-Use">Tool Use</h4>
<ul>
<li>“[Gorilla: Large Language Model Connected with Massive APIs](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.15334?utm_campaign=The">https://arxiv.org/abs/2305.15334?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz–9ARMthd09q0ABUi-abo6BH62BLbcwPo13LrXs9hUezs-L050Ay7b_rHdWuRIqBVOD6k_S),” Patil et al. (2023)</li>
<li>“[MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.11381?utm_campaign=The">https://arxiv.org/abs/2303.11381?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz–9ARMthd09q0ABUi-abo6BH62BLbcwPo13LrXs9hUezs-L050Ay7b_rHdWuRIqBVOD6k_S),” Yang et al. (2023)</li>
<li>“[Efficient Tool Use with Chain-of-Abstraction Reasoning](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.17464?utm_campaign=The">https://arxiv.org/abs/2401.17464?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz–9ARMthd09q0ABUi-abo6BH62BLbcwPo13LrXs9hUezs-L050Ay7b_rHdWuRIqBVOD6k_S),” Gao et al. (2024)</li>
</ul>
<br>
<h4 id="Planning">Planning</h4>
<ul>
<li>“[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.11903?utm_campaign=The">https://arxiv.org/abs/2201.11903?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8Kh954rkXmE4vgpKvro3Klpjhn7IuT-Y_eXIYtgVIq9PTzwa5zFWX7FZZqv1tuDEEsTDuY),” Wei et al. (2022)</li>
<li>“[HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.17580?utm_campaign=The">https://arxiv.org/abs/2303.17580?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8Kh954rkXmE4vgpKvro3Klpjhn7IuT-Y_eXIYtgVIq9PTzwa5zFWX7FZZqv1tuDEEsTDuY),” Shen et al. (2023)</li>
<li>“[Understanding the planning of LLM agents: A survey](<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.02716.pdf?utm_campaign=The">https://arxiv.org/pdf/2402.02716.pdf?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8Kh954rkXmE4vgpKvro3Klpjhn7IuT-Y_eXIYtgVIq9PTzwa5zFWX7FZZqv1tuDEEsTDuY),” by Huang et al. (2024)</li>
</ul>
<br>
<h4 id="Multi-Agent-Collaboration">Multi-Agent Collaboration</h4>
<ul>
<li>“[Communicative Agents for Software Development](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.07924?utm_campaign=The">https://arxiv.org/abs/2307.07924?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8TZzur2df1qdnGx09b-Fg94DTsc3-xXao4StKvKNU2HR51el3n8yOm0CPSw6GiAoLQNKua),” Qian et al. (2023) (the ChatDev paper)</li>
<li>“[AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.08155?utm_campaign=The">https://arxiv.org/abs/2308.08155?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8TZzur2df1qdnGx09b-Fg94DTsc3-xXao4StKvKNU2HR51el3n8yOm0CPSw6GiAoLQNKua),” Wu et al. (2023)</li>
<li>“[MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.00352?utm_campaign=The">https://arxiv.org/abs/2308.00352?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8TZzur2df1qdnGx09b-Fg94DTsc3-xXao4StKvKNU2HR51el3n8yOm0CPSw6GiAoLQNKua),” Hong et al. (2023)</li>
</ul>
<br>
<h2 id="MCP">MCP</h2>
<p>Model Context Protocol</p>
<p>ref: <a target="_blank" rel="noopener" href="https://modelcontextprotocol.io/introduction">https://modelcontextprotocol.io/introduction</a></p>
<p>ref: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29001189476">https://zhuanlan.zhihu.com/p/29001189476</a></p>
<img src="/2025/04/21/AI-Agent/v2-2bcd98f6541da0b6f14dc9082ee2dcda_1440w.jpg" class="" title="mcp">
<br>
<p><a target="_blank" rel="noopener" href="https://github.com/modelcontextprotocol/python-sdk/tree/main/examples/clients/simple-chatbot/mcp_simple_chatbot">example</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># client </span></span><br><span class="line">   ... <span class="comment"># 省略了无关的代码</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">start</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># 初始化所有的 mcp server</span></span><br><span class="line">    <span class="keyword">for</span> server <span class="keyword">in</span> self.servers:</span><br><span class="line">        <span class="keyword">await</span> server.initialize()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取所有的 tools 命名为 all_tools</span></span><br><span class="line">    all_tools = []</span><br><span class="line">    <span class="keyword">for</span> server <span class="keyword">in</span> self.servers:</span><br><span class="line">        tools = <span class="keyword">await</span> server.list_tools()</span><br><span class="line">        all_tools.extend(tools)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将所有的 tools 的功能描述格式化成字符串供 LLM 使用</span></span><br><span class="line">    <span class="comment"># tool.format_for_llm() 我放到了这段代码最后，方便阅读。</span></span><br><span class="line">    tools_description = <span class="string">&quot;\n&quot;</span>.join(</span><br><span class="line">        [tool.format_for_llm() <span class="keyword">for</span> tool <span class="keyword">in</span> all_tools]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里就不简化了，以供参考，实际上就是基于 prompt 和当前所有工具的信息</span></span><br><span class="line">    <span class="comment"># 询问 LLM（Claude） 应该使用哪些工具。</span></span><br><span class="line">    system_message = (</span><br><span class="line">        <span class="string">&quot;You are a helpful assistant with access to these tools:\n\n&quot;</span></span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;tools_description&#125;</span>\n&quot;</span></span><br><span class="line">        <span class="string">&quot;Choose the appropriate tool based on the user&#x27;s question. &quot;</span></span><br><span class="line">        <span class="string">&quot;If no tool is needed, reply directly.\n\n&quot;</span></span><br><span class="line">        <span class="string">&quot;IMPORTANT: When you need to use a tool, you must ONLY respond with &quot;</span></span><br><span class="line">        <span class="string">&quot;the exact JSON object format below, nothing else:\n&quot;</span></span><br><span class="line">        <span class="string">&quot;&#123;\n&quot;</span></span><br><span class="line">        <span class="string">&#x27;    &quot;tool&quot;: &quot;tool-name&quot;,\n&#x27;</span></span><br><span class="line">        <span class="string">&#x27;    &quot;arguments&quot;: &#123;\n&#x27;</span></span><br><span class="line">        <span class="string">&#x27;        &quot;argument-name&quot;: &quot;value&quot;\n&#x27;</span></span><br><span class="line">        <span class="string">&quot;    &#125;\n&quot;</span></span><br><span class="line">        <span class="string">&quot;&#125;\n\n&quot;</span></span><br><span class="line">        <span class="string">&quot;After receiving a tool&#x27;s response:\n&quot;</span></span><br><span class="line">        <span class="string">&quot;1. Transform the raw data into a natural, conversational response\n&quot;</span></span><br><span class="line">        <span class="string">&quot;2. Keep responses concise but informative\n&quot;</span></span><br><span class="line">        <span class="string">&quot;3. Focus on the most relevant information\n&quot;</span></span><br><span class="line">        <span class="string">&quot;4. Use appropriate context from the user&#x27;s question\n&quot;</span></span><br><span class="line">        <span class="string">&quot;5. Avoid simply repeating the raw data\n\n&quot;</span></span><br><span class="line">        <span class="string">&quot;Please use only the tools that are explicitly defined above.&quot;</span></span><br><span class="line">    )</span><br><span class="line">    messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: system_message&#125;]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># Final... 假设这里已经处理了用户消息输入.</span></span><br><span class="line">        messages.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: user_input&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 system_message 和用户消息输入一起发送给 LLM</span></span><br><span class="line">        llm_response = self.llm_client.get_response(messages)</span><br><span class="line"></span><br><span class="line">    ... <span class="comment"># 后面和确定使用哪些工具无关</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># server</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tool</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Represents a tool with its properties and formatting.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, name: <span class="built_in">str</span>, description: <span class="built_in">str</span>, input_schema: <span class="built_in">dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.name: <span class="built_in">str</span> = name</span><br><span class="line">        self.description: <span class="built_in">str</span> = description</span><br><span class="line">        self.input_schema: <span class="built_in">dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>] = input_schema</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把工具的名字 / 工具的用途（description）和工具所需要的参数（args_desc）转化为文本</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">format_for_llm</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Format tool information for LLM.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            A formatted string describing the tool.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        args_desc = []</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;properties&quot;</span> <span class="keyword">in</span> self.input_schema:</span><br><span class="line">            <span class="keyword">for</span> param_name, param_info <span class="keyword">in</span> self.input_schema[<span class="string">&quot;properties&quot;</span>].items():</span><br><span class="line">                arg_desc = (</span><br><span class="line">                    <span class="string">f&quot;- <span class="subst">&#123;param_name&#125;</span>: <span class="subst">&#123;param_info.get(<span class="string">&#x27;description&#x27;</span>, <span class="string">&#x27;No description&#x27;</span>)&#125;</span>&quot;</span></span><br><span class="line">                )</span><br><span class="line">                <span class="keyword">if</span> param_name <span class="keyword">in</span> self.input_schema.get(<span class="string">&quot;required&quot;</span>, []):</span><br><span class="line">                    arg_desc += <span class="string">&quot; (required)&quot;</span></span><br><span class="line">                args_desc.append(arg_desc)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Tool: <span class="subst">&#123;self.name&#125;</span></span></span><br><span class="line"><span class="string">Description: <span class="subst">&#123;self.description&#125;</span></span></span><br><span class="line"><span class="string">Arguments:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;<span class="built_in">chr</span>(<span class="number">10</span>).join(args_desc)&#125;</span></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<br>
<h2 id="KBQA">KBQA</h2>
<p>Knowledge Based Question Answering (KBQA) aims to answer factual questions based on the provided knowledge base (KB).</p>
<p>ref: <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Awesome-KBQA">https://github.com/RUCAIBox/Awesome-KBQA</a></p>
<br>
<h3 id="Semantic-Parsing-based-Methods">Semantic Parsing-based Methods</h3>
<br>
<h3 id="Information-retrieval-based-Methods">Information retrieval-based Methods</h3>
<br>
<h3 id="Other-Methods">Other Methods</h3>
<br>
<h2 id="Improved-RAG">Improved RAG</h2>
<h3 id="GraphRAG">GraphRAG</h3>
<p>ref: <a target="_blank" rel="noopener" href="https://x.com/DailyDoseOfDS_/status/1941429358620766553">https://x.com/DailyDoseOfDS_/status/1941429358620766553</a></p>
<img src="/2025/04/21/AI-Agent/image-20250810092707204.png" class="" title="image-20250810092707204">
<p><a target="_blank" rel="noopener" href="https://microsoft.github.io/graphrag/">https://microsoft.github.io/graphrag/</a></p>
<ol>
<li>
<p>Source Documents → Text Chunks</p>
<p>To start, the documents in the corpus are split into text chunks. The LLM extracts information from each chunk for downstream processing.</p>
</li>
<li>
<p>Text Chunks → Entities &amp; Relationships</p>
<p>the LLM is prompted（throught few-shot prompt） to extract instances of important entities and the relationships between the entities from a given chunk. Additionally, the LLM generates short descriptions for the entities and relationships</p>
<img src="/2025/04/21/AI-Agent/image-20250604165407470.png" class="" title="image-20250604165407470">
<p>The LLM can also be prompted to extract claims about detected entities. Claims are important factual statements about entities, such as dates, events, and interactions with other entities. As with entities and relationships, in-context learning exemplars can provide domain-specific guidance.</p>
<img src="/2025/04/21/AI-Agent/image-20250604170724823.png" class="" title="image-20250604170724823">
</li>
<li>
<p>Entities &amp; Relationships → Knowledge Graph</p>
<p>In the final step of the knowledge graph extraction process, these instances of entities and relationships become individual nodes and edges in the graph. Entity descriptions are aggregated and summarized for each node and edge. Relationships are aggregated into graph edges, where the number of duplicates for a given relationship becomes edge weights. Claims are aggregated similarly</p>
</li>
<li>
<p>Knowledge Graph → Graph Communities</p>
<p>we use Leiden community detection (Traag et al., 2019) in a hierarchical manner, recursively detecting sub-communities within each detected community until reaching leaf communities that can no longer be partitioned. Each level of this hierarchy provides a community partition that covers the nodes of the graph in a mutually exclusive, collectively exhaustive way, enabling divide-and-conquer global summarization.</p>
</li>
<li>
<p>Graph Communities → Community Summaries</p>
<img src="/2025/04/21/AI-Agent/image-20250604173158175.png" class="" title="image-20250604173158175">
</li>
<li>
<p>Community Summaries → Community Answers → Global Answer</p>
<img src="/2025/04/21/AI-Agent/image-20250604173209083.png" class="" title="image-20250604173209083">
</li>
</ol>
<h3 id="LightRAG">LightRAG</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.05779">arxiv.org/abs/2410.05779</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/HKUDS/LightRAG">https://github.com/HKUDS/LightRAG</a></p>
<img src="/2025/04/21/AI-Agent/b2aaf634151b4706892693ffb43d9093.png" class="" title="LightRAG Diagram">
<h2 id="Prompt-Engineering">Prompt Engineering</h2>
<p>ref: <a target="_blank" rel="noopener" href="https://dannyzheng.me/2025/02/21/prompt-engineering/#the-prompt-engineering-lifecycle">https://dannyzheng.me/2025/02/21/prompt-engineering/#the-prompt-engineering-lifecycle</a></p>
<img src="/2025/04/21/AI-Agent/prompt_components.png" class="" title="img">
<h3 id="Base-2">Base</h3>
<h4 id="Zero-Shot-Prompting">Zero-Shot Prompting</h4>
<p>Zero-Shot Prompting 是指在不给模型任何示例的情况下，直接提出问题或任务。该方法依赖于模型的预训练知识，适用于模型已广泛学习相关领域信息的情况。例如：</p>
<blockquote>
<p><strong>Prompt</strong>：将下面这句话翻译成英文：我喜欢学习人工智能。<br>
<strong>Output</strong>：I like studying artificial intelligence.</p>
</blockquote>
<p>该方法简单高效，适用于任务明确、模型已具备相关背景知识的场景。</p>
<br>
<h4 id="Few-Shot-Prompting">Few-Shot Prompting</h4>
<p>Few-Shot Prompting 是在提示中提供少量（通常是1-5个）示例，以帮助模型理解任务格式或逻辑。这种方式可以显著提升模型在结构化任务中的表现。例如：</p>
<blockquote>
<p><strong>Prompt</strong>：<br>
翻译下列句子：<br>
例1：我爱编程。 → I love programming.<br>
例2：天气很好。 → The weather is nice.<br>
请翻译：我在看书。<br>
<strong>Output</strong>：I am reading a book.</p>
</blockquote>
<p>Few-Shot Prompting 利用“类比”方式，帮助模型对任务形成更明确的理解。</p>
<br>
<h4 id="Chain-of-Thought-CoT-Prompting">Chain-of-Thought (CoT) Prompting</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://dannyzheng.me/2025/02/21/prompt-engineering/#the-prompt-engineering-lifecycle">https://dannyzheng.me/2025/02/21/prompt-engineering/#the-prompt-engineering-lifecycle</a></p>
<blockquote>
<p>CoT tip: Always have LLM output its thinking. Without outputting its thought process, no thinking occurs!</p>
</blockquote>
<ul>
<li>
<p>Basic prompt: Include “Think step-by-step” in your prompt.</p>
<ul>
<li>
<p>Lacks guidance on how to think (which is especially not ideal if a task is very specific to your app, use case, or organization)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Draft personalized emails to donors asking for contributions to this year’s Care for Kids program.</span><br><span class="line"></span><br><span class="line">Program information:</span><br><span class="line">&lt;program&gt;&#123;&#123;PROGRAM_DETAILS&#125;&#125;</span><br><span class="line">&lt;/program&gt;</span><br><span class="line"></span><br><span class="line">Donor information:</span><br><span class="line">&lt;donor&gt;&#123;&#123;DONOR_DETAILS&#125;&#125;</span><br><span class="line">&lt;/donor&gt;</span><br><span class="line"></span><br><span class="line">Think step-by-step before you write the email.</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>Guided prompt: Outline specific steps for LLM to follow in its thinking process.</p>
<ul>
<li>
<p>Lacks structuring to make it easy to strip out and separate the answer from the thinking.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Draft personalized emails to donors asking for contributions to this year’s Care for Kids program.</span><br><span class="line"></span><br><span class="line">Program information:</span><br><span class="line">&lt;program&gt;&#123;&#123;PROGRAM_DETAILS&#125;&#125;</span><br><span class="line">&lt;/program&gt;</span><br><span class="line"></span><br><span class="line">Donor information:</span><br><span class="line">&lt;donor&gt;&#123;&#123;DONOR_DETAILS&#125;&#125;</span><br><span class="line">&lt;/donor&gt;</span><br><span class="line"></span><br><span class="line">Think before you write the email. First, think through what messaging might appeal to this donor given their donation history and which campaigns they’ve supported in the past. Then, think through what aspects of the Care for Kids program would appeal to them, given their history. Finally, write the personalized donor email using your analysis.</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>Structured prompt: Use XML tags like &lt;think&gt; &lt;/think&gt; and &lt;answer&gt; &lt;/answer&gt; to separate reasoning from the final answer.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Draft personalized emails to donors asking for contributions to this year’s Care for Kids program.</span><br><span class="line"></span><br><span class="line">Program information:</span><br><span class="line">&lt;program&gt;&#123;&#123;PROGRAM_DETAILS&#125;&#125;</span><br><span class="line">&lt;/program&gt;</span><br><span class="line"></span><br><span class="line">Donor information:</span><br><span class="line">&lt;donor&gt;&#123;&#123;DONOR_DETAILS&#125;&#125;</span><br><span class="line">&lt;/donor&gt;</span><br><span class="line"></span><br><span class="line">Think before you write the email in &lt;thinking&gt; tags. First, think through what messaging might appeal to this donor given their donation history and which campaigns they’ve supported in the past. Then, think through what aspects of the Care for Kids program would appeal to them, given their history. Finally, write the personalized donor email in &lt;email&gt; tags, using your analysis.</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Use code (e.g. extract from <code>&lt;answer&gt;</code> tags) to extract the desired answer from the LLM’s response</p>
</blockquote>
</li>
</ul>
<br>
<h4 id="Self-Consistency-Prompting">Self-Consistency Prompting</h4>
<p>Self-Consistency Prompting 是 Chain-of-Thought 的一种增强策略。其核心思想是：在面对复杂推理问题时，通过生成多个不同的推理路径（多次生成），然后对这些路径的最终结果进行投票或汇总，以提高答案的可靠性和稳定性。</p>
<p>例如，对于一道数学题，模型可能在不同尝试中给出不同的思路或答案，但通过“多数投票”可以获得更一致、准确的结果。</p>
<blockquote>
<p><strong>应用示例</strong>：</p>
<ul>
<li>生成5条不同的推理路径</li>
<li>汇总5个最终答案</li>
<li>选择出现频率最高的一个作为最终输出</li>
</ul>
</blockquote>
<p>这种方法特别适合对单次输出不够稳定的任务，比如复杂逻辑、数学题等场景。</p>
<br>
<h4 id="Retrieval-Augmented-Generation-RAG">Retrieval-Augmented Generation (RAG)</h4>
<p>RAG 是将外部知识检索机制（如文档、数据库、搜索引擎）与语言模型生成能力结合起来的一种方法。模型在回答问题前，会先“检索”相关内容，再基于检索结果生成答案。</p>
<p>这种方法克服了大模型“记忆有限”的问题，尤其在处理需要时效性或特定背景知识的任务时非常有效。</p>
<blockquote>
<p><strong>示例流程</strong>：</p>
<ol>
<li>用户提问：“请解释什么是量子纠缠？”</li>
<li>模型调用检索模块，从知识库或网络中找到高相关资料</li>
<li>基于资料，生成符合上下文、准确可靠的回答</li>
</ol>
</blockquote>
<p>RAG 适用于问答系统、知识密集型对话系统、企业内部知识库应用等场景。</p>
<br>
<h3 id="Medprompt">Medprompt</h3>
<p>MedPrompt is composed of the following prompting techniques:</p>
<ul>
<li>Dynamic few-shot selection: instead of using static few-shot examples, Medprompt selects few-shot examples dynamically based on the question.</li>
<li>Self-generated chain of thought.</li>
<li>Choice shuffle ensembling: performs choice shuffle and self-consistency prompting.</li>
</ul>
<img src="/2025/04/21/AI-Agent/Medqa-comp.png" class="" title="img">
<h3 id="Prompt-优化">Prompt 优化</h3>
<p>ref: <a target="_blank" rel="noopener" href="https://www.comet.com/docs/opik/evaluation/metrics/heuristic_metrics#levenshteinratio">https://www.comet.com/docs/opik/evaluation/metrics/heuristic_metrics#levenshteinratio</a></p>
<blockquote>
<p>莱文斯坦距离，又称编辑距离（Edit Distance），是指两个字符串之间，由一个转换成另一个所需的最少单字符编辑操作次数。这些操作包括：</p>
<ul>
<li><strong>插入 (Insertion):</strong> 在一个字符串中插入一个字符。</li>
<li><strong>删除 (Deletion):</strong> 从一个字符串中删除一个字符。</li>
<li><strong>替换 (Substitution):</strong> 将一个字符串中的一个字符替换为另一个字符。</li>
</ul>
<p>这个距离通常使用动态规划的方法来计算。</p>
<p>莱文斯坦比率 (Levenshtein Ratio)</p>
<p>LevenshteinRatio 并不是直接使用编辑距离，而是将其转换成一个更直观的相似度比率。其计算公式通常为：</p>
<p><strong>ratio = (len_sum - ldist) / len_sum</strong></p>
<p>其中：</p>
<ul>
<li>len_sum 是两个待比较字符串的长度之和。</li>
<li>ldist 是两个字符串之间的莱文斯坦距离。</li>
</ul>
<p><strong>一个重要的实现细节是：</strong> 在计算用于比率的莱文斯坦距离时，不同的操作权重可能不同。在一些常见的 Python 库（如 python-Levenshtein）中，为了更合理地计算比率，<strong>替换操作的权重被视为2</strong>，而插入和删除的权重为1。 这样做是为了处理像 ‘a’ 和 ‘c’ 这样完全不相关的单个字符的替换，使其相似度为0。</p>
</blockquote>
<p>使用测试集评估当前prompt在测试集上的分数</p>
<p>可用量化手段</p>
<ul>
<li>LLM-as-Judgement</li>
<li>Levenshtein Ratio</li>
</ul>
<p>使用LLM迭代优化Prompt</p>
<br>
<h2 id="其他">其他</h2>
<h3 id="workflow实现微调数据集生成">workflow实现微调数据集生成</h3>
<p>chunk分块方案参考上文</p>
<p><strong>问题创建&amp;问题求解</strong>: 针对于分割好的文本块，通过LLM为其生成对应问题</p>
<br>
<h3 id="幻觉处理">幻觉处理</h3>
<p><a target="_blank" rel="noopener" href="https://cvs-health.github.io/uqlm/latest/index.html">https://cvs-health.github.io/uqlm/latest/index.html</a></p>
<p>黑盒评分器</p>
<img src="/2025/04/21/AI-Agent/black_box_graphic.png" class="" title="_images&#x2F;black_box_graphic.png">
<p>白盒评分器</p>
<img src="/2025/04/21/AI-Agent/white_box_graphic.png" class="" title="_images&#x2F;white_box_graphic.png">
<p>LLM评分</p>
<img src="/2025/04/21/AI-Agent/judges_graphic.png" class="" title="_images&#x2F;judges_graphic.png">
<p>集成评分器</p>
<img src="/2025/04/21/AI-Agent/uqensemble_generate_score.png" class="" title="_images&#x2F;uqensemble_generate_score.png">
<br>
<h3 id="RAG实践">RAG实践</h3>
<p><a target="_blank" rel="noopener" href="https://holistic-authority-5c2.notion.site/RAG-1e3f66b636ba80f1b425dc15d7fae04c">https://holistic-authority-5c2.notion.site/RAG-1e3f66b636ba80f1b425dc15d7fae04c</a></p>
<p>&lt;br</p>
<h3 id="GPT-记忆实现">GPT 记忆实现</h3>
<p>ref: <a target="_blank" rel="noopener" href="https://macro.com/app/md/54115a42-3409-4f5b-9120-f144d3ecd23a">https://macro.com/app/md/54115a42-3409-4f5b-9120-f144d3ecd23a</a></p>
<img src="/2025/04/21/AI-Agent/GsmyMNLasAQ_DY6" class="" title="img">
<img src="/2025/04/21/AI-Agent/GsmySEdaQAACdBZ" class="" title="img">
<br>
<h3 id="CC-实现">CC 实现</h3>
<p>Claude Code</p>
<p>ref: <a target="_blank" rel="noopener" href="https://minusx.ai/blog/decoding-claude-code/#appendix">https://minusx.ai/blog/decoding-claude-code/#appendix</a></p>
<br>
<h3 id="CoA">CoA</h3>
<p>ref: Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.13167">https://arxiv.org/abs/2508.13167</a></p>
<br>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python-Pytorch/" rel="tag"># Python, Pytorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/04/16/Search-Ads-Rec-Rela/" rel="prev" title="SAR&Rela">
      <i class="fa fa-chevron-left"></i> SAR&Rela
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/04/22/Machine-Learning/" rel="next" title="Machine-Learning">
      Machine-Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">AI-Agent</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Base"><span class="nav-text">Base</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-rela"><span class="nav-text">Model rela</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Key-methods"><span class="nav-text">Key methods</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tool-calling"><span class="nav-text">Tool calling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Structured-Output"><span class="nav-text">Structured Output</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multimodality"><span class="nav-text">Multimodality</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Memory"><span class="nav-text">Memory</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86"><span class="nav-text">短期记忆</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BC%96%E8%BE%91%E6%B6%88%E6%81%AF%E5%88%97%E8%A1%A8"><span class="nav-text">编辑消息列表</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E8%BF%87%E5%BE%80%E5%AF%B9%E8%AF%9D"><span class="nav-text">总结过往对话</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86"><span class="nav-text">长期记忆</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%B0%E5%BF%86%E5%AD%98%E5%82%A8"><span class="nav-text">记忆存储</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%B0%E5%BF%86%E7%B1%BB%E5%9E%8B"><span class="nav-text">记忆类型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Chunking"><span class="nav-text">Chunking</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Doc-structure-Chunking"><span class="nav-text">Doc structure Chunking</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Embedding"><span class="nav-text">Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Retrieval"><span class="nav-text">Retrieval</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#BM25"><span class="nav-text">BM25</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dense"><span class="nav-text">Dense</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reranker"><span class="nav-text">Reranker</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RRF"><span class="nav-text">RRF</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Model-based-Reranking"><span class="nav-text">Model-based Reranking</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Extended-Method"><span class="nav-text">Extended Method</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%81%87%E8%AE%BE%E6%80%A7%E6%96%87%E6%A1%A3%E5%B5%8C%E5%85%A5-Hypothetical-Document-Embedding-HDE"><span class="nav-text">假设性文档嵌入 (Hypothetical Document Embedding - HDE)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3%E5%BC%8F%E6%A3%80%E7%B4%A2%E7%94%9F%E6%88%90-Iterative-Retrieval-Generation-IRG"><span class="nav-text">迭代式检索生成 (Iterative Retrieval Generation - IRG)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%A8%A0%E5%AF%86%E5%8A%A0%E7%A8%80%E7%96%8F%E6%A3%80%E7%B4%A2-Optimized-Dense-Sparse-Retrieval"><span class="nav-text">优化稠密加稀疏检索 (Optimized Dense + Sparse Retrieval)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Agent"><span class="nav-text">Agent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tool"><span class="nav-text">Tool</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Graph"><span class="nav-text">Graph</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Workflow"><span class="nav-text">Workflow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Gate"><span class="nav-text">Gate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Routing"><span class="nav-text">Routing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parallelization"><span class="nav-text">Parallelization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Orchestrator-workers"><span class="nav-text">Orchestrator-workers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluator-optimizer"><span class="nav-text">Evaluator-optimizer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Agent-2"><span class="nav-text">Agent</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Design-Pattern"><span class="nav-text">Design Pattern</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Frame"><span class="nav-text">Frame</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ReAct"><span class="nav-text">ReAct</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-Agent"><span class="nav-text">Multi-Agent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tool-Integrated-Reasoning"><span class="nav-text">Tool-Integrated Reasoning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Paper-Rela"><span class="nav-text">Paper Rela</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Reflection"><span class="nav-text">Reflection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tool-Use"><span class="nav-text">Tool Use</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Planning"><span class="nav-text">Planning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-Agent-Collaboration"><span class="nav-text">Multi-Agent Collaboration</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MCP"><span class="nav-text">MCP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KBQA"><span class="nav-text">KBQA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Semantic-Parsing-based-Methods"><span class="nav-text">Semantic Parsing-based Methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Information-retrieval-based-Methods"><span class="nav-text">Information retrieval-based Methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-Methods"><span class="nav-text">Other Methods</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Improved-RAG"><span class="nav-text">Improved RAG</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GraphRAG"><span class="nav-text">GraphRAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LightRAG"><span class="nav-text">LightRAG</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Prompt-Engineering"><span class="nav-text">Prompt Engineering</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Base-2"><span class="nav-text">Base</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Zero-Shot-Prompting"><span class="nav-text">Zero-Shot Prompting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Few-Shot-Prompting"><span class="nav-text">Few-Shot Prompting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Chain-of-Thought-CoT-Prompting"><span class="nav-text">Chain-of-Thought (CoT) Prompting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Self-Consistency-Prompting"><span class="nav-text">Self-Consistency Prompting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Retrieval-Augmented-Generation-RAG"><span class="nav-text">Retrieval-Augmented Generation (RAG)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Medprompt"><span class="nav-text">Medprompt</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Prompt-%E4%BC%98%E5%8C%96"><span class="nav-text">Prompt 优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-text">其他</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#workflow%E5%AE%9E%E7%8E%B0%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE%E9%9B%86%E7%94%9F%E6%88%90"><span class="nav-text">workflow实现微调数据集生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%BB%E8%A7%89%E5%A4%84%E7%90%86"><span class="nav-text">幻觉处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RAG%E5%AE%9E%E8%B7%B5"><span class="nav-text">RAG实践</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPT-%E8%AE%B0%E5%BF%86%E5%AE%9E%E7%8E%B0"><span class="nav-text">GPT 记忆实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CC-%E5%AE%9E%E7%8E%B0"><span class="nav-text">CC 实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CoA"><span class="nav-text">CoA</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="marigo1d"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">marigo1d</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/marigo1d" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;marigo1d" rel="noopener" target="_blank">GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">marigo1d</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">551k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">16:42</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
