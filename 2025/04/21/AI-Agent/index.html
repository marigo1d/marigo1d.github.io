<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"marigo1d.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="AI-Agent设计相关 施工中…">
<meta property="og:type" content="article">
<meta property="og:title" content="AI-Agent">
<meta property="og:url" content="https://marigo1d.github.io/2025/04/21/AI-Agent/index.html">
<meta property="og:site_name" content="Marigold">
<meta property="og:description" content="AI-Agent设计相关 施工中…">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/Gu-NqzLb0AAG0lX">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/tool_calling_components-bef9d2bcb9d3706c2fe58b57bf8ccb60.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/structured_output-2c42953cee807dedd6e96f3e1db17f69.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/short-vs-long.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/filter.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/summary.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/5.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/v2-2bcd98f6541da0b6f14dc9082ee2dcda_1440w.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250604165407470.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250604170724823.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250604173158175.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/image-20250604173209083.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/b2aaf634151b4706892693ffb43d9093.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/prompt_components.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/Medqa-comp.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/black_box_graphic.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/white_box_graphic.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/judges_graphic.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/uqensemble_generate_score.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/GsmyMNLasAQ_DY6">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/GsmySEdaQAACdBZ">
<meta property="article:published_time" content="2025-04-21T01:19:56.000Z">
<meta property="article:modified_time" content="2025-07-05T08:40:00.624Z">
<meta property="article:author" content="marigo1d">
<meta property="article:tag" content="Python, Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://marigo1d.github.io/2025/04/21/AI-Agent/Gu-NqzLb0AAG0lX">

<link rel="canonical" href="https://marigo1d.github.io/2025/04/21/AI-Agent/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AI-Agent | Marigold</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Marigold</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Salt, Pepper and Birds~</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://marigo1d.github.io/2025/04/21/AI-Agent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="marigo1d">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Marigold">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AI-Agent
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-04-21 09:19:56" itemprop="dateCreated datePublished" datetime="2025-04-21T09:19:56+08:00">2025-04-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-07-05 16:40:00" itemprop="dateModified" datetime="2025-07-05T16:40:00+08:00">2025-07-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>44k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1:20</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>AI-Agent设计相关</p>
<p>施工中…</p>
<span id="more"></span>
<h1>AI-Agent</h1>
<img src="/2025/04/21/AI-Agent/Gu-NqzLb0AAG0lX" class="" title="Agent_workflow">
<p>AI-Agent侧区分</p>
<ul>
<li>LLM部分，包含对基座LLM的特化，如SFT和RLHF</li>
<li>Agent部分，包含执行链，如上下文管理，MCP（工具管理与使用）</li>
<li>数据部分，包含训练数据和检索数据，如RAG检索数据，SFT训练数据，FAISS库</li>
</ul>
<blockquote>
<p>LLM部分详见 LLM&amp;Rela</p>
</blockquote>
<br>
<h2 id="LangChain">LangChain</h2>
<p>LangChain 是一个用于构建由语言模型驱动的应用程序的框架。它的目标是让 LLM（大型语言模型）能够访问外部数据源、执行复杂任务链式逻辑，并具备决策能力。LangChain 支持多种模块组合，如提示模板、上下文管理、链式调用、文档问答、代理等。</p>
<blockquote>
<p>官方文档：<a target="_blank" rel="noopener" href="https://docs.langchain.com/">https://docs.langchain.com/</a></p>
</blockquote>
<br>
<h3 id="Model">Model</h3>
<h4 id="Key-methods">Key methods</h4>
<ol>
<li>invoke：与聊天模型交互的主要方法。它接受一个消息列表作为输入，并返回一个消息列表作为输出。</li>
<li>stream：允许您以流的形式接收聊天模型生成的输出的方法。</li>
<li>batch：允许您将多个请求批量发送到聊天模型，以提高处理效率的方法。</li>
<li>bind_tools：一种允许您将工具绑定到聊天模型，以便在模型的执行上下文中使用的方法。</li>
<li>with_structured_output：针对原生支持结构化输出的模型的 <code>invoke</code> 方法的包装器。</li>
</ol>
<br>
<h4 id="Tool-calling">Tool calling</h4>
<img src="/2025/04/21/AI-Agent/tool_calling_components-bef9d2bcb9d3706c2fe58b57bf8ccb60.png" class="" title="Conceptual parts of tool calling">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tool creation</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.tools <span class="keyword">import</span> tool</span><br><span class="line"></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiply a and b.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tool binding</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">a: <span class="built_in">int</span>, b: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiply a and b.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        a: first int</span></span><br><span class="line"><span class="string">        b: second int</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line">llm_with_tools = tool_calling_model.bind_tools([multiply])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tool calls</span></span><br><span class="line">query = <span class="string">&quot;What is 3 * 12? Also, what is 11 + 49?&quot;</span></span><br><span class="line"></span><br><span class="line">llm_with_tools.invoke(query).tool_calls</span><br></pre></td></tr></table></figure>
<br>
<h4 id="Structured-Output">Structured Output</h4>
<img src="/2025/04/21/AI-Agent/structured_output-2c42953cee807dedd6e96f3e1db17f69.png" class="" title="Structured output">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define schema</span></span><br><span class="line">schema = &#123;<span class="string">&quot;foo&quot;</span>: <span class="string">&quot;bar&quot;</span>&#125;</span><br><span class="line"><span class="comment"># Bind schema to model</span></span><br><span class="line">model_with_structure = model.with_structured_output(schema)</span><br><span class="line"><span class="comment"># Invoke the model to produce structured output that matches the schema</span></span><br><span class="line">structured_output = model_with_structure.invoke(user_input)</span><br></pre></td></tr></table></figure>
<br>
<h4 id="Multimodality">Multimodality</h4>
<p>多模态指的是处理不同形式数据的能力，例如文本、音频、图像和视频。多模态可以出现在各种组件中，使模型和系统能够无缝地处理这些数据类型的混合。</p>
<ul>
<li>聊天模型：理论上，这些模型可以接受和生成多模态输入和输出，处理各种数据类型，如文本、图像、音频和视频。</li>
<li>嵌入模型：嵌入模型可以表示多模态内容，将各种形式的数据（如文本、图像和音频）嵌入到向量空间中。</li>
<li>向量存储：向量存储可以搜索表示多模态数据的嵌入，实现不同类型信息的检索。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage</span><br><span class="line"></span><br><span class="line">message = HumanMessage(</span><br><span class="line">    content=[</span><br><span class="line">        &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>, <span class="string">&quot;text&quot;</span>: <span class="string">&quot;Describe the weather in this image:&quot;</span>&#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;image&quot;</span>,</span><br><span class="line">            <span class="string">&quot;source_type&quot;</span>: <span class="string">&quot;base64&quot;</span>,</span><br><span class="line">            <span class="string">&quot;data&quot;</span>: <span class="string">&quot;&lt;base64 string&gt;&quot;</span>,</span><br><span class="line">            <span class="string">&quot;mime_type&quot;</span>: <span class="string">&quot;image/jpeg&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line">response = model.invoke([message])</span><br><span class="line"></span><br><span class="line">message = HumanMessage(</span><br><span class="line">    content=[</span><br><span class="line">        &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>, <span class="string">&quot;text&quot;</span>: <span class="string">&quot;Describe the weather in this image:&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;type&quot;</span>: <span class="string">&quot;image_url&quot;</span>, <span class="string">&quot;image_url&quot;</span>: &#123;<span class="string">&quot;url&quot;</span>: image_url&#125;&#125;,</span><br><span class="line">    ],</span><br><span class="line">)</span><br><span class="line">response = model.invoke([message])</span><br></pre></td></tr></table></figure>
<br>
<h4 id="Memory">Memory</h4>
<img src="/2025/04/21/AI-Agent/short-vs-long.png" class="" title="img">
<h5 id="短期记忆">短期记忆</h5>
<p>会话历史管理</p>
<h6 id="编辑消息列表">编辑消息列表</h6>
<p>聊天模型通过消息接受上下文，这些消息包括开发者提供的指令（系统消息）和用户输入（人类消息）。在聊天应用中，消息在人类输入和模型响应之间交替，导致消息列表随着时间的推移而变长。由于上下文窗口有限且富标记的消息列表可能成本高昂，许多应用可以从使用手动删除或忘记过时信息的技术中受益。</p>
<img src="/2025/04/21/AI-Agent/filter.png" class="" title="img">
<p>最直接的方法是从列表中删除旧消息（类似于最近最少使用缓存）</p>
<br>
<h6 id="总结过往对话">总结过往对话</h6>
<p>问题在于修剪或删除消息，如上所示，我们可能会从消息队列的筛选中丢失信息。因此，一些应用程序从使用聊天模型对消息历史进行更复杂的总结方法中受益。</p>
<img src="/2025/04/21/AI-Agent/summary.png" class="" title="img">
<br>
<h5 id="长期记忆">长期记忆</h5>
<h6 id="记忆存储">记忆存储</h6>
<p>通过{（用户名，上下文）：记忆}保存</p>
<br>
<h6 id="长期记忆思考框架">长期记忆思考框架</h6>
<br>
<h5 id="记忆类型">记忆类型</h5>
<ul>
<li>
<p>Semantic Memory：</p>
<p>语义记忆通常用于通过记住过去交互中的事实或概念来个性化应用程序。个性化的记忆</p>
<p>例如个人资料和收藏</p>
</li>
<li>
<p>Episodic Memory：</p>
<p>情景记忆通常用于帮助代理记住如何完成任务。</p>
</li>
<li>
<p>Procedural Memory：</p>
<p>程序性记忆是模型权重、代理代码和代理提示的组合；</p>
<p>一般仅修改系统提示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个节点函数，该函数使用存储中的指令来调用模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">call_model</span>(<span class="params">state: State, store: BaseStore</span>):</span><br><span class="line">    <span class="comment"># 定义命名空间，这里使用&quot;agent_instructions&quot;作为命名空间</span></span><br><span class="line">    namespace = (<span class="string">&quot;agent_instructions&quot;</span>, )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从存储中获取键为&quot;agent_a&quot;的指令数据</span></span><br><span class="line">    <span class="comment"># [0]表示获取搜索结果的第一项(假设search返回列表)</span></span><br><span class="line">    instructions = store.get(namespace, key=<span class="string">&quot;agent_a&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 应用逻辑：使用获取的指令构建提示词</span></span><br><span class="line">    <span class="comment"># prompt_template是一个格式化字符串模板，包含&#123;instructions&#125;占位符</span></span><br><span class="line">    prompt = prompt_template.<span class="built_in">format</span>(instructions=instructions.value[<span class="string">&quot;instructions&quot;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 这里应该有调用模型等后续操作...</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个节点函数，该函数更新存储中的指令</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_instructions</span>(<span class="params">state: State, store: BaseStore</span>):</span><br><span class="line">    <span class="comment"># 定义命名空间，这里使用&quot;instructions&quot;作为命名空间</span></span><br><span class="line">    namespace = (<span class="string">&quot;instructions&quot;</span>,)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 搜索当前命名空间下的所有指令，并获取第一个结果</span></span><br><span class="line">    current_instructions = store.search(namespace)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 记忆逻辑：使用当前指令和对话状态构建提示词</span></span><br><span class="line">    prompt = prompt_template.<span class="built_in">format</span>(</span><br><span class="line">        instructions=instructions.value[<span class="string">&quot;instructions&quot;</span>], </span><br><span class="line">        conversation=state[<span class="string">&quot;messages&quot;</span>]</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调用语言模型获取输出</span></span><br><span class="line">    output = llm.invoke(prompt)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从模型输出中提取新的指令</span></span><br><span class="line">    new_instructions = output[<span class="string">&#x27;new_instructions&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将新指令存储到&quot;agent_instructions&quot;命名空间下，键为&quot;agent_a&quot;</span></span><br><span class="line">    store.put(</span><br><span class="line">        (<span class="string">&quot;agent_instructions&quot;</span>,),  <span class="comment"># 命名空间</span></span><br><span class="line">        <span class="string">&quot;agent_a&quot;</span>,                <span class="comment"># 键名</span></span><br><span class="line">        &#123;<span class="string">&quot;instructions&quot;</span>: new_instructions&#125;  <span class="comment"># 值(新指令)</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 这里可能有其他后续操作...</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
</li>
</ul>
<br>
<h5 id="记忆写入">记忆写入</h5>
<ul>
<li>
<p>热写入</p>
</li>
<li>
<p>后台写入</p>
</li>
</ul>
<br>
<h4 id="Caching">Caching</h4>
<p>一种替代方法是使用语义缓存，即根据输入的意义而不是精确输入本身来缓存响应。在某些情况下这可能有效，但在其他情况下则不然。</p>
<p>语义缓存引入了对应用程序关键路径上另一个模型的依赖（例如，语义缓存可能依赖于嵌入模型将文本转换为向量表示），并且无法保证准确捕捉输入的意义。</p>
<p>然而，可能存在一些情况下缓存聊天模型响应是有益的。例如，如果您有一个用于回答常见问题的聊天模型，缓存响应可以帮助减轻模型提供者的负担、降低成本并提高响应速度。</p>
<br>
<h3 id="Prompt">Prompt</h3>
<h4 id="PromptTemplate">PromptTemplate</h4>
<p>LangChain 提供了 <code>PromptTemplate</code> 模块用于动态地构建提示词（Prompt）。它支持变量替换，让开发者能灵活构造提示语句。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line">template = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;product&quot;</span>],</span><br><span class="line">    template=<span class="string">&quot;请为以下产品写一段广告文案：&#123;product&#125;&quot;</span></span><br><span class="line">)</span><br><span class="line">prompt = template.<span class="built_in">format</span>(product=<span class="string">&quot;智能手表&quot;</span>)</span><br></pre></td></tr></table></figure>
<br>
<h3 id="Semantic-search-engine">Semantic search engine</h3>
<p>LangChain 非常适合构建基于知识库的问答系统，其流程如下：</p>
<h4 id="Chunking">Chunking</h4>
<p>将长文档按段落或 token 限制拆分为小块（chunk），以避免输入超长。</p>
<p><code>RecursiveCharacterTextSplitter</code> 通过<strong>递归尝试不同的分隔符层级</strong>，逐步将文本分割为语义连贯的块。它优先尝试更高级别的分隔符（如段落），如果分割后的块仍过大，则递归使用更低级别的分隔符（如句子、单词）进行二次分割。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line">splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">500</span>, chunk_overlap=<span class="number">50</span>)</span><br><span class="line">chunks = splitter.split_text(document_text)</span><br></pre></td></tr></table></figure>
<br>
<h5 id="Better-Chunking">Better Chunking</h5>
<p>ref: <a target="_blank" rel="noopener" href="https://github.com/ConardLi/easy-dataset">https://github.com/ConardLi/easy-dataset</a></p>
<ol>
<li>首先需要设定文本块的最小，最大分割长度</li>
<li>自动对章节（Markdown文件中的 <code>#, ##, ###</code>）进行识别</li>
<li>对已识别到的章节字数进行计数，在恰好位于 &gt; 最小分割长度 和 &lt; 最大分割长度的前提下进行分段</li>
<li>遇到长段落（超出最大分割长度）时，执行递归分段算法 <code>RecursiveCharacterTextSplitter</code>；</li>
</ol>
<blockquote>
<p>其实就是加了一个对章节的识别分割</p>
</blockquote>
<br>
<h4 id="Embedding-Vector-Store">Embedding &amp; Vector Store</h4>
<p>使用 Embedding 模型（如 OpenAI、HuggingFace）将文本向量化，然后存入向量数据库（如 FAISS、Chroma、Pinecone）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-large&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FAISS存储</span></span><br><span class="line">embedding_dim = <span class="built_in">len</span>(embeddings.embed_query(<span class="string">&quot;hello world&quot;</span>))</span><br><span class="line">index = faiss.IndexFlatL2(embedding_dim)</span><br><span class="line"></span><br><span class="line">vector_store = FAISS(</span><br><span class="line">    embedding_function=embeddings,</span><br><span class="line">    index=index,</span><br><span class="line">    docstore=InMemoryDocstore(),</span><br><span class="line">    index_to_docstore_id=&#123;&#125;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">ids = vector_store.add_documents(documents=all_splits)</span><br></pre></td></tr></table></figure>
<br>
<h4 id="Retrieval">Retrieval</h4>
<p>使用向量检索获取相关 chunk，输入给 LLM，生成答案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> chain</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@chain</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">retriever</span>(<span class="params">query: <span class="built_in">str</span></span>) -&gt; <span class="type">List</span>[Document]:</span><br><span class="line">    <span class="keyword">return</span> vector_store.similarity_search(query, k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">retriever.batch(</span><br><span class="line">    [</span><br><span class="line">        <span class="string">&quot;How many distribution centers does Nike have in the US?&quot;</span>,</span><br><span class="line">        <span class="string">&quot;When was Nike incorporated?&quot;</span>,</span><br><span class="line">    ],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<br>
<br>
<h3 id="Agent">Agent</h3>
<p>Agent 是 LangChain 中实现 LLM 推理和决策的核心模块，支持动态选择工具（Tool）来完成多步任务。</p>
<h4 id="Base">Base</h4>
<p>核心组件：</p>
<ul>
<li><strong>Agent</strong>：负责推理和调用工具</li>
<li><strong>Tool</strong>：工具函数，如 Web 搜索、数据库查询、Python 计算等</li>
<li><strong>LLM</strong>：驱动 Agent 的大脑</li>
<li><strong>AgentExecutor</strong>：执行器，负责管理 Agent 的执行逻辑</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import relevant functionality</span></span><br><span class="line"><span class="keyword">from</span> langchain_anthropic <span class="keyword">import</span> ChatAnthropic</span><br><span class="line"><span class="keyword">from</span> langchain_community.tools.tavily_search <span class="keyword">import</span> TavilySearchResults</span><br><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage</span><br><span class="line"><span class="keyword">from</span> langgraph.checkpoint.memory <span class="keyword">import</span> MemorySaver</span><br><span class="line"><span class="keyword">from</span> langgraph.prebuilt <span class="keyword">import</span> create_react_agent</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the agent</span></span><br><span class="line">memory = MemorySaver()</span><br><span class="line">model = ChatAnthropic(model_name=<span class="string">&quot;claude-3-sonnet-20240229&quot;</span>)</span><br><span class="line">search = TavilySearchResults(max_results=<span class="number">2</span>)</span><br><span class="line">tools = [search]</span><br><span class="line">agent_executor = create_react_agent(model, tools, checkpointer=memory)</span><br></pre></td></tr></table></figure>
<br>
<h4 id="QA-Processing-Method">QA Processing Method</h4>
<p>Stuff Method（塞入式）</p>
<p>直接将所有相关文档的内容“塞”进 Prompt 里，适用于内容少、上下文短的场景。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LLMChain + 文档组合直接生成 Prompt</span></span><br></pre></td></tr></table></figure>
<p>MapReduce</p>
<p>将所有 chunk 分别用 LLM 处理后，再用另一个 LLM 汇总（Reduce），类似 MapReduce 思维。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文档切片 -&gt; 各片段生成回答（Map）-&gt; 汇总为最终答案（Reduce）</span></span><br></pre></td></tr></table></figure>
<p>Refine</p>
<p>初始文档回答一个粗略答案，接着逐步引入更多 chunk 来细化和完善答案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初步回答 + 后续 refine，适合逐步深入处理</span></span><br></pre></td></tr></table></figure>
<p>MapRerank</p>
<p>类似 Map 方法，但最后不汇总所有回答，而是由 LLM 对每个回答打分，选择最优答案。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多个回答 -&gt; 打分排序 -&gt; 返回最高分</span></span><br></pre></td></tr></table></figure>
<br>
<h4 id="Streaming-message-Streaming-tokens">Streaming message &amp; Streaming tokens</h4>
<p><strong>streaming tokens</strong></p>
<p>指逐个生成文本片段（Token），适用于需要实时展示生成过程的场景，如逐词显示回答。</p>
<p>测试代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">model = ChatOpenAI(model=<span class="string">&quot;gpt-4o-mini&quot;</span>, streaming=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 同步流式处理</span></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> model.stream(<span class="string">&quot;天空是什么颜色？&quot;</span>):</span><br><span class="line">    <span class="built_in">print</span>(chunk.content, end=<span class="string">&quot;|&quot;</span>, flush=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>输出效果：<br>
<code>The| sky| appears| blue| during| the| day|.</code></p>
<br>
<p><strong>Streaming Messages</strong></p>
<p>我们已经看到如何使用 <code>.invoke</code> 来调用代理以获取最终响应。如果代理执行多个步骤，这可能需要一些时间。为了展示中间进度，我们可以按消息发生时实时返回消息。</p>
<p>测试代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> agent_executor.stream(</span><br><span class="line">    &#123;<span class="string">&quot;messages&quot;</span>: [HumanMessage(content=<span class="string">&quot;whats the weather in sf?&quot;</span>)]&#125;,</span><br><span class="line">    stream_mode=<span class="string">&quot;values&quot;</span>,</span><br><span class="line">):</span><br><span class="line">    step[<span class="string">&quot;messages&quot;</span>][-<span class="number">1</span>].pretty_print()</span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">================================[1m Human Message [0m=================================</span><br><span class="line"></span><br><span class="line">whats the weather <span class="keyword">in</span> sf?</span><br><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line"></span><br><span class="line">[&#123;<span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;Okay, let me look up the current weather for San Francisco using a search engine:&#x27;</span>, <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;text&#x27;</span>&#125;, &#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;toolu_01H1brh5EZpZqtqHBxkosPtN&#x27;</span>, <span class="string">&#x27;input&#x27;</span>: &#123;<span class="string">&#x27;query&#x27;</span>: <span class="string">&#x27;san francisco weather&#x27;</span>&#125;, <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;tavily_search_results_json&#x27;</span>, <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;tool_use&#x27;</span>&#125;]</span><br><span class="line">Tool Calls:</span><br><span class="line">  tavily_search_results_json (toolu_01H1brh5EZpZqtqHBxkosPtN)</span><br><span class="line"> Call ID: toolu_01H1brh5EZpZqtqHBxkosPtN</span><br><span class="line">  Args:</span><br><span class="line">    query: san francisco weather</span><br><span class="line">=================================[1m Tool Message [0m=================================</span><br><span class="line">Name: tavily_search_results_json</span><br><span class="line"></span><br><span class="line">[&#123;<span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://www.weatherapi.com/&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;&#123;&#x27;location&#x27;: &#123;&#x27;name&#x27;: &#x27;San Francisco&#x27;, &#x27;region&#x27;: &#x27;California&#x27;, &#x27;country&#x27;: &#x27;United States of America&#x27;, &#x27;lat&#x27;: 37.775, &#x27;lon&#x27;: -122.4183, &#x27;tz_id&#x27;: &#x27;America/Los_Angeles&#x27;, &#x27;localtime_epoch&#x27;: 1739994486, &#x27;localtime&#x27;: &#x27;2025-02-19 11:48&#x27;&#125;, &#x27;current&#x27;: &#123;&#x27;last_updated_epoch&#x27;: 1739994300, &#x27;last_updated&#x27;: &#x27;2025-02-19 11:45&#x27;, &#x27;temp_c&#x27;: 13.3, &#x27;temp_f&#x27;: 55.9, &#x27;is_day&#x27;: 1, &#x27;condition&#x27;: &#123;&#x27;text&#x27;: &#x27;Light rain&#x27;, &#x27;icon&#x27;: &#x27;//cdn.weatherapi.com/weather/64x64/day/296.png&#x27;, &#x27;code&#x27;: 1183&#125;, &#x27;wind_mph&#x27;: 5.8, &#x27;wind_kph&#x27;: 9.4, &#x27;wind_degree&#x27;: 195, &#x27;wind_dir&#x27;: &#x27;SSW&#x27;, &#x27;pressure_mb&#x27;: 1023.0, &#x27;pressure_in&#x27;: 30.2, &#x27;precip_mm&#x27;: 0.0, &#x27;precip_in&#x27;: 0.0, &#x27;humidity&#x27;: 87, &#x27;cloud&#x27;: 100, &#x27;feelslike_c&#x27;: 12.7, &#x27;feelslike_f&#x27;: 54.8, &#x27;windchill_c&#x27;: 9.1, &#x27;windchill_f&#x27;: 48.4, &#x27;heatindex_c&#x27;: 10.2, &#x27;heatindex_f&#x27;: 50.3, &#x27;dewpoint_c&#x27;: 9.8, &#x27;dewpoint_f&#x27;: 49.7, &#x27;vis_km&#x27;: 4.0, &#x27;vis_miles&#x27;: 2.0, &#x27;uv&#x27;: 1.4, &#x27;gust_mph&#x27;: 8.9, &#x27;gust_kph&#x27;: 14.4&#125;&#125;&quot;</span>&#125;, &#123;<span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://world-weather.info/forecast/usa/san_francisco/february-2025/&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Weather in San Francisco in February 2025 (California) - Detailed Weather Forecast for a Month Weather World Weather in San Francisco Weather in San Francisco in February 2025 San Francisco Weather Forecast for February 2025, is based on previous years&#x27; statistical data. +59°+50° +59°+52° +59°+50° +61°+52° +59°+50° +61°+50° +61°+52° +63°+52° +61°+52° +61°+50° +61°+50° +61°+50° +59°+50° +59°+50° +61°+50° +61°+52° +59°+50° +59°+48° +57°+48° +59°+50° +59°+48° +59°+50° +57°+46° +61°+50° +61°+50° +59°+50° +59°+48° +59°+50° Extended weather forecast in San Francisco HourlyWeek10-Day14-Day30-DayYear Weather in large and nearby cities Weather in Washington, D.C.+41° Sacramento+55° Pleasanton+55° Redwood City+55° San Leandro+55° San Mateo+54° San Rafael+52° San Ramon+52° South San Francisco+54° Vallejo+50° Palo Alto+55° Pacifica+55° Berkeley+54° Castro Valley+55° Concord+52° Daly City+54° Noverd+52° Sign Hill+54° world&#x27;s temperature today day day Temperature units&quot;</span>&#125;]</span><br><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line"></span><br><span class="line">The search results provide details on the current weather conditions and forecast <span class="keyword">for</span> San Francisco. Some key details:</span><br><span class="line"></span><br><span class="line">- It is lightly raining <span class="keyword">in</span> San Francisco right now, with a temperature around 55°F/13°C. </span><br><span class="line">- The forecast <span class="keyword">for</span> the rest of February 2025 shows daytime highs mostly <span class="keyword">in</span> the upper 50s to low 60s F, with night lows <span class="keyword">in</span> the upper 40s to low 50s F. </span><br><span class="line">- Typical weather includes some rain, clouds, cool temperatures and breezy conditions.</span><br><span class="line"></span><br><span class="line">So <span class="keyword">in</span> summary, as is common <span class="keyword">for</span> San Francisco <span class="keyword">in</span> late winter, it is currently cool with light rain showers, and similar mild, unsettled weather is expected over the next couple weeks. Layers and a light jacket would be advisable <span class="keyword">for</span> being outdoors. Let me know <span class="keyword">if</span> you need any other details!</span><br></pre></td></tr></table></figure>
<br>
<h4 id="Memory-Adding">Memory Adding</h4>
<p>通过config实现对线程上下文的记忆</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">agent_executor = create_react_agent(model, tools, checkpointer=memory)</span><br><span class="line"></span><br><span class="line">config = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;thread_id&quot;</span>: <span class="string">&quot;abc123&quot;</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> agent_executor.stream(</span><br><span class="line">    &#123;<span class="string">&quot;messages&quot;</span>: [HumanMessage(content=<span class="string">&quot;hi im bob!&quot;</span>)]&#125;, config</span><br><span class="line">):</span><br><span class="line">    <span class="built_in">print</span>(chunk)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;agent&#x27;</span>: &#123;<span class="string">&#x27;messages&#x27;</span>: [AIMessage(content=<span class="string">&quot;Hello Bob! It&#x27;s nice to meet you again.&quot;</span>, response_metadata=&#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;msg_013C1z2ZySagEFwmU1EsysR2&#x27;</span>, <span class="string">&#x27;model&#x27;</span>: <span class="string">&#x27;claude-3-sonnet-20240229&#x27;</span>, <span class="string">&#x27;stop_reason&#x27;</span>: <span class="string">&#x27;end_turn&#x27;</span>, <span class="string">&#x27;stop_sequence&#x27;</span>: None, <span class="string">&#x27;usage&#x27;</span>: &#123;<span class="string">&#x27;input_tokens&#x27;</span>: 1162, <span class="string">&#x27;output_tokens&#x27;</span>: 14&#125;&#125;, <span class="built_in">id</span>=<span class="string">&#x27;run-f878acfd-d195-44e8-9166-e2796317e3f8-0&#x27;</span>, usage_metadata=&#123;<span class="string">&#x27;input_tokens&#x27;</span>: 1162, <span class="string">&#x27;output_tokens&#x27;</span>: 14, <span class="string">&#x27;total_tokens&#x27;</span>: 1176&#125;)]&#125;&#125;</span><br><span class="line">----</span><br></pre></td></tr></table></figure>
<p>尝试上下文记忆</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> agent_executor.stream(</span><br><span class="line">    &#123;<span class="string">&quot;messages&quot;</span>: [HumanMessage(content=<span class="string">&quot;whats my name?&quot;</span>)]&#125;, config</span><br><span class="line">):</span><br><span class="line">    <span class="built_in">print</span>(chunk)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;agent&#x27;</span>: &#123;<span class="string">&#x27;messages&#x27;</span>: [AIMessage(content=<span class="string">&#x27;You mentioned your name is Bob when you introduced yourself earlier. So your name is Bob.&#x27;</span>, response_metadata=&#123;<span class="string">&#x27;id&#x27;</span>: <span class="string">&#x27;msg_01WNwnRNGwGDRw6vRdivt6i1&#x27;</span>, <span class="string">&#x27;model&#x27;</span>: <span class="string">&#x27;claude-3-sonnet-20240229&#x27;</span>, <span class="string">&#x27;stop_reason&#x27;</span>: <span class="string">&#x27;end_turn&#x27;</span>, <span class="string">&#x27;stop_sequence&#x27;</span>: <span class="literal">None</span>, <span class="string">&#x27;usage&#x27;</span>: &#123;<span class="string">&#x27;input_tokens&#x27;</span>: <span class="number">1184</span>, <span class="string">&#x27;output_tokens&#x27;</span>: <span class="number">21</span>&#125;&#125;, <span class="built_in">id</span>=<span class="string">&#x27;run-f5c0b957-8878-405a-9d4b-a7cd38efe81f-0&#x27;</span>, usage_metadata=&#123;<span class="string">&#x27;input_tokens&#x27;</span>: <span class="number">1184</span>, <span class="string">&#x27;output_tokens&#x27;</span>: <span class="number">21</span>, <span class="string">&#x27;total_tokens&#x27;</span>: <span class="number">1205</span>&#125;)]&#125;&#125;</span><br><span class="line">----</span><br></pre></td></tr></table></figure>
<br>
<h3 id="Example">Example</h3>
<h4 id="Extraction-Chain">Extraction Chain</h4>
<h5 id="Schema-Extrator">Schema &amp; Extrator</h5>
<p>Schema 描述如何从文本中提取信息，通过结构化指定 + prompt invoke实现信息提取</p>
<blockquote>
<p>关于 pydantic 库的使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">User</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="comment"># 必填字段，字符串类型</span></span><br><span class="line">    username: <span class="built_in">str</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 可选字段，带有默认值</span></span><br><span class="line">    age: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用 Field 添加额外验证和元数据</span></span><br><span class="line">    email: <span class="built_in">str</span> = Field(</span><br><span class="line">        ...,  <span class="comment"># 表示必填字段</span></span><br><span class="line">        min_length=<span class="number">5</span>,</span><br><span class="line">        max_length=<span class="number">100</span>,</span><br><span class="line">        regex=<span class="string">r&quot;^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$&quot;</span>,</span><br><span class="line">        description=<span class="string">&quot;用户的电子邮件地址&quot;</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 带有默认值和额外验证的字段</span></span><br><span class="line">    score: <span class="built_in">float</span> = Field(</span><br><span class="line">        default=<span class="number">0.0</span>,</span><br><span class="line">        ge=<span class="number">0</span>,</span><br><span class="line">        le=<span class="number">100</span>,</span><br><span class="line">        description=<span class="string">&quot;用户评分，范围0-100&quot;</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 别名示例</span></span><br><span class="line">    full_name: <span class="built_in">str</span> = Field(..., alias=<span class="string">&quot;fullName&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用模型</span></span><br><span class="line">user_data = &#123;</span><br><span class="line">    <span class="string">&quot;username&quot;</span>: <span class="string">&quot;john_doe&quot;</span>,</span><br><span class="line">    <span class="string">&quot;email&quot;</span>: <span class="string">&quot;john@example.com&quot;</span>,</span><br><span class="line">    <span class="string">&quot;fullName&quot;</span>: <span class="string">&quot;John Doe&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">user = User(**user_data)</span><br><span class="line"><span class="built_in">print</span>(user)</span><br><span class="line"><span class="comment"># 输出: username=&#x27;john_doe&#x27; age=None email=&#x27;john@example.com&#x27; score=0.0 full_name=&#x27;John Doe&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(user.<span class="built_in">dict</span>(by_alias=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># 输出: &#123;&#x27;username&#x27;: &#x27;john_doe&#x27;, &#x27;age&#x27;: None, &#x27;email&#x27;: &#x27;john@example.com&#x27;, &#x27;score&#x27;: 0.0, &#x27;fullName&#x27;: &#x27;John Doe&#x27;&#125;</span></span><br></pre></td></tr></table></figure>
<p>说明：</p>
<ul>
<li><code>default</code>: 字段的默认值</li>
<li><code>default_factory</code>: 生成默认值的可调用对象</li>
<li><code>alias</code>: 字段的别名（用于序列化/反序列化）</li>
<li><code>title</code>: 字段的标题（用于文档）</li>
<li><code>description</code>: 字段的描述（用于文档）</li>
<li><code>gt</code>: 大于 (greater than)</li>
<li><code>ge</code>: 大于等于 (greater than or equal)</li>
<li><code>lt</code>: 小于 (less than)</li>
<li><code>le</code>: 小于等于 (less than or equal)</li>
<li><code>min_length</code>: 最小长度（字符串、列表等）</li>
<li><code>max_length</code>: 最大长度（字符串、列表等）</li>
<li><code>regex</code>: 正则表达式验证</li>
</ul>
</blockquote>
<br>
<p>案例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span></span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, MessagesPlaceholder</span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Person</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Information about a person.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ^ Doc-string for the entity Person.</span></span><br><span class="line">    <span class="comment"># This doc-string is sent to the LLM as the description of the schema Person,</span></span><br><span class="line">    <span class="comment"># and it can help to improve extraction results.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Note that:</span></span><br><span class="line">    <span class="comment"># 1. Each field is an `optional` -- this allows the model to decline to extract it!</span></span><br><span class="line">    <span class="comment"># 2. Each field has a `description` -- this description is used by the LLM.</span></span><br><span class="line">    <span class="comment"># Having a good description can help improve extraction results.</span></span><br><span class="line">    name: <span class="type">Optional</span>[<span class="built_in">str</span>] = Field(default=<span class="literal">None</span>, description=<span class="string">&quot;The name of the person&quot;</span>)</span><br><span class="line">    hair_color: <span class="type">Optional</span>[<span class="built_in">str</span>] = Field(</span><br><span class="line">        default=<span class="literal">None</span>, description=<span class="string">&quot;The color of the person&#x27;s hair if known&quot;</span></span><br><span class="line">    )</span><br><span class="line">    height_in_meters: <span class="type">Optional</span>[<span class="built_in">str</span>] = Field(</span><br><span class="line">        default=<span class="literal">None</span>, description=<span class="string">&quot;Height measured in meters&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Data</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Extracted data about people.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Creates a model so that we can extract multiple entities.</span></span><br><span class="line">    people: <span class="type">List</span>[Person]        </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># 为LLM设置schema</span></span><br><span class="line"><span class="comment"># structured_llm = llm.with_structured_output(schema=Person)</span></span><br><span class="line"></span><br><span class="line">structured_llm = llm.with_structured_output(schema=Data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a custom prompt to provide instructions and any additional context.</span></span><br><span class="line"><span class="comment"># 1) You can add examples into the prompt template to improve extraction quality</span></span><br><span class="line"><span class="comment"># 2) Introduce additional parameters to take context into account (e.g., include metadata</span></span><br><span class="line"><span class="comment">#    about the document from which the text was extracted.)</span></span><br><span class="line">prompt_template = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (</span><br><span class="line">            <span class="string">&quot;system&quot;</span>,</span><br><span class="line">            <span class="string">&quot;You are an expert extraction algorithm. &quot;</span></span><br><span class="line">            <span class="string">&quot;Only extract relevant information from the text. &quot;</span></span><br><span class="line">            <span class="string">&quot;If you do not know the value of an attribute asked to extract, &quot;</span></span><br><span class="line">            <span class="string">&quot;return null for the attribute&#x27;s value.&quot;</span>,</span><br><span class="line">        ),</span><br><span class="line">        <span class="comment"># Please see the how-to about improving performance with</span></span><br><span class="line">        <span class="comment"># reference examples.</span></span><br><span class="line">        <span class="comment"># MessagesPlaceholder(&#x27;examples&#x27;),</span></span><br><span class="line">        (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;text&#125;&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.&quot;</span></span><br><span class="line">prompt = prompt_template.invoke(&#123;<span class="string">&quot;text&quot;</span>: text&#125;)</span><br><span class="line">structured_llm.invoke(prompt)</span><br></pre></td></tr></table></figure>
<br>
<p>output</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Data(people=[Person(name=<span class="string">&#x27;Jeff&#x27;</span>, hair_color=<span class="string">&#x27;black&#x27;</span>, height_in_meters=<span class="string">&#x27;1.83&#x27;</span>), Person(name=<span class="string">&#x27;Anna&#x27;</span>, hair_color=<span class="string">&#x27;black&#x27;</span>, height_in_meters=None)])</span><br></pre></td></tr></table></figure>
<br>
<h5 id="RAG-based-long-context-extraction">RAG based long context extraction</h5>
<p>使用 FAISS</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span>, <span class="type">Optional</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, MessagesPlaceholder</span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KeyDevelopment</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Information about a development in the history of cars.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    year: <span class="built_in">int</span> = Field(</span><br><span class="line">        ...,  <span class="comment"># `...` 表示必填字段</span></span><br><span class="line">        description=<span class="string">&quot;The year when there was an important historic development.&quot;</span></span><br><span class="line">    )</span><br><span class="line">    description: <span class="built_in">str</span> = Field(</span><br><span class="line">        ..., </span><br><span class="line">        description=<span class="string">&quot;What happened in this year? What was the development?&quot;</span></span><br><span class="line">    )</span><br><span class="line">    evidence: <span class="built_in">str</span> = Field(</span><br><span class="line">        ...,</span><br><span class="line">        description=<span class="string">&quot;Repeat verbatim the sentence(s) from which the year and description were extracted.&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ExtractionData</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Extracted information about key developments in the history of cars.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    key_developments: <span class="type">List</span>[KeyDevelopment]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a custom prompt to provide instructions and any additional context.</span></span><br><span class="line"><span class="comment"># 1) You can add examples into the prompt template to improve extraction quality</span></span><br><span class="line"><span class="comment"># 2) Introduce additional parameters to take context into account (e.g., include metadata</span></span><br><span class="line"><span class="comment">#    about the document from which the text was extracted.)</span></span><br><span class="line">prompt = ChatPromptTemplate.from_messages(</span><br><span class="line">    [</span><br><span class="line">        (</span><br><span class="line">            <span class="string">&quot;system&quot;</span>,</span><br><span class="line">            <span class="string">&quot;You are an expert at identifying key historic development in text. &quot;</span></span><br><span class="line">            <span class="string">&quot;Only extract important historic developments. Extract nothing if no important information can be found in the text.&quot;</span>,</span><br><span class="line">        ),</span><br><span class="line">        (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;text&#125;&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入必要的库和模块</span></span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS  <span class="comment"># FAISS向量数据库，用于高效相似性搜索</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.documents <span class="keyword">import</span> Document      <span class="comment"># 文档处理基础类</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableLambda <span class="comment"># 可运行Lambda函数</span></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings     <span class="comment"># OpenAI的文本嵌入模型</span></span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter  <span class="comment"># 文本分割器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用文本分割器将文档内容分割成多个文本块</span></span><br><span class="line"><span class="comment"># document.page_content是原始文档内容</span></span><br><span class="line">texts = text_splitter.split_text(document.page_content)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用FAISS创建向量存储库</span></span><br><span class="line"><span class="comment"># 将分割后的文本块(texts)转换为向量表示，使用OpenAI的嵌入模型</span></span><br><span class="line">vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建检索器(retriever)，用于从向量库中检索相关文档</span></span><br><span class="line"><span class="comment"># search_kwargs=&#123;&quot;k&quot;: 1&#125; 表示只返回最相关的1个文档</span></span><br><span class="line">retriever = vectorstore.as_retriever(</span><br><span class="line">    search_kwargs=&#123;<span class="string">&quot;k&quot;</span>: <span class="number">1</span>&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">extractor = prompt | llm.with_structured_output(</span><br><span class="line">    schema=ExtractionData,</span><br><span class="line">    include_raw=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建RAG(检索增强生成)提取器</span></span><br><span class="line"><span class="comment"># 1. 首先使用retriever检索文档</span></span><br><span class="line"><span class="comment"># 2. 然后通过Lambda函数提取第一个文档的内容(docs[0].page_content)</span></span><br><span class="line"><span class="comment"># 3. 最后将提取的文本传递给extractor进行处理</span></span><br><span class="line">rag_extractor = &#123;</span><br><span class="line">    <span class="string">&quot;text&quot;</span>: retriever | (<span class="keyword">lambda</span> docs: docs[<span class="number">0</span>].page_content)</span><br><span class="line">&#125; | extractor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用RAG提取器，查询&quot;与汽车相关的关键发展&quot;</span></span><br><span class="line"><span class="comment"># 系统会先检索相关文档，然后提取信息</span></span><br><span class="line">results = rag_extractor.invoke(<span class="string">&quot;Key developments associated with cars&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历并打印提取出的每个关键发展</span></span><br><span class="line"><span class="keyword">for</span> key_development <span class="keyword">in</span> results.key_developments:</span><br><span class="line">    <span class="built_in">print</span>(key_development)</span><br></pre></td></tr></table></figure>
<br>
<h4 id="Classify">Classify</h4>
<p>案例代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Classification</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    sentiment: <span class="built_in">str</span> = Field(..., enum=[<span class="string">&quot;happy&quot;</span>, <span class="string">&quot;neutral&quot;</span>, <span class="string">&quot;sad&quot;</span>])</span><br><span class="line">    aggressiveness: <span class="built_in">int</span> = Field(</span><br><span class="line">        ...,</span><br><span class="line">        description=<span class="string">&quot;describes how aggressive the statement is, the higher the number the more aggressive&quot;</span>,</span><br><span class="line">        enum=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">    )</span><br><span class="line">    language: <span class="built_in">str</span> = Field(</span><br><span class="line">        ..., enum=[<span class="string">&quot;spanish&quot;</span>, <span class="string">&quot;english&quot;</span>, <span class="string">&quot;french&quot;</span>, <span class="string">&quot;german&quot;</span>, <span class="string">&quot;italian&quot;</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tagging_prompt = ChatPromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Extract the desired information from the following passage.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Only extract the properties mentioned in the &#x27;Classification&#x27; function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Passage:</span></span><br><span class="line"><span class="string">&#123;input&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># LLM</span></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4o-mini&quot;</span>).with_structured_output(</span><br><span class="line">    Classification</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">inp = <span class="string">&quot;Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!&quot;</span></span><br><span class="line">prompt = tagging_prompt.invoke(&#123;<span class="string">&quot;input&quot;</span>: inp&#125;)</span><br><span class="line">response = llm.invoke(prompt)</span><br><span class="line"></span><br><span class="line">response</span><br><span class="line"><span class="comment"># output: </span></span><br><span class="line"><span class="comment"># Classification(sentiment=&#x27;positive&#x27;, aggressiveness=1, language=&#x27;Spanish&#x27;)</span></span><br><span class="line">response.model_dump()  <span class="comment"># dictionary output</span></span><br><span class="line"><span class="comment"># output: </span></span><br><span class="line"><span class="comment"># sentiment=&#x27;positive&#x27;, aggressiveness=1, language=&#x27;Spanish&#x27;</span></span><br></pre></td></tr></table></figure>
<br>
<h2 id="LangGraph">LangGraph</h2>
<h3 id="Graph">Graph</h3>
<p>Graph 由 nodes 和 edge 构成</p>
<p>state 为每个图的 context，在定义图时传入；</p>
<p>每个 <code>node</code> 都可以接收当前的 <code>State</code> 作为输入，并输出状态更新。</p>
<p>The <code>State</code> includes the graph’s schema and <a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers">reducer functions</a> that handle state updates.</p>
<br>
<p><strong>常用方法</strong></p>
<p><code>add_node</code></p>
<ul>
<li>
<p>通过 <code>.add_node(str, node)</code> 添加节点到图，第一个参数为唯一节点名，第二个参数为节点被使用时所调用的函数或对象；</p>
<blockquote>
<p>每个node相当于一个可执行函数，可以在函数内做任何事情</p>
</blockquote>
</li>
</ul>
<br>
<p><code>add_edge</code></p>
<ul>
<li>通过 <code>.add_edge(START, str)</code> 和 <code>.add_edge(node, str)</code> 添加 <code>entry</code> point 和 <code>finish</code> point，指名图的起点和终点，str 为节点名</li>
</ul>
<br>
<p><code>add_conditional_edges</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">add_conditional_edges(</span><br><span class="line">    source: <span class="built_in">str</span>,</span><br><span class="line">    path: <span class="type">Union</span>[</span><br><span class="line">        <span class="type">Callable</span>[..., <span class="type">Union</span>[Hashable, <span class="built_in">list</span>[Hashable]]],</span><br><span class="line">        <span class="type">Callable</span>[</span><br><span class="line">            ..., Awaitable[<span class="type">Union</span>[Hashable, <span class="built_in">list</span>[Hashable]]]</span><br><span class="line">        ],</span><br><span class="line">        Runnable[<span class="type">Any</span>, <span class="type">Union</span>[Hashable, <span class="built_in">list</span>[Hashable]]],</span><br><span class="line">    ],</span><br><span class="line">    path_map: <span class="type">Optional</span>[</span><br><span class="line">        <span class="type">Union</span>[<span class="built_in">dict</span>[Hashable, <span class="built_in">str</span>], <span class="built_in">list</span>[<span class="built_in">str</span>]]</span><br><span class="line">    ] = <span class="literal">None</span>,</span><br><span class="line">    then: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>,</span><br><span class="line">) -&gt; Self</span><br></pre></td></tr></table></figure>
<p>从起始节点添加条件边到任意数量的目标节点。</p>
<p><strong>Parameters: 参数：</strong></p>
<ul>
<li><code>source</code> （ <code>str</code> ）- 起始节点。当退出此节点时，将运行此条件边。</li>
<li><code>path</code> （ <code>Union[Callable, Runnable]</code> ）- 确定下一个节点或节点的可调用对象。如果不指定 <code>path_map</code> ，它应该返回一个或多个节点。如果返回 END，则图形将停止执行。</li>
<li><code>path_map</code> （ <code>Optional[dict[Hashable, str]]</code> ，默认： <code>None</code> ）- 可选的路径到节点名称的映射。如果省略，则 <code>path</code> 返回的路径应该是节点名称。</li>
<li><code>then</code> （ <code>Optional[str]</code> ，默认： <code>None</code> ）- 在 <code>path</code> 选择的节点之后执行节点的名称。</li>
</ul>
<p><strong>Returns: 返回值：</strong></p>
<ul>
<li><code>Self</code> （ <code>Self</code> ）- 图的实例，允许方法链式调用。</li>
</ul>
<br>
<p><code>interrupt</code></p>
<p>LangGraph 中的 <code>interrupt</code> 函数通过在特定节点暂停图、向人类展示信息，并使用他们的输入继续图来启用人类在循环中的工作流程。此功能适用于审批、编辑或收集额外输入等任务。 <code>interrupt</code> 函数与 <code>Command</code> 对象一起使用，以人类提供的值继续图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">human_approval</span>(<span class="params">state: State</span>) -&gt; Command[<span class="type">Literal</span>[<span class="string">&quot;some_node&quot;</span>, <span class="string">&quot;another_node&quot;</span>]]:</span><br><span class="line">    is_approved = interrupt(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;question&quot;</span>: <span class="string">&quot;Is this correct?&quot;</span>,</span><br><span class="line">            <span class="comment"># Surface the output that should be</span></span><br><span class="line">            <span class="comment"># reviewed and approved by the human.</span></span><br><span class="line">            <span class="string">&quot;llm_output&quot;</span>: state[<span class="string">&quot;llm_output&quot;</span>]</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_approved:</span><br><span class="line">        <span class="keyword">return</span> Command(goto=<span class="string">&quot;some_node&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> Command(goto=<span class="string">&quot;another_node&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the node to the graph in an appropriate location</span></span><br><span class="line"><span class="comment"># and connect it to the relevant nodes.</span></span><br><span class="line">graph_builder.add_node(<span class="string">&quot;human_approval&quot;</span>, human_approval)</span><br><span class="line">graph = graph_builder.<span class="built_in">compile</span>(checkpointer=checkpointer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># After running the graph and hitting the interrupt, the graph will pause.</span></span><br><span class="line"><span class="comment"># Resume it with either an approval or rejection.</span></span><br><span class="line">thread_config = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;thread_id&quot;</span>: <span class="string">&quot;some_id&quot;</span>&#125;&#125;</span><br><span class="line">graph.invoke(Command(resume=<span class="literal">True</span>), config=thread_config)</span><br></pre></td></tr></table></figure>
<br>
<p><strong>案例代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    messages: Annotated[<span class="built_in">list</span>, add_messages]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 工具：通过interrupt与人类交互</span></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="comment"># Note that because we are generating a ToolMessage for a state update, we</span></span><br><span class="line"><span class="comment"># generally require the ID of the corresponding tool call. We can use</span></span><br><span class="line"><span class="comment"># LangChain&#x27;s InjectedToolCallId to signal that this argument should not</span></span><br><span class="line"><span class="comment"># be revealed to the model in the tool&#x27;s schema.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">human_assistance</span>(<span class="params"></span></span><br><span class="line"><span class="params">    name: <span class="built_in">str</span>, birthday: <span class="built_in">str</span>, tool_call_id: Annotated[<span class="built_in">str</span>, InjectedToolCallId]</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Request assistance from a human.&quot;&quot;&quot;</span></span><br><span class="line">    human_response = interrupt(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;question&quot;</span>: <span class="string">&quot;Is this correct?&quot;</span>,</span><br><span class="line">            <span class="string">&quot;name&quot;</span>: name,</span><br><span class="line">            <span class="string">&quot;birthday&quot;</span>: birthday,</span><br><span class="line">        &#125;,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># If the information is correct, update the state as-is.</span></span><br><span class="line">    <span class="keyword">if</span> human_response.get(<span class="string">&quot;correct&quot;</span>, <span class="string">&quot;&quot;</span>).lower().startswith(<span class="string">&quot;y&quot;</span>):</span><br><span class="line">        verified_name = name</span><br><span class="line">        verified_birthday = birthday</span><br><span class="line">        response = <span class="string">&quot;Correct&quot;</span></span><br><span class="line">    <span class="comment"># Otherwise, receive information from the human reviewer.</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        verified_name = human_response.get(<span class="string">&quot;name&quot;</span>, name)</span><br><span class="line">        verified_birthday = human_response.get(<span class="string">&quot;birthday&quot;</span>, birthday)</span><br><span class="line">        response = <span class="string">f&quot;Made a correction: <span class="subst">&#123;human_response&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># This time we explicitly update the state with a ToolMessage inside</span></span><br><span class="line">    <span class="comment"># the tool.</span></span><br><span class="line">    state_update = &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: verified_name,</span><br><span class="line">        <span class="string">&quot;birthday&quot;</span>: verified_birthday,</span><br><span class="line">        <span class="string">&quot;messages&quot;</span>: [ToolMessage(response, tool_call_id=tool_call_id)],</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># We return a Command object in the tool to update our state.</span></span><br><span class="line">    <span class="keyword">return</span> Command(update=state_update)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Modification: tell the LLM which tools it can call</span></span><br><span class="line">tool = TavilySearchResults(max_results=<span class="number">2</span>)</span><br><span class="line">tools = [tool, human_assistance]</span><br><span class="line"></span><br><span class="line">llm = ChatAnthropic(model=<span class="string">&quot;claude-3-5-sonnet-20240620&quot;</span>)</span><br><span class="line">llm_with_tools = llm.bind_tools(tools)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chatbot</span>(<span class="params">state: State</span>):</span><br><span class="line">    message = llm_with_tools.invoke(state[<span class="string">&quot;messages&quot;</span>])</span><br><span class="line">    <span class="comment"># Because we will be interrupting during tool execution,</span></span><br><span class="line">    <span class="comment"># we disable parallel tool calling to avoid repeating any</span></span><br><span class="line">    <span class="comment"># tool invocations when we resume.</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(message.tool_calls) &lt;= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: [llm_with_tools.invoke(state[<span class="string">&quot;messages&quot;</span>])]&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">graph_builder = StateGraph(State)</span><br><span class="line">graph_builder.add_node(<span class="string">&quot;chatbot&quot;</span>, chatbot)</span><br><span class="line"></span><br><span class="line">tool_node = ToolNode(tools=[tool])</span><br><span class="line">graph_builder.add_node(<span class="string">&quot;tools&quot;</span>, tool_node)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">route_tools</span>(<span class="params"></span></span><br><span class="line"><span class="params">    state: State,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Use in the conditional_edge to route to the ToolNode if the last message</span></span><br><span class="line"><span class="string">    has tool calls. Otherwise, route to the end.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(state, <span class="built_in">list</span>):</span><br><span class="line">        ai_message = state[-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">elif</span> messages := state.get(<span class="string">&quot;messages&quot;</span>, []):</span><br><span class="line">        ai_message = messages[-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">f&quot;No messages found in input state to tool_edge: <span class="subst">&#123;state&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(ai_message, <span class="string">&quot;tool_calls&quot;</span>) <span class="keyword">and</span> <span class="built_in">len</span>(ai_message.tool_calls) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;tools&quot;</span>  <span class="comment"># path 函数返回的为节点名称</span></span><br><span class="line">    <span class="keyword">return</span> END  <span class="comment"># path 函数返回的为节点名称</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># The `tools_condition` function returns &quot;tools&quot; if the chatbot asks to use a tool, and &quot;END&quot; if</span></span><br><span class="line"><span class="comment"># it is fine directly responding. This conditional routing defines the main agent loop.</span></span><br><span class="line">graph_builder.add_conditional_edges(</span><br><span class="line">    <span class="string">&quot;chatbot&quot;</span>,</span><br><span class="line">    route_tools,</span><br><span class="line">    <span class="comment"># The following dictionary lets you tell the graph to interpret the condition&#x27;s outputs as a specific node</span></span><br><span class="line">    <span class="comment"># It defaults to the identity function, but if you</span></span><br><span class="line">    <span class="comment"># want to use a node named something else apart from &quot;tools&quot;,</span></span><br><span class="line">    <span class="comment"># You can update the value of the dictionary to something else</span></span><br><span class="line">    <span class="comment"># e.g., &quot;tools&quot;: &quot;my_tools&quot;</span></span><br><span class="line">    <span class="comment"># 可选的路径到节点名称的映射。如果省略，则 `path` 返回的路径应该是节点名称。</span></span><br><span class="line">    &#123;<span class="string">&quot;tools&quot;</span>: <span class="string">&quot;tools&quot;</span>, END: END&#125;,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># Any time a tool is called, we return to the chatbot to decide the next step</span></span><br><span class="line">graph_builder.add_edge(<span class="string">&quot;tools&quot;</span>, <span class="string">&quot;chatbot&quot;</span>)</span><br><span class="line">graph_builder.add_edge(START, <span class="string">&quot;chatbot&quot;</span>)</span><br><span class="line"><span class="comment"># graph = graph_builder.compile()</span></span><br><span class="line">memory = MemorySaver()  <span class="comment"># 添加检查点</span></span><br><span class="line">graph = graph_builder.<span class="built_in">compile</span>(checkpointer=memory)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">config = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;thread_id&quot;</span>: <span class="string">&quot;1&quot;</span>&#125;&#125;</span><br><span class="line">events = graph.stream(</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;messages&quot;</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">                <span class="string">&quot;content&quot;</span>: (</span><br><span class="line">                    <span class="string">&quot;I&#x27;m learning LangGraph. &quot;</span></span><br><span class="line">                    <span class="string">&quot;Could you do some research on it for me?&quot;</span></span><br><span class="line">                ),</span><br><span class="line">            &#125;,</span><br><span class="line">        ],</span><br><span class="line">    &#125;,</span><br><span class="line">    config,</span><br><span class="line">    stream_mode=<span class="string">&quot;values&quot;</span>,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">for</span> event <span class="keyword">in</span> events:</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;messages&quot;</span> <span class="keyword">in</span> event:</span><br><span class="line">        event[<span class="string">&quot;messages&quot;</span>][-<span class="number">1</span>].pretty_print()</span><br></pre></td></tr></table></figure>
<br>
<br>
<h2 id="Agentic-Design-Patterns">Agentic Design Patterns</h2>
<h3 id="Design-Pattern">Design Pattern</h3>
<img src="/2025/04/21/AI-Agent/5.png" class="" title="alt text">
<ul>
<li>Short-term memory: 上下文Context</li>
<li>Long-term memory: 外挂数据库，例如RAG技术</li>
<li>Tool: MCP(function call…)</li>
<li>Planning: Reflection，Self-critics…</li>
</ul>
<br>
<h3 id="Reflection">Reflection</h3>
<ul>
<li>“[Self-Refine: Iterative Refinement with Self-Feedback](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.17651?utm_campaign=The">https://arxiv.org/abs/2303.17651?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B),” Madaan et al. (2023)</li>
<li>“[Reflexion: Language Agents with Verbal Reinforcement Learning](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.11366?utm_campaign=The">https://arxiv.org/abs/2303.11366?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B),” Shinn et al. (2023)</li>
<li>“[CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.11738?utm_campaign=The">https://arxiv.org/abs/2305.11738?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B),” Gou et al. (2024)</li>
</ul>
<br>
<h3 id="Tool-Use">Tool Use</h3>
<ul>
<li>“[Gorilla: Large Language Model Connected with Massive APIs](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.15334?utm_campaign=The">https://arxiv.org/abs/2305.15334?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz–9ARMthd09q0ABUi-abo6BH62BLbcwPo13LrXs9hUezs-L050Ay7b_rHdWuRIqBVOD6k_S),” Patil et al. (2023)</li>
<li>“[MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.11381?utm_campaign=The">https://arxiv.org/abs/2303.11381?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz–9ARMthd09q0ABUi-abo6BH62BLbcwPo13LrXs9hUezs-L050Ay7b_rHdWuRIqBVOD6k_S),” Yang et al. (2023)</li>
<li>“[Efficient Tool Use with Chain-of-Abstraction Reasoning](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.17464?utm_campaign=The">https://arxiv.org/abs/2401.17464?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz–9ARMthd09q0ABUi-abo6BH62BLbcwPo13LrXs9hUezs-L050Ay7b_rHdWuRIqBVOD6k_S),” Gao et al. (2024)</li>
</ul>
<br>
<h3 id="Planning">Planning</h3>
<ul>
<li>“[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.11903?utm_campaign=The">https://arxiv.org/abs/2201.11903?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8Kh954rkXmE4vgpKvro3Klpjhn7IuT-Y_eXIYtgVIq9PTzwa5zFWX7FZZqv1tuDEEsTDuY),” Wei et al. (2022)</li>
<li>“[HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.17580?utm_campaign=The">https://arxiv.org/abs/2303.17580?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8Kh954rkXmE4vgpKvro3Klpjhn7IuT-Y_eXIYtgVIq9PTzwa5zFWX7FZZqv1tuDEEsTDuY),” Shen et al. (2023)</li>
<li>“[Understanding the planning of LLM agents: A survey](<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.02716.pdf?utm_campaign=The">https://arxiv.org/pdf/2402.02716.pdf?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8Kh954rkXmE4vgpKvro3Klpjhn7IuT-Y_eXIYtgVIq9PTzwa5zFWX7FZZqv1tuDEEsTDuY),” by Huang et al. (2024)</li>
</ul>
<br>
<h3 id="Multi-Agent-Collaboration">Multi-Agent Collaboration</h3>
<ul>
<li>“[Communicative Agents for Software Development](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.07924?utm_campaign=The">https://arxiv.org/abs/2307.07924?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8TZzur2df1qdnGx09b-Fg94DTsc3-xXao4StKvKNU2HR51el3n8yOm0CPSw6GiAoLQNKua),” Qian et al. (2023) (the ChatDev paper)</li>
<li>“[AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.08155?utm_campaign=The">https://arxiv.org/abs/2308.08155?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8TZzur2df1qdnGx09b-Fg94DTsc3-xXao4StKvKNU2HR51el3n8yOm0CPSw6GiAoLQNKua),” Wu et al. (2023)</li>
<li>“[MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework](<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2308.00352?utm_campaign=The">https://arxiv.org/abs/2308.00352?utm_campaign=The</a> Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8TZzur2df1qdnGx09b-Fg94DTsc3-xXao4StKvKNU2HR51el3n8yOm0CPSw6GiAoLQNKua),” Hong et al. (2023)</li>
</ul>
<br>
<h2 id="MCP">MCP</h2>
<p>Model Context Protocol</p>
<p>ref: <a target="_blank" rel="noopener" href="https://modelcontextprotocol.io/introduction">https://modelcontextprotocol.io/introduction</a></p>
<p>ref: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29001189476">https://zhuanlan.zhihu.com/p/29001189476</a></p>
<img src="/2025/04/21/AI-Agent/v2-2bcd98f6541da0b6f14dc9082ee2dcda_1440w.jpg" class="" title="mcp">
<br>
<p><a target="_blank" rel="noopener" href="https://github.com/modelcontextprotocol/python-sdk/tree/main/examples/clients/simple-chatbot/mcp_simple_chatbot">example</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># client </span></span><br><span class="line">   ... <span class="comment"># 省略了无关的代码</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">start</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># 初始化所有的 mcp server</span></span><br><span class="line">    <span class="keyword">for</span> server <span class="keyword">in</span> self.servers:</span><br><span class="line">        <span class="keyword">await</span> server.initialize()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取所有的 tools 命名为 all_tools</span></span><br><span class="line">    all_tools = []</span><br><span class="line">    <span class="keyword">for</span> server <span class="keyword">in</span> self.servers:</span><br><span class="line">        tools = <span class="keyword">await</span> server.list_tools()</span><br><span class="line">        all_tools.extend(tools)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将所有的 tools 的功能描述格式化成字符串供 LLM 使用</span></span><br><span class="line">    <span class="comment"># tool.format_for_llm() 我放到了这段代码最后，方便阅读。</span></span><br><span class="line">    tools_description = <span class="string">&quot;\n&quot;</span>.join(</span><br><span class="line">        [tool.format_for_llm() <span class="keyword">for</span> tool <span class="keyword">in</span> all_tools]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里就不简化了，以供参考，实际上就是基于 prompt 和当前所有工具的信息</span></span><br><span class="line">    <span class="comment"># 询问 LLM（Claude） 应该使用哪些工具。</span></span><br><span class="line">    system_message = (</span><br><span class="line">        <span class="string">&quot;You are a helpful assistant with access to these tools:\n\n&quot;</span></span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;tools_description&#125;</span>\n&quot;</span></span><br><span class="line">        <span class="string">&quot;Choose the appropriate tool based on the user&#x27;s question. &quot;</span></span><br><span class="line">        <span class="string">&quot;If no tool is needed, reply directly.\n\n&quot;</span></span><br><span class="line">        <span class="string">&quot;IMPORTANT: When you need to use a tool, you must ONLY respond with &quot;</span></span><br><span class="line">        <span class="string">&quot;the exact JSON object format below, nothing else:\n&quot;</span></span><br><span class="line">        <span class="string">&quot;&#123;\n&quot;</span></span><br><span class="line">        <span class="string">&#x27;    &quot;tool&quot;: &quot;tool-name&quot;,\n&#x27;</span></span><br><span class="line">        <span class="string">&#x27;    &quot;arguments&quot;: &#123;\n&#x27;</span></span><br><span class="line">        <span class="string">&#x27;        &quot;argument-name&quot;: &quot;value&quot;\n&#x27;</span></span><br><span class="line">        <span class="string">&quot;    &#125;\n&quot;</span></span><br><span class="line">        <span class="string">&quot;&#125;\n\n&quot;</span></span><br><span class="line">        <span class="string">&quot;After receiving a tool&#x27;s response:\n&quot;</span></span><br><span class="line">        <span class="string">&quot;1. Transform the raw data into a natural, conversational response\n&quot;</span></span><br><span class="line">        <span class="string">&quot;2. Keep responses concise but informative\n&quot;</span></span><br><span class="line">        <span class="string">&quot;3. Focus on the most relevant information\n&quot;</span></span><br><span class="line">        <span class="string">&quot;4. Use appropriate context from the user&#x27;s question\n&quot;</span></span><br><span class="line">        <span class="string">&quot;5. Avoid simply repeating the raw data\n\n&quot;</span></span><br><span class="line">        <span class="string">&quot;Please use only the tools that are explicitly defined above.&quot;</span></span><br><span class="line">    )</span><br><span class="line">    messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: system_message&#125;]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># Final... 假设这里已经处理了用户消息输入.</span></span><br><span class="line">        messages.append(&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: user_input&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 system_message 和用户消息输入一起发送给 LLM</span></span><br><span class="line">        llm_response = self.llm_client.get_response(messages)</span><br><span class="line"></span><br><span class="line">    ... <span class="comment"># 后面和确定使用哪些工具无关</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># server</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tool</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Represents a tool with its properties and formatting.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, name: <span class="built_in">str</span>, description: <span class="built_in">str</span>, input_schema: <span class="built_in">dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>]</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.name: <span class="built_in">str</span> = name</span><br><span class="line">        self.description: <span class="built_in">str</span> = description</span><br><span class="line">        self.input_schema: <span class="built_in">dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>] = input_schema</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把工具的名字 / 工具的用途（description）和工具所需要的参数（args_desc）转化为文本</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">format_for_llm</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Format tool information for LLM.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            A formatted string describing the tool.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        args_desc = []</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;properties&quot;</span> <span class="keyword">in</span> self.input_schema:</span><br><span class="line">            <span class="keyword">for</span> param_name, param_info <span class="keyword">in</span> self.input_schema[<span class="string">&quot;properties&quot;</span>].items():</span><br><span class="line">                arg_desc = (</span><br><span class="line">                    <span class="string">f&quot;- <span class="subst">&#123;param_name&#125;</span>: <span class="subst">&#123;param_info.get(<span class="string">&#x27;description&#x27;</span>, <span class="string">&#x27;No description&#x27;</span>)&#125;</span>&quot;</span></span><br><span class="line">                )</span><br><span class="line">                <span class="keyword">if</span> param_name <span class="keyword">in</span> self.input_schema.get(<span class="string">&quot;required&quot;</span>, []):</span><br><span class="line">                    arg_desc += <span class="string">&quot; (required)&quot;</span></span><br><span class="line">                args_desc.append(arg_desc)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Tool: <span class="subst">&#123;self.name&#125;</span></span></span><br><span class="line"><span class="string">Description: <span class="subst">&#123;self.description&#125;</span></span></span><br><span class="line"><span class="string">Arguments:</span></span><br><span class="line"><span class="string"><span class="subst">&#123;<span class="built_in">chr</span>(<span class="number">10</span>).join(args_desc)&#125;</span></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<br>
<h2 id="微调数据集构建">微调数据集构建</h2>
<h3 id="Chunking-2">Chunking</h3>
<p>分块方案参考上文</p>
<h3 id="问题创建-问题求解">问题创建&amp;问题求解</h3>
<p>针对于分割好的文本块，通过LLM为其生成对应问题</p>
<br>
<h2 id="KBQA">KBQA</h2>
<p>Knowledge Based Question Answering (KBQA) aims to answer factual questions based on the provided knowledge base (KB).</p>
<p>ref: <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Awesome-KBQA">https://github.com/RUCAIBox/Awesome-KBQA</a></p>
<br>
<h3 id="Semantic-Parsing-based-Methods">Semantic Parsing-based Methods</h3>
<br>
<h3 id="Information-retrieval-based-Methods">Information retrieval-based Methods</h3>
<br>
<h3 id="Other-Methods">Other Methods</h3>
<br>
<h2 id="RAG">RAG</h2>
<p>技术入门：<a target="_blank" rel="noopener" href="https://github.com/fareedkhan-dev/all-rag-techniques">https://github.com/fareedkhan-dev/all-rag-techniques</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/NirDiamant/RAG_Techniques">https://github.com/NirDiamant/RAG_Techniques</a></p>
<h3 id="RAG-Design">RAG Design</h3>
<p>ref: <a target="_blank" rel="noopener" href="https://abdullin.com/ilya/how-to-build-best-rag/">https://abdullin.com/ilya/how-to-build-best-rag/</a></p>
<h3 id="Effective-Chunking-Strategies-for-RAG">Effective Chunking Strategies for RAG</h3>
<p><a target="_blank" rel="noopener" href="https://docs.cohere.com/page/chunking-strategies">https://docs.cohere.com/page/chunking-strategies</a></p>
<h3 id="Embedding">Embedding</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/FlagOpen/FlagEmbedding">https://github.com/FlagOpen/FlagEmbedding</a></p>
<h3 id="Reranker">Reranker</h3>
<p><a target="_blank" rel="noopener" href="https://www.pinecone.io/learn/series/rag/rerankers/">https://www.pinecone.io/learn/series/rag/rerankers/</a></p>
<p>好的，我们来完成这三项关于 RAG (Retrieval-Augmented Generation) 扩展方法的详细说明。</p>
<hr>
<h3 id="Extended-Method">Extended Method</h3>
<h4 id="假设性文档嵌入-Hypothetical-Document-Embedding-HDE">假设性文档嵌入 (Hypothetical Document Embedding - HDE)</h4>
<p><strong>核心思想：</strong><br>
与其直接对用户的简短、模糊的查询（Query）进行嵌入，不如先让一个大型语言模型 (LLM) 根据该查询生成一个它认为最理想、最完整的“假设性答案”或“假设性文档”，然后对这个内容更丰富、语义更明确的假设性文档进行嵌入，再用其生成的向量去知识库中进行向量检索。</p>
<p><strong>解决的问题：</strong><br>
它主要解决“<strong>查询与文档之间的语义鸿沟 (Semantic Gap)</strong>”问题。用户的查询通常很短（如“什么是光合作用？”），而知识库中的文档通常很长，包含详细的解释、背景和例子。这两种不同形式的文本在嵌入空间中的向量可能相距甚远，导致检索效果不佳。HDE通过生成一个与目标文档在格式、长度和内容上都更相似的“中间文档”，极大地缩小了这一鸿沟。</p>
<p><strong>工作原理：</strong></p>
<ol>
<li>
<p><strong>接收查询 (Receive Query):</strong> 用户输入一个查询，例如：“如何为我的初创公司进行种子轮融资？”</p>
</li>
<li>
<p><strong>生成假设性文档 (Generate Hypothetical Document):</strong> 系统将该查询发送给一个 LLM，并提示它：“请生成一份详细回答‘如何为我的初创公司进行种子轮融资？’这个问题的文档。”</p>
</li>
<li>
<p><strong>LLM 输出:</strong> LLM 可能会生成如下的假设性文档：</p>
<blockquote>
<p>“为初创公司进行种子轮融资通常涉及几个关键步骤。首先，你需要一个清晰的商业计划书、一个最小可行产品（MVP）和一个有说服力的演讲稿。其次，建立你的投资人网络至关重要，可以从天使投资人和早期风险投资机构开始。在接触投资人时，要清晰地阐述你的市场机会、团队优势和财务预测…”</p>
</blockquote>
</li>
<li>
<p><strong>嵌入假设性文档 (Embed Hypothetical Document):</strong> 系统将这个由 LLM 生成的、内容丰富的假设性文档通过嵌入模型（Embedding Model）转换成一个向量。</p>
</li>
<li>
<p><strong>执行检索 (Perform Retrieval):</strong> 使用这个新生成的向量，在文档/知识库的向量索引中进行相似性搜索，找出与这个“理想答案”最匹配的实际文档。</p>
</li>
<li>
<p><strong>生成最终答案 (Generate Final Answer):</strong> 将检索到的文档和原始查询一起提供给 LLM，生成最终的、有事实依据的答案。</p>
</li>
</ol>
<p><strong>优势：</strong></p>
<ul>
<li><strong>提高检索精度：</strong> 显著改善了查询和文档之间的语义匹配度，能找到更相关的内容。</li>
<li><strong>处理模糊查询：</strong> 对于意图不明确或过于简短的查询，效果提升尤为明显。</li>
</ul>
<p><strong>挑战与考量：</strong></p>
<ul>
<li><strong>增加延迟：</strong> 在检索前增加了一次 LLM 调用，导致整体响应时间变长。</li>
<li><strong>增加成本：</strong> 额外的 LLM 调用会产生额外费用。</li>
<li><strong>依赖生成质量：</strong> 假设性文档的质量直接决定了检索效果的好坏。</li>
</ul>
<hr>
<h4 id="迭代式检索生成-Iterative-Retrieval-Generation-IRG">迭代式检索生成 (Iterative Retrieval Generation - IRG)</h4>
<p><strong>核心思想：</strong><br>
将检索和生成过程从“一次性完成”的线性流程，转变为一个循环、迭代、逐步求精的过程。系统会根据初步生成的答案或中间思考，主动发现知识缺口，并生成新的查询以获取更多信息，直到能够形成一个完整、准确的答案为止。</p>
<p><strong>解决的问题：</strong><br>
它主要解决<strong>复杂、多步骤或需要整合多方面知识的查询 (Complex, Multi-hop Questions)</strong>。对于这类问题，单次检索往往无法获取所有必要的信息。例如，“比较一下《指环王》和《冰与火之歌》在世界构建和角色塑造上的异同？”</p>
<p><strong>工作原理：</strong></p>
<ol>
<li><strong>初始检索 (Initial Retrieval):</strong> 系统接收初始查询，并进行第一次检索，获取一批相关文档。</li>
<li><strong>初步生成与分析 (Initial Generation &amp; Analysis):</strong> LLM 基于第一次检索到的文档，尝试生成答案。在生成过程中，它会识别出当前信息中的不足之处。例如，它可能找到了关于《指环王》世界构建的资料，但缺少《冰与火之歌》角色塑造的细节。</li>
<li><strong>生成新查询 (Generate New Queries):</strong> 基于识别出的知识缺口，系统（或 LLM 本身）会生成一个或多个新的、更具针对性的查询。例如：“《冰与火之歌》中的主要角色塑造技巧”、“《指环王》的魔法系统设定”。</li>
<li><strong>迭代检索 (Iterative Retrieval):</strong> 系统使用这些新查询去知识库中进行新一轮的检索，获取补充信息。</li>
<li><strong>整合与精炼 (Integration &amp; Refinement):</strong> 将所有轮次检索到的文档进行整合，并交由 LLM 进行最终的综合性回答。</li>
<li><strong>循环终止：</strong> 当 LLM 判断信息已经足够全面，或者达到预设的迭代次数时，循环结束。</li>
</ol>
<p><strong>优势：</strong></p>
<ul>
<li><strong>处理复杂问题：</strong> 能够像人类研究员一样，逐步深入，解决需要多方面信息支持的复杂问题。</li>
<li><strong>提高答案的全面性和深度：</strong> 通过补充检索，确保答案覆盖了问题的所有方面。</li>
</ul>
<p><strong>挑战与考量：</strong></p>
<ul>
<li><strong>高延迟和高成本：</strong> 多次检索和生成步骤显著增加了响应时间和计算成本。</li>
<li><strong>逻辑复杂性：</strong> 实现一个有效的迭代循环、判断何时停止以及如何生成有效的后续查询，技术上更具挑战性。</li>
<li><strong>可能陷入无效循环：</strong> 如果后续查询生成得不好，可能会导致检索不到有用信息，陷入低效循环。</li>
</ul>
<hr>
<h4 id="优化稠密加稀疏检索-Optimized-Dense-Sparse-Retrieval">优化稠密加稀疏检索 (Optimized Dense + Sparse Retrieval)</h4>
<p><strong>核心思想：</strong><br>
这是一种混合检索（Hybrid Retrieval）策略，它结合了两种主流检索技术的优点：<strong>稠密检索 (Dense Retrieval)</strong> 和 <strong>稀疏检索 (Sparse Retrieval)</strong>，以实现比单一技术更强大、更鲁棒的检索效果。</p>
<ul>
<li>
<p><strong>稠密检索 (Dense Retrieval):</strong></p>
<ul>
<li><strong>技术:</strong> 基于向量嵌入（Embeddings）和向量相似性搜索（如余弦相似度）。</li>
<li><strong>优点:</strong> 擅长理解<strong>语义和上下文</strong>。能够匹配意思相近但用词不同的查询和文档（例如，查询“美国总统官邸”能匹配到包含“白宫”的文档）。</li>
<li><strong>缺点:</strong> 对于<strong>关键词、专业术语、ID 或缩写</strong>的精确匹配能力较弱。</li>
</ul>
</li>
<li>
<p><strong>稀疏检索 (Sparse Retrieval):</strong></p>
<ul>
<li><strong>技术:</strong> 基于关键词频率和分布的传统信息检索方法（如 BM25、TF-IDF）。</li>
<li><strong>优点:</strong> 擅长<strong>精确匹配关键词</strong>。对于包含特定术语、产品型号（如“iPhone 15 Pro”）或代码标识符的查询，效果极佳。计算速度快，资源消耗低。</li>
<li><strong>缺点:</strong> 无法理解语义。无法匹配同义词或近义词（查询“汽车”无法匹配到包含“automobile”的文档）。</li>
</ul>
</li>
</ul>
<p><strong>工作原理 (优化结合):</strong></p>
<ol>
<li><strong>并行检索 (Parallel Retrieval):</strong> 当用户输入查询时，系统会<strong>同时</strong>使用两种方法进行检索：
<ul>
<li><strong>稠密路径：</strong> 将查询嵌入为向量，在向量数据库中搜索最相似的文档。</li>
<li><strong>稀疏路径：</strong> 对查询进行分词，在倒排索引（如 Elasticsearch）中使用 BM25 算法搜索最相关的文档。</li>
</ul>
</li>
<li><strong>结果融合 (Result Fusion):</strong> 系统会得到两个独立的、按相关性排序的文档列表。下一步是智能地将这两个列表融合成一个最终列表。最常用的融合算法是 <strong>“倒数排序融合” (Reciprocal Rank Fusion - RRF)</strong>。
<ul>
<li><strong>RRF 算法：</strong> 该算法不关心每个检索系统的原始分数（因为它们的量纲不同），只关心文档在各自列表中的<strong>排名 (Rank)</strong>。一个文档的最终分数是它在每个列表中排名的倒数之和。排名越靠前，倒数越大，最终分数也越高。</li>
<li><strong>公式:</strong> <code>RRF_Score(doc) = Σ (1 / (k + rank_i(doc)))</code>，其中 <code>rank_i(doc)</code> 是文档在第 <code>i</code> 个检索结果列表中的排名，<code>k</code> 是一个常数（通常设为60），用于降低低排名文档的影响。</li>
</ul>
</li>
<li><strong>最终排序与截断 (Final Ranking &amp; Truncation):</strong> 根据 RRF 分数对所有文档进行重新排序，并选取排名最高的 Top-K 个文档，用于后续的生成环节。</li>
</ol>
<p><strong>优势：</strong></p>
<ul>
<li><strong>取长补短：</strong> 结合了语义理解和关键词匹配的能力，是目前最有效的通用检索策略之一。</li>
<li><strong>鲁棒性强：</strong> 即使一种检索方法效果不佳，另一种方法仍可能召回相关的文档，大大降低了检索失败的风险。</li>
<li><strong>显著提升相关性：</strong> 在多数场景下，混合检索召回的文档质量远高于任何单一检索方法。</li>
</ul>
<p><strong>挑战与考量：</strong></p>
<ul>
<li><strong>系统复杂性：</strong> 需要同时维护向量数据库和关键词索引两种基础设施。</li>
<li><strong>调优挑战：</strong> 融合策略（如 RRF中的 <code>k</code> 值）或不同检索结果的权重需要进行实验和调优，以达到最佳效果。</li>
</ul>
<h3 id="Improved-RAG">Improved RAG</h3>
<h4 id="GraphRAG">GraphRAG</h4>
<p><a target="_blank" rel="noopener" href="https://microsoft.github.io/graphrag/">https://microsoft.github.io/graphrag/</a></p>
<ol>
<li>
<p>Source Documents → Text Chunks</p>
<p>To start, the documents in the corpus are split into text chunks. The LLM extracts information from each chunk for downstream processing.</p>
</li>
<li>
<p>Text Chunks → Entities &amp; Relationships</p>
<p>the LLM is prompted（throught few-shot prompt） to extract instances of important entities and the relationships between the entities from a given chunk. Additionally, the LLM generates short descriptions for the entities and relationships</p>
<img src="/2025/04/21/AI-Agent/image-20250604165407470.png" class="" title="image-20250604165407470">
<p>The LLM can also be prompted to extract claims about detected entities. Claims are important factual statements about entities, such as dates, events, and interactions with other entities. As with entities and relationships, in-context learning exemplars can provide domain-specific guidance.</p>
<img src="/2025/04/21/AI-Agent/image-20250604170724823.png" class="" title="image-20250604170724823">
</li>
<li>
<p>Entities &amp; Relationships → Knowledge Graph</p>
<p>In the final step of the knowledge graph extraction process, these instances of entities and relationships become individual nodes and edges in the graph. Entity descriptions are aggregated and summarized for each node and edge. Relationships are aggregated into graph edges, where the number of duplicates for a given relationship becomes edge weights. Claims are aggregated similarly</p>
</li>
<li>
<p>Knowledge Graph → Graph Communities</p>
<p>we use Leiden community detection (Traag et al., 2019) in a hierarchical manner, recursively detecting sub-communities within each detected community until reaching leaf communities that can no longer be partitioned. Each level of this hierarchy provides a community partition that covers the nodes of the graph in a mutually exclusive, collectively exhaustive way, enabling divide-and-conquer global summarization.</p>
</li>
<li>
<p>Graph Communities → Community Summaries</p>
<img src="/2025/04/21/AI-Agent/image-20250604173158175.png" class="" title="image-20250604173158175">
</li>
<li>
<p>Community Summaries → Community Answers → Global Answer</p>
<img src="/2025/04/21/AI-Agent/image-20250604173209083.png" class="" title="image-20250604173209083">
</li>
</ol>
<h4 id="LightRAG">LightRAG</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2410.05779">arxiv.org/abs/2410.05779</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/HKUDS/LightRAG">https://github.com/HKUDS/LightRAG</a></p>
<img src="/2025/04/21/AI-Agent/b2aaf634151b4706892693ffb43d9093.png" class="" title="LightRAG Diagram">
<h2 id="Prompt-Engineering">Prompt Engineering</h2>
<p>ref: <a target="_blank" rel="noopener" href="https://dannyzheng.me/2025/02/21/prompt-engineering/#the-prompt-engineering-lifecycle">https://dannyzheng.me/2025/02/21/prompt-engineering/#the-prompt-engineering-lifecycle</a></p>
<img src="/2025/04/21/AI-Agent/prompt_components.png" class="" title="img">
<h3 id="Base-2">Base</h3>
<h4 id="Zero-Shot-Prompting">Zero-Shot Prompting</h4>
<p>Zero-Shot Prompting 是指在不给模型任何示例的情况下，直接提出问题或任务。该方法依赖于模型的预训练知识，适用于模型已广泛学习相关领域信息的情况。例如：</p>
<blockquote>
<p><strong>Prompt</strong>：将下面这句话翻译成英文：我喜欢学习人工智能。<br>
<strong>Output</strong>：I like studying artificial intelligence.</p>
</blockquote>
<p>该方法简单高效，适用于任务明确、模型已具备相关背景知识的场景。</p>
<br>
<h4 id="Few-Shot-Prompting">Few-Shot Prompting</h4>
<p>Few-Shot Prompting 是在提示中提供少量（通常是1-5个）示例，以帮助模型理解任务格式或逻辑。这种方式可以显著提升模型在结构化任务中的表现。例如：</p>
<blockquote>
<p><strong>Prompt</strong>：<br>
翻译下列句子：<br>
例1：我爱编程。 → I love programming.<br>
例2：天气很好。 → The weather is nice.<br>
请翻译：我在看书。<br>
<strong>Output</strong>：I am reading a book.</p>
</blockquote>
<p>Few-Shot Prompting 利用“类比”方式，帮助模型对任务形成更明确的理解。</p>
<br>
<h4 id="Chain-of-Thought-CoT-Prompting">Chain-of-Thought (CoT) Prompting</h4>
<p>ref: <a target="_blank" rel="noopener" href="https://dannyzheng.me/2025/02/21/prompt-engineering/#the-prompt-engineering-lifecycle">https://dannyzheng.me/2025/02/21/prompt-engineering/#the-prompt-engineering-lifecycle</a></p>
<blockquote>
<p>CoT tip: Always have LLM output its thinking. Without outputting its thought process, no thinking occurs!</p>
</blockquote>
<ul>
<li>
<p>Basic prompt: Include “Think step-by-step” in your prompt.</p>
<ul>
<li>
<p>Lacks guidance on how to think (which is especially not ideal if a task is very specific to your app, use case, or organization)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Draft personalized emails to donors asking for contributions to this year’s Care for Kids program.</span><br><span class="line"></span><br><span class="line">Program information:</span><br><span class="line">&lt;program&gt;&#123;&#123;PROGRAM_DETAILS&#125;&#125;</span><br><span class="line">&lt;/program&gt;</span><br><span class="line"></span><br><span class="line">Donor information:</span><br><span class="line">&lt;donor&gt;&#123;&#123;DONOR_DETAILS&#125;&#125;</span><br><span class="line">&lt;/donor&gt;</span><br><span class="line"></span><br><span class="line">Think step-by-step before you write the email.</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>Guided prompt: Outline specific steps for LLM to follow in its thinking process.</p>
<ul>
<li>
<p>Lacks structuring to make it easy to strip out and separate the answer from the thinking.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Draft personalized emails to donors asking for contributions to this year’s Care for Kids program.</span><br><span class="line"></span><br><span class="line">Program information:</span><br><span class="line">&lt;program&gt;&#123;&#123;PROGRAM_DETAILS&#125;&#125;</span><br><span class="line">&lt;/program&gt;</span><br><span class="line"></span><br><span class="line">Donor information:</span><br><span class="line">&lt;donor&gt;&#123;&#123;DONOR_DETAILS&#125;&#125;</span><br><span class="line">&lt;/donor&gt;</span><br><span class="line"></span><br><span class="line">Think before you write the email. First, think through what messaging might appeal to this donor given their donation history and which campaigns they’ve supported in the past. Then, think through what aspects of the Care for Kids program would appeal to them, given their history. Finally, write the personalized donor email using your analysis.</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li>
<p>Structured prompt: Use XML tags like &lt;think&gt; &lt;/think&gt; and &lt;answer&gt; &lt;/answer&gt; to separate reasoning from the final answer.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Draft personalized emails to donors asking for contributions to this year’s Care for Kids program.</span><br><span class="line"></span><br><span class="line">Program information:</span><br><span class="line">&lt;program&gt;&#123;&#123;PROGRAM_DETAILS&#125;&#125;</span><br><span class="line">&lt;/program&gt;</span><br><span class="line"></span><br><span class="line">Donor information:</span><br><span class="line">&lt;donor&gt;&#123;&#123;DONOR_DETAILS&#125;&#125;</span><br><span class="line">&lt;/donor&gt;</span><br><span class="line"></span><br><span class="line">Think before you write the email in &lt;thinking&gt; tags. First, think through what messaging might appeal to this donor given their donation history and which campaigns they’ve supported in the past. Then, think through what aspects of the Care for Kids program would appeal to them, given their history. Finally, write the personalized donor email in &lt;email&gt; tags, using your analysis.</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Use code (e.g. extract from <code>&lt;answer&gt;</code> tags) to extract the desired answer from the LLM’s response</p>
</blockquote>
</li>
</ul>
<br>
<h4 id="Self-Consistency-Prompting">Self-Consistency Prompting</h4>
<p>Self-Consistency Prompting 是 Chain-of-Thought 的一种增强策略。其核心思想是：在面对复杂推理问题时，通过生成多个不同的推理路径（多次生成），然后对这些路径的最终结果进行投票或汇总，以提高答案的可靠性和稳定性。</p>
<p>例如，对于一道数学题，模型可能在不同尝试中给出不同的思路或答案，但通过“多数投票”可以获得更一致、准确的结果。</p>
<blockquote>
<p><strong>应用示例</strong>：</p>
<ul>
<li>生成5条不同的推理路径</li>
<li>汇总5个最终答案</li>
<li>选择出现频率最高的一个作为最终输出</li>
</ul>
</blockquote>
<p>这种方法特别适合对单次输出不够稳定的任务，比如复杂逻辑、数学题等场景。</p>
<br>
<h4 id="Retrieval-Augmented-Generation-RAG">Retrieval-Augmented Generation (RAG)</h4>
<p>RAG 是将外部知识检索机制（如文档、数据库、搜索引擎）与语言模型生成能力结合起来的一种方法。模型在回答问题前，会先“检索”相关内容，再基于检索结果生成答案。</p>
<p>这种方法克服了大模型“记忆有限”的问题，尤其在处理需要时效性或特定背景知识的任务时非常有效。</p>
<blockquote>
<p><strong>示例流程</strong>：</p>
<ol>
<li>用户提问：“请解释什么是量子纠缠？”</li>
<li>模型调用检索模块，从知识库或网络中找到高相关资料</li>
<li>基于资料，生成符合上下文、准确可靠的回答</li>
</ol>
</blockquote>
<p>RAG 适用于问答系统、知识密集型对话系统、企业内部知识库应用等场景。</p>
<br>
<h3 id="Medprompt">Medprompt</h3>
<p>MedPrompt is composed of the following prompting techniques:</p>
<ul>
<li>Dynamic few-shot selection: instead of using static few-shot examples, Medprompt selects few-shot examples dynamically based on the question.</li>
<li>Self-generated chain of thought.</li>
<li>Choice shuffle ensembling: performs choice shuffle and self-consistency prompting.</li>
</ul>
<img src="/2025/04/21/AI-Agent/Medqa-comp.png" class="" title="img">
<h2 id="其他">其他</h2>
<h3 id="RAG总结">RAG总结</h3>
<p><a target="_blank" rel="noopener" href="https://holistic-authority-5c2.notion.site/RAG-1e3f66b636ba80f1b425dc15d7fae04c">https://holistic-authority-5c2.notion.site/RAG-1e3f66b636ba80f1b425dc15d7fae04c</a></p>
<h3 id="Cursor实现每秒-1000-tokens-的文件编辑">Cursor实现每秒 1000 tokens 的文件编辑</h3>
<p><a target="_blank" rel="noopener" href="https://web.archive.org/web/20240605010559/https://www.cursor.com/blog/instant-apply">https://web.archive.org/web/20240605010559/https://www.cursor.com/blog/instant-apply</a></p>
<p>第一阶段 - 规划（Planning）：使用 GPT 或 Claude 等强大模型 通过聊天界面理解用户需求，制定编辑计划</p>
<p>第二阶段 - 代码生成（Code Generation）：同样使用 Claude/GPT 等大模型 根据规划生成具体的代码内容和编辑指令 输出类似&quot;在这个位置添加这段代码&quot;的详细变更描述</p>
<p>第三阶段 - 快速应用（Fast Apply）：使用 Cursor 自己训练的 Llama-3-70b-ft 模型 专门负责将第二阶段生成的代码变更快速、准确地应用到实际文件中 通过 speculative edits 技术实现 ~1000 tokens/秒的文件重写速度</p>
<p>关键点：前两个阶段：Claude/GPT 负责思考和生成代码内容 第三个阶段：Cursor 自训练模型负责高速执行文件操作</p>
<p>为什么需要第三阶段的专门模型？ 1. 速度瓶颈：Claude/GPT 在文件编辑时太慢，会打断编程流程 2. 技术限制：无法在第三方 API 中实现 speculative edits 优化 3. 准确性问题：大模型在 diff 格式上容易出错，经常有语法错误或&quot;懒惰&quot;行为 4. 成本控制：避免每次文件操作都调用昂贵的商业 API 这种设计让大模型专注于它们擅长的推理和代码生成，而让专门优化的模型处理精确、高速的文件编辑操作。</p>
<br>
<h3 id="幻觉处理">幻觉处理</h3>
<p><a target="_blank" rel="noopener" href="https://cvs-health.github.io/uqlm/latest/index.html">https://cvs-health.github.io/uqlm/latest/index.html</a></p>
<p>黑盒评分器</p>
<img src="/2025/04/21/AI-Agent/black_box_graphic.png" class="" title="_images&#x2F;black_box_graphic.png">
<p>白盒评分器</p>
<img src="/2025/04/21/AI-Agent/white_box_graphic.png" class="" title="_images&#x2F;white_box_graphic.png">
<p>LLM评分</p>
<img src="/2025/04/21/AI-Agent/judges_graphic.png" class="" title="_images&#x2F;judges_graphic.png">
<p>集成评分器</p>
<img src="/2025/04/21/AI-Agent/uqensemble_generate_score.png" class="" title="_images&#x2F;uqensemble_generate_score.png">
<br>
<p>GPT记忆</p>
<p>ref: <a target="_blank" rel="noopener" href="https://macro.com/app/md/54115a42-3409-4f5b-9120-f144d3ecd23a">https://macro.com/app/md/54115a42-3409-4f5b-9120-f144d3ecd23a</a></p>
<img src="/2025/04/21/AI-Agent/GsmyMNLasAQ_DY6" class="" title="img">
<img src="/2025/04/21/AI-Agent/GsmySEdaQAACdBZ" class="" title="img">
<br>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python-Pytorch/" rel="tag"># Python, Pytorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/04/16/Search-Ads-Rec-Rela/" rel="prev" title="SAR&Rela">
      <i class="fa fa-chevron-left"></i> SAR&Rela
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/04/22/Machine-Learning/" rel="next" title="Machine-Learning">
      Machine-Learning <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">AI-Agent</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LangChain"><span class="nav-text">LangChain</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model"><span class="nav-text">Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Key-methods"><span class="nav-text">Key methods</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tool-calling"><span class="nav-text">Tool calling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Structured-Output"><span class="nav-text">Structured Output</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multimodality"><span class="nav-text">Multimodality</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Memory"><span class="nav-text">Memory</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86"><span class="nav-text">短期记忆</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%BC%96%E8%BE%91%E6%B6%88%E6%81%AF%E5%88%97%E8%A1%A8"><span class="nav-text">编辑消息列表</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E8%BF%87%E5%BE%80%E5%AF%B9%E8%AF%9D"><span class="nav-text">总结过往对话</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86"><span class="nav-text">长期记忆</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%AE%B0%E5%BF%86%E5%AD%98%E5%82%A8"><span class="nav-text">记忆存储</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86%E6%80%9D%E8%80%83%E6%A1%86%E6%9E%B6"><span class="nav-text">长期记忆思考框架</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%B0%E5%BF%86%E7%B1%BB%E5%9E%8B"><span class="nav-text">记忆类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%B0%E5%BF%86%E5%86%99%E5%85%A5"><span class="nav-text">记忆写入</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Caching"><span class="nav-text">Caching</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Prompt"><span class="nav-text">Prompt</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PromptTemplate"><span class="nav-text">PromptTemplate</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Semantic-search-engine"><span class="nav-text">Semantic search engine</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Chunking"><span class="nav-text">Chunking</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Better-Chunking"><span class="nav-text">Better Chunking</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Embedding-Vector-Store"><span class="nav-text">Embedding &amp; Vector Store</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Retrieval"><span class="nav-text">Retrieval</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Agent"><span class="nav-text">Agent</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Base"><span class="nav-text">Base</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#QA-Processing-Method"><span class="nav-text">QA Processing Method</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Streaming-message-Streaming-tokens"><span class="nav-text">Streaming message &amp; Streaming tokens</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Memory-Adding"><span class="nav-text">Memory Adding</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Example"><span class="nav-text">Example</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Extraction-Chain"><span class="nav-text">Extraction Chain</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Schema-Extrator"><span class="nav-text">Schema &amp; Extrator</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RAG-based-long-context-extraction"><span class="nav-text">RAG based long context extraction</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Classify"><span class="nav-text">Classify</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LangGraph"><span class="nav-text">LangGraph</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Graph"><span class="nav-text">Graph</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Agentic-Design-Patterns"><span class="nav-text">Agentic Design Patterns</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Design-Pattern"><span class="nav-text">Design Pattern</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reflection"><span class="nav-text">Reflection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tool-Use"><span class="nav-text">Tool Use</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Planning"><span class="nav-text">Planning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Agent-Collaboration"><span class="nav-text">Multi-Agent Collaboration</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MCP"><span class="nav-text">MCP</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA"><span class="nav-text">微调数据集构建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Chunking-2"><span class="nav-text">Chunking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E5%88%9B%E5%BB%BA-%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3"><span class="nav-text">问题创建&amp;问题求解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KBQA"><span class="nav-text">KBQA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Semantic-Parsing-based-Methods"><span class="nav-text">Semantic Parsing-based Methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Information-retrieval-based-Methods"><span class="nav-text">Information retrieval-based Methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-Methods"><span class="nav-text">Other Methods</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RAG"><span class="nav-text">RAG</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RAG-Design"><span class="nav-text">RAG Design</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Effective-Chunking-Strategies-for-RAG"><span class="nav-text">Effective Chunking Strategies for RAG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Embedding"><span class="nav-text">Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reranker"><span class="nav-text">Reranker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Extended-Method"><span class="nav-text">Extended Method</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%81%87%E8%AE%BE%E6%80%A7%E6%96%87%E6%A1%A3%E5%B5%8C%E5%85%A5-Hypothetical-Document-Embedding-HDE"><span class="nav-text">假设性文档嵌入 (Hypothetical Document Embedding - HDE)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3%E5%BC%8F%E6%A3%80%E7%B4%A2%E7%94%9F%E6%88%90-Iterative-Retrieval-Generation-IRG"><span class="nav-text">迭代式检索生成 (Iterative Retrieval Generation - IRG)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%A8%A0%E5%AF%86%E5%8A%A0%E7%A8%80%E7%96%8F%E6%A3%80%E7%B4%A2-Optimized-Dense-Sparse-Retrieval"><span class="nav-text">优化稠密加稀疏检索 (Optimized Dense + Sparse Retrieval)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Improved-RAG"><span class="nav-text">Improved RAG</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GraphRAG"><span class="nav-text">GraphRAG</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LightRAG"><span class="nav-text">LightRAG</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Prompt-Engineering"><span class="nav-text">Prompt Engineering</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Base-2"><span class="nav-text">Base</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Zero-Shot-Prompting"><span class="nav-text">Zero-Shot Prompting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Few-Shot-Prompting"><span class="nav-text">Few-Shot Prompting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Chain-of-Thought-CoT-Prompting"><span class="nav-text">Chain-of-Thought (CoT) Prompting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Self-Consistency-Prompting"><span class="nav-text">Self-Consistency Prompting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Retrieval-Augmented-Generation-RAG"><span class="nav-text">Retrieval-Augmented Generation (RAG)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Medprompt"><span class="nav-text">Medprompt</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-text">其他</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RAG%E6%80%BB%E7%BB%93"><span class="nav-text">RAG总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cursor%E5%AE%9E%E7%8E%B0%E6%AF%8F%E7%A7%92-1000-tokens-%E7%9A%84%E6%96%87%E4%BB%B6%E7%BC%96%E8%BE%91"><span class="nav-text">Cursor实现每秒 1000 tokens 的文件编辑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%BB%E8%A7%89%E5%A4%84%E7%90%86"><span class="nav-text">幻觉处理</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="marigo1d"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">marigo1d</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/marigo1d" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;marigo1d" rel="noopener" target="_blank">GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">marigo1d</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">529k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">16:01</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
