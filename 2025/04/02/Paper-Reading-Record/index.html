<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"marigo1d.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Paper阅读记录">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper-Reading-Record">
<meta property="og:url" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/index.html">
<meta property="og:site_name" content="Marigold">
<meta property="og:description" content="Paper阅读记录">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/v2-ee9b5d4a0761d2d1d10acb37cebefba3_1440w.jpg">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/image-20250530104004180.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/image-20250530112142071.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/image-20250530110615396.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/image-20250530145000198.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/image-20250603133711303.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/20250410155409605.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/20250410214653107.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/20250421211315671.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/image-20250613211803671.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/image-20250604100042926.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/image-20250613211948220.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/image-20250601135610999.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/image-20250605114032032.png">
<meta property="og:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/image-20250605114141206.png">
<meta property="article:published_time" content="2025-04-02T07:21:11.000Z">
<meta property="article:modified_time" content="2025-06-25T06:30:35.008Z">
<meta property="article:author" content="marigo1d">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/v2-ee9b5d4a0761d2d1d10acb37cebefba3_1440w.jpg">

<link rel="canonical" href="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Paper-Reading-Record | Marigold</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Marigold</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Salt, Pepper and Birds~</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://marigo1d.github.io/2025/04/02/Paper-Reading-Record/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="marigo1d">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Marigold">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Paper-Reading-Record
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-04-02 15:21:11" itemprop="dateCreated datePublished" datetime="2025-04-02T15:21:11+08:00">2025-04-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-06-25 14:30:35" itemprop="dateModified" datetime="2025-06-25T14:30:35+08:00">2025-06-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>23k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>43 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Paper阅读记录</p>
<span id="more"></span>
<h1>Paper-Reading-Record</h1>
<img src="/2025/04/02/Paper-Reading-Record/v2-ee9b5d4a0761d2d1d10acb37cebefba3_1440w.jpg" class="" title="img">
<h2 id="Corpus">Corpus</h2>
<p>corpus 语料库</p>
<p>“conditioning the model” “调节模型”</p>
<p>“word representations” “词向量表示”</p>
<p>“log-bilinear” “对数双线性”</p>
<p>“factored 3-way” “因子分解三向”</p>
<p>modality-biased 模态偏置</p>
<p>Generation and Retrieval 生成和检索</p>
<p>SOTA  “State - Of - The - Art”</p>
<p>lexicon 词汇表</p>
<br>
<p><strong>n-gram</strong>：指连续的 <em>n</em> 个词（或字符）组成的序列。例如：</p>
<ul>
<li><strong>Unigram (1-gram)</strong>: “the”, “cat”, “sat”</li>
<li><strong>Bigram (2-gram)</strong>: “the cat”, “cat sat”</li>
<li><strong>Trigram (3-gram)</strong>: “the cat sat”</li>
</ul>
<br>
<p><strong>BLEU（Bilingual Evaluation Understudy）</strong></p>
<ul>
<li><strong>用途</strong>：机器翻译质量评估指标，通过对比生成文本与人工参考译文的n-gram匹配度计算得分（0-100，越高越好）。</li>
<li><strong>任务</strong>：<code>EN-DE</code>（英译德）和<code>EN-FR</code>（英译法）。</li>
</ul>
<p><strong>FLOPs（Floating Point Operations）</strong></p>
<ul>
<li><strong>用途</strong>：训练成本量化指标，表示模型训练所需浮点运算总量（数值越低，计算效率越高）</li>
</ul>
<br>
<p><strong>Perplexity</strong></p>
<p>困惑度（Perplexity）是评估语言模型性能的核心指标，用于衡量模型对未知数据的预测能力。困惑度反映了模型对测试数据中每个词的预测“不确定程度”。值越低，说明模型预测越准确（即更“不困惑”）。</p>
<p>困惑度的数学表达式为：<br>
$$<br>
\log_2 C(w_{1:n} | x) = -\frac{1}{N} \sum \log_2 P(w_n = i | w_{1:n-1}, x)<br>
$$<br>
其中：</p>
<ul>
<li><strong>$w_{1:n}$</strong>：长度为 n$ 的文本序列（例如句子或词序列）。</li>
<li><strong>$x$</strong>：额外模态的输入（如图像、音频等，在多模态任务中可能作为条件输入）。</li>
<li><strong>$P(w_n = i | w_{1:n-1}, x)$</strong>：模型基于历史序列 $w_{1:n-1}$ 和额外信息 $x$，预测下一个词 $w_n$ 为 $i$ 的概率。</li>
<li><strong>$N$</strong>：序列的总长度（或词数）。</li>
</ul>
<br>
<h2 id="Datasets">Datasets</h2>
<p>image-text descriptions: IAPR TC-12, Attributes Discovery, and the SBU datasets.</p>
<br>
<h2 id="Paper">Paper</h2>
<h3 id="Distribution-Base">Distribution-Base</h3>
<h4 id="CAP">CAP</h4>
<p>CAP twelve years later: How the “rules” have changed</p>
<p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/6133253">https://ieeexplore.ieee.org/abstract/document/6133253</a></p>
<p><strong>一致性 (Consistency, C)：</strong></p>
<ul>
<li><strong>含义：</strong> 在分布式系统中，对数据进行读取操作时，所有客户端（或者说从任何节点读取）都能看到相同且最新的数据。就好比系统里只有一份数据，而且这份数据总是最新的。</li>
</ul>
<p><strong>高可用性 (High Availability, A)：</strong></p>
<ul>
<li><strong>含义：</strong> 系统在非故障情况下，应该能够处理所有请求并返回响应（无论是成功还是失败）。换句话说，系统的大部分或全部节点应该始终保持运行状态，并且能够响应来自客户端的请求。特别是对于写入操作，客户端应该总是能够成功地将数据写入系统。</li>
</ul>
<p><strong>网络分区容忍性 (Tolerance to Network Partitions, P)：</strong></p>
<ul>
<li><strong>含义：</strong> 当分布式系统中的节点之间发生网络通信故障时（即网络分区，导致部分节点无法与另一部分节点通信），系统仍然能够正常运行。</li>
</ul>
<p><strong>核心概念总结：</strong></p>
<p>当网络出现分区（P）时，不可能同时拥有完美的“一致性”（C）和完美的“可用性”（A）。你必须在这两者之间做出权衡：</p>
<ul>
<li><strong>CP 系统：</strong> 优先保证一致性，在分区时牺牲可用性。</li>
<li><strong>AP 系统：</strong> 优先保证可用性，在分区时牺牲一致性（意味着可能需要后期处理数据冲突）。</li>
</ul>
<h3 id="LLM-Base">LLM-Base</h3>
<h4 id="Attention">Attention</h4>
<p><strong>Attention is All You Need</strong></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>
<br>
<h4 id="GPT">GPT</h4>
<p><a target="_blank" rel="noopener" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</a></p>
<p>Improving Language Understanding by Generative Pre-Training</p>
<p>GPT是基于Transformer的<strong>Decoder</strong>部分，但移除了交叉注意力。</p>
<ul>
<li><strong>“生成式”</strong>：指的是预训练阶段的任务是语言模型任务，即预测下一个词，这本质上是一种生成文本的能力。</li>
<li><strong>“预训练”</strong>：指的是模型在大量无标签文本数据上，通过上述生成任务进行训练，从而学习到通用的语言表示和能力，这些能力可以在后续的下游任务中进行迁移。</li>
</ul>
<br>
<p><strong>Two Stage Training</strong></p>
<p><strong>Unsupervised pre-training</strong></p>
<ul>
<li>
<p>“Given an unsupervised corpus of tokens $\mathcal{U} = {u_1, \ldots, u_n}$, we use a standard language modeling objective to maximize the following likelihood:”</p>
<p>$L_1(\mathcal{U}) = \sum_i \log P(u_i|u_{i-k}, \ldots, u_{i-1}; \Theta)$</p>
</li>
<li>
<p>“where $k$ is the size of the context window, and the conditional probability $P$ is modeled using a neural network with parameters $\Theta$. These parameters are trained using stochastic gradient descent [51].”</p>
</li>
<li>
<p>“In our experiments, we use a multi-layer <strong>Transformer decoder</strong> [34] for the language model, which is a variant of the transformer [62]. This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens:”<br>
$h_0 = UW_e + W_p$<br>
$h_l = \text{transformer_block}(h_{l-1}) \forall i \in [1, n]$<br>
$P(u) = \text{softmax}(h_n W_e^T)$</p>
</li>
<li>
<p>“where $U = (u_{-k}, \ldots, u_{-1})$ is the context vector of tokens, $n$ is the number of layers, $W_e$ is the token embedding matrix, and $W_p$ is the position embedding matrix.”</p>
</li>
</ul>
<br>
<p><strong>Supervised fine-tuning</strong></p>
<ul>
<li>
<p>“After training the model with the objective in Eq. 1, we adapt the parameters to the supervised target task.”</p>
</li>
<li>
<p>“We assume a labeled dataset $\mathcal{C}$, where each instance consists of a sequence of input tokens, $x^1, \ldots, x^m$, along with a label $y$.”<br>
这里定义了用于微调的监督数据集 $\mathcal{C}$。数据集中的每个样本都包含一个输入序列（由 $m$ 个 token 组成，$x^1, \ldots, x^m$）以及一个对应的标签 $y$。</p>
</li>
<li>
<p>“The inputs are passed through our pre-trained model to obtain the final transformer block’s activation $h_l^m$, which is then fed into an added linear output layer with parameters $W_y$ to predict $y$:”<br>
这部分是微调的关键机制。</p>
<ol>
<li><strong>输入通过预训练模型：</strong> 将输入序列 $x^1, \ldots, x^m$ 传入预训练好的 Transformer 模型。</li>
<li><strong>获取最终激活：</strong> 从模型中获取<strong>最后一个 Transformer block 的激活 $h_l^m$</strong>。这里的 $h_l^m$ 通常指的是整个输入序列经过最后一层 Transformer block 处理后，<strong>最后一个 token ($x^m$) 对应的隐藏状态（激活值）</strong>。在分类任务中，通常使用序列末尾的隐藏状态作为整个序列的表示。</li>
<li><strong>连接线性输出层：</strong> 将 $h_l^m$ 输入到一个新添加的线性输出层。这个线性层的参数是 $W_y$。</li>
<li><strong>预测标签 $y$：</strong> 通过这个线性层预测标签 $y$。</li>
</ol>
</li>
<li>
<p>$P(y|x^1, \ldots, x^m) = \text{softmax}(h_l^m W_y).$<br>
这个公式描述了在给定输入序列 $x^1, \ldots, x^m$ 的情况下，预测标签 $y$ 的概率。</p>
<ul>
<li>$h_l^m W_y$ 是最后一个 token 的激活 $h_l^m$ 与线性层参数 $W_y$ 的矩阵乘法。这会得到一个与标签类别数量相同维度的向量。</li>
<li><code>softmax</code> 函数将这个向量转换为概率分布，使得所有类别的概率之和为 1。</li>
</ul>
</li>
<li>
<p>“This gives us the following objective to maximize:”<br>
$L_2(\mathcal{C}) = \sum_{(x,y)} \log P(y|x^1, \ldots, x^m).$ (4)<br>
这是监督微调阶段的主要目标函数。</p>
<ul>
<li>它是一个对数似然函数（或交叉熵损失的负值）。</li>
<li>目标是最大化在给定输入序列 $(x^1, \ldots, x^m)$ 的情况下，模型预测正确标签 $y$ 的对数概率之和。</li>
<li>求和遍历了监督数据集 $\mathcal{C}$ 中的所有样本 $(x, y)$。</li>
</ul>
</li>
<li>
<p>“We additionally found that including language modeling as an auxiliary objective to the fine-tuning helped learning by (a) improving generalization of the supervised model, and (b) accelerating convergence. This is in line with prior work [50, 43], who also observed improved performance with such an auxiliary objective.”<br>
这是一个重要的发现和优化策略。作者发现，在微调阶段除了使用监督任务的目标函数 $L_2(\mathcal{C})$，<strong>同时引入语言模型目标 $L_1(\mathcal{C})$ 作为辅助目标</strong>，能够：</p>
<ul>
<li>a) 提高监督模型的泛化能力。</li>
<li>b) 加速收敛。</li>
<li>这与之前的工作（[50, 43]）观察到的结果一致。</li>
</ul>
</li>
<li>
<p>“Specifically, we optimize the following objective (with weight $\lambda$):”<br>
$L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda * L_1(\mathcal{C})$<br>
这是最终用于微调的总目标函数。</p>
<ul>
<li>$L_3(\mathcal{C})$ 是 $L_2(\mathcal{C})$（监督任务目标）和 $L_1(\mathcal{C})$（语言模型目标）的加权和。</li>
<li>$\lambda$ 是一个超参数，用于控制语言模型辅助目标的重要性。</li>
<li>这里的 $L_1(\mathcal{C})$ 是在微调数据集 $\mathcal{C}$ 上计算的语言模型损失。这意味着在微调过程中，模型不仅要学习如何完成下游任务，还要继续保持其语言模型的生成能力。</li>
</ul>
</li>
<li>
<p>“Overall, the only extra parameters we require during fine-tuning are $W_y$, and embeddings for delimiter tokens (described below in Section 3.3).”<br>
这句话指出了在微调阶段，除了预训练模型的所有参数外，唯一需要额外添加和训练的参数是：</p>
<ul>
<li>$W_y$：用于监督任务的线性输出层的参数。</li>
<li><strong>分隔符 token 的嵌入</strong>：这在下一节 3.3 中会详细描述，这些特殊 token 用于将输入序列格式化以适应不同的下游任务（例如，分类任务可能需要在序列开头或结尾添加特殊 token）。</li>
</ul>
</li>
</ul>
<br>
<p><strong>Task-specific input transformations</strong></p>
<img src="/2025/04/02/Paper-Reading-Record/image-20250530104004180.png" class="" title="image-20250530104004180">
<ul>
<li>
<p>“For some tasks, like text classification, we can directly fine-tune our model as described above. Certain other tasks, like question answering or textual entailment, have structured inputs such as ordered sentence pairs, or triplets of document, question, and answers. Since our pre-trained model was trained on contiguous sequences of text, we require some modifications to apply it to these tasks.”</p>
<ul>
<li>对于简单的任务（如文本分类），可以直接按照 3.2 节描述的方式微调模型。</li>
<li>但对于更复杂的任务（如问答或文本蕴涵），输入通常是结构化的，例如有序的句子对或文档、问题和答案的三元组。</li>
<li>由于预训练模型是基于连续的文本序列进行训练的，为了适应这些结构化输入，需要进行一些修改。</li>
</ul>
</li>
<li>
<p>“Previous work proposed learning task specific architectures on top of transferred representations [44]. Such an approach re-introduces a significant amount of task-specific customization and does not use transfer learning for these additional architectural components.”</p>
<ul>
<li>以前的工作（如 [44]）通常会在迁移的表示之上构建任务特定的架构。</li>
<li>这种方法会引入大量的任务特定定制，并且这些额外的架构组件无法享受到迁移学习的优势。</li>
</ul>
</li>
<li>
<p>“Instead, we use a traversal-style approach [52], where we convert structured inputs into an ordered sequence that our pre-trained model can process. These input transformations allow us to avoid making extensive changes to the architecture across tasks. We provide a brief description of these input transformations below and Figure 1 provides a visual illustration. All transformations include adding randomly initialized start and end tokens ($\langle s \rangle$, $\langle e \rangle$).”</p>
<ul>
<li>GPT 采用了“遍历式”方法（参考 [52]），即将结构化输入转换成一个有序的序列，使得预训练模型可以直接处理。</li>
<li>这种输入转换避免了对模型架构进行大量的任务间修改，从而保持了架构的统一性。</li>
<li>所有转换都包括添加随机初始化的<strong>开始 token ($\langle s \rangle$)</strong> 和<strong>结束 token ($\langle e \rangle$)</strong>。这些特殊 token 的嵌入在微调阶段也会被训练。</li>
</ul>
</li>
</ul>
<p><strong>Textual entailment (文本蕴涵):</strong><br>
“For entailment tasks, we concatenate the premise $p$ and hypothesis $h$ token sequences, with a delimiter token ($ $$) in between.”</p>
<ul>
<li><strong>任务：</strong> 判断一个“假说（hypothesis）”是否可以从一个“前提（premise）”中推断出来。</li>
<li><strong>输入转换：</strong> 将前提 $p$ 的 token 序列和假说 $h$ 的 token 序列<strong>连接起来</strong>，并在它们之间插入一个<strong>分隔符 token ($$$)</strong>。</li>
<li><strong>格式：</strong> $\langle s \rangle \text{ premise } \text{ $ } \text{ hypothesis } \langle e \rangle$</li>
</ul>
<p><strong>Similarity (相似度):</strong><br>
“For similarity tasks, there is no inherent ordering of the two sentences being compared. To reflect this, we modify the input sequence to contain both possible sentence orderings (with a delimiter in between) and process each independently to produce two sequence representations $h_l^m$ which are added element-wise before being fed into the linear output layer.”</p>
<ul>
<li><strong>任务：</strong> 判断两个句子之间的相似度。</li>
<li><strong>挑战：</strong> 两个句子没有固有的顺序。</li>
<li><strong>输入转换：</strong> 为了解决无序性，模型会生成<strong>两种可能的句子顺序</strong>的输入序列：
<ol>
<li><strong>顺序 1：</strong> 句子 A $\text{ $ }$ 句子 B</li>
<li><strong>顺序 2：</strong> 句子 B $\text{ $ }$ 句子 A</li>
</ol>
</li>
<li><strong>处理方式：</strong>
<ul>
<li>分别将这两个序列独立地传入预训练模型。</li>
<li>得到两个序列的最终表示 $h_l^m$（即每个序列的最后一个 token 的隐藏状态）。</li>
<li>将这两个 $h_l^m$ <strong>进行逐元素相加（element-wise add）</strong>。</li>
<li>将相加后的结果输入到线性输出层进行预测。</li>
</ul>
</li>
<li><strong>格式：</strong>
<ul>
<li>Sequence 1: $\langle s \rangle \text{ sentence}_1 \text{ $ } \text{ sentence}_2 \langle e \rangle$</li>
<li>Sequence 2: $\langle s \rangle \text{ sentence}_2 \text{ $ } \text{ sentence}_1 \langle e \rangle$</li>
<li>然后将两个序列的输出表示相加。</li>
</ul>
</li>
</ul>
<p><strong>Question Answering and Commonsense Reasoning (问答和常识推理):</strong><br>
“For these tasks, we are given a context document $z$, a question $q$, and a set of possible answers ${a_k}$. We concatenate the document context and question with each possible answer, adding a delimiter token in between to get $[z; q; $; a_k]$. Each of these sequences are processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers.”</p>
<ul>
<li><strong>任务：</strong> 给定一个文档 $z$、一个问题 $q$ 和一组可能的答案 ${a_k}$，选择正确答案。</li>
<li><strong>输入转换：</strong> 对于每个可能的答案 $a_k$，构造一个独立的输入序列。
<ul>
<li>将<strong>文档上下文 $z$</strong>、<strong>问题 $q$</strong> 和<strong>当前可能的答案 $a_k$</strong> 连接起来。</li>
<li>在文档上下文和问题之间，以及问题和答案之间，都插入一个<strong>分隔符 token ($$$)</strong>。</li>
</ul>
</li>
<li><strong>格式：</strong> $\langle s \rangle \text{ document } \text{ $ } \text{ question } \text{ $ } \text{ answer}_k \langle e \rangle$</li>
<li><strong>处理方式：</strong>
<ul>
<li>对于每个构造的序列（即每个可能的答案），都独立地通过模型进行处理。</li>
<li>得到每个序列的最终表示后，通过 softmax 层归一化，得到每个答案的概率分布。</li>
<li>模型最终选择概率最高的答案。</li>
</ul>
</li>
</ul>
<br>
<br>
<h4 id="BERT">BERT</h4>
<p><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>
<p>BERT arch</p>
<p>denote input embedding as $E$, the final hidden vector of the special [CLS] token as $C \in \mathbb{R}^H$, and the final hidden vector for the $i^{\text{th}}$ input token as $T_i \in \mathbb{R}^H$.</p>
<img src="/2025/04/02/Paper-Reading-Record/image-20250530112142071.png" class="" title="image-20250530112142071">
<p>BERT token representation</p>
<img src="/2025/04/02/Paper-Reading-Record/image-20250530110615396.png" class="" title="image-20250530110615396">
<br>
<p><strong>pre-training</strong></p>
<p>$BERT_{BASE}$ was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left</p>
<p>BERT是基于Transformer的<strong>Encoder</strong>部分</p>
<p><strong>MLM</strong></p>
<p>“遮蔽语言模型”（MLM）方法</p>
<p>MLM的核心思想是：</p>
<ol>
<li><strong>随机遮蔽（Mask）输入序列中的一部分词语。</strong> 就像在句子中挖空一样。</li>
<li><strong>然后，模型的目标是预测这些被遮蔽的词语是什么。</strong></li>
</ol>
<p>这与传统的“完形填空”（Cloze task）非常相似，即给出一个句子，其中有一些缺失的词，要求填入正确的词。</p>
<p>当模型处理完被遮蔽的输入序列后，对于每个 [MASK] 标记，BERT会输出一个对应的<strong>最终隐藏向量（final hidden vector）</strong>。这个隐藏向量包含了模型对该位置上下文信息的理解。</p>
<p>然后，这个隐藏向量会被送入一个<strong>输出Softmax层</strong>。这个Softmax层的输出维度等于词汇表的大小（vocabulary size）。Softmax会为词汇表中的每个词分配一个概率，表示该词是 [MASK] 位置的正确词的概率。模型会选择概率最高的词作为预测结果。这个过程类似于标准语言模型的预测方式，只是这里只针对被遮蔽的词。</p>
<ul>
<li><strong>遮蔽比例：</strong> 在BERT的所有实验中，每个输入序列中**15%**的WordPiece tokens（BERT使用的子词分词单元）会被随机选中并遮蔽。</li>
<li><strong>与去噪自编码器的区别：</strong> 传统的去噪自编码器（Denoising Auto-encoders）在输入中加入噪声（比如遮蔽），然后模型的目标是<strong>重构整个原始输入</strong>。而BERT的MLM不同，它<strong>只预测被遮蔽的词</strong>，而不是重构整个句子。这使得BERT能够更专注于理解上下文，并预测缺失的信息。</li>
</ul>
<p>MLM确实让BERT获得了双向的上下文理解能力，这是一个巨大的优势。然而，它也带来了一个问题：在预训练阶段，模型会看到特殊的 [MASK] 标记。但在<strong>微调（fine-tuning）阶段</strong>，实际的下游任务（如情感分析、问答、命名实体识别等）的输入数据中<strong>不会出现 [MASK] 标记</strong>。这导致了预训练和微调之间的<strong>不匹配（mismatch）</strong>：模型在预训练时学习了如何处理 [MASK]，但在实际应用中却从未遇到它。这可能会影响模型的性能。</p>
<p>为了缓解上述预训练-微调不匹配问题，BERT引入了一个巧妙的策略：</p>
<ol>
<li><strong>首先，确定要被“遮蔽”的词：</strong> 仍然随机选择**15%**的token位置作为要被预测的“目标”词。假设第i个token被选中。</li>
<li><strong>对这15%的选中词，采取三种不同的处理方式：</strong>
<ul>
<li><strong>80% 的情况：用 [MASK] 替换。</strong> 这是最常见的处理方式，和通常的MLM一样。模型学习了 [MASK] 的上下文表示。</li>
<li><strong>10% 的情况：用一个随机词替换。</strong> 这是一个非常关键的技巧。
<ul>
<li><strong>目的：</strong> 强制模型不仅要依赖 [MASK] 标记来预测，还要学会处理“错误”的输入词。这意味着模型需要利用其<strong>双向上下文</strong>来判断当前词是否合理，并预测出它原来的正确词。这提高了模型的鲁棒性。</li>
</ul>
</li>
<li><strong>10% 的情况：保持不变（不替换）。</strong>
<ul>
<li><strong>目的：</strong> 这迫使模型不仅仅是“填空”，它还需要识别出哪些词是它需要预测的（即使它们没有被替换），并且在没有 [MASK] 标记的情况下也能进行预测。这进一步减少了预训练和微调之间的差异，因为在微调时，模型总是看到“正常的”词，而没有 [MASK]。</li>
</ul>
</li>
</ul>
</li>
<li><strong>预测目标：</strong> 不论上述哪种情况，模型的目标<strong>始终是预测原始的、未被修改的token</strong>。损失函数是<strong>交叉熵损失（cross entropy loss）</strong>，用于衡量模型预测的概率分布与真实词的独热编码（one-hot encoding）之间的差异。</li>
</ol>
<blockquote>
<p><strong>例子：</strong><br>
原始句子：The quick brown fox jumps over the lazy dog.<br>
假设 brown 被选中作为15%要预测的词之一。</p>
<ul>
<li><strong>情况1 (80%): [MASK] 替换</strong>
<ul>
<li>输入：The quick [MASK] fox jumps over the lazy dog.</li>
<li>模型预测：brown</li>
</ul>
</li>
<li><strong>情况2 (10%): 随机词替换</strong>
<ul>
<li>假设随机选择了词 apple。</li>
<li>输入：The quick apple fox jumps over the lazy dog.</li>
<li>模型预测：brown (虽然输入是 apple，但模型需要根据上下文 The quick ___ fox 判断出 apple 是不合理的，并预测原词 brown)</li>
</ul>
</li>
<li><strong>情况3 (10%): 保持不变</strong>
<ul>
<li>输入：The quick brown fox jumps over the lazy dog.</li>
<li>模型预测：brown (即使 brown 没有被遮蔽，模型也需要学会预测它。这有点像一个“自我纠正”机制，或者说，在没有 [MASK] 提示的情况下，模型依然能够识别出某些位置是它需要特别关注和预测的，因为它被标记为“目标预测词”之一。)</li>
</ul>
</li>
</ul>
<p>以 <code>quick</code> 这个词为例：</p>
<ul>
<li><strong>真实标签 (Target):</strong> <code>quick</code>。在one-hot编码中，只有 <code>quick</code> 对应的维度是1，其他都是0。</li>
<li><strong>BERT模型预测概率 (Prediction):</strong> BERT模型对于 <code>[MASK]</code> 位置的输出，经过softmax后，会得到一个关于整个词汇表的概率分布，例如：
<ul>
<li>$P(\text{quick}) = 0.9$</li>
<li>$P(\text{slow}) = 0.05$</li>
<li>$P(\text{brown}) = 0.01$</li>
<li>…</li>
</ul>
</li>
</ul>
<p>交叉熵损失计算将是：<br>
$Loss_{quick} = - (1 \cdot \log(P(\text{quick})) + 0 \cdot \log(P(\text{slow})) + …)$</p>
<p><strong>同理，对 <code>fox</code> 和 <code>lazy</code> 进行计算：</strong></p>
<ul>
<li>
<p><strong>对于 <code>fox</code> (被替换为 <code>apple</code> 的位置):</strong></p>
<ul>
<li>真实标签: <code>fox</code></li>
<li>BERT模型预测概率: $P(\text{fox})$</li>
<li>$Loss_{fox} = - \log(P(\text{fox}))$</li>
</ul>
</li>
<li>
<p><strong>对于 <code>lazy</code> (保持不变的位置):</strong></p>
<ul>
<li>真实标签: <code>lazy</code></li>
<li>BERT模型预测概率: $P(\text{lazy})$</li>
<li>$Loss_{lazy} = - \log(P(\text{lazy}))$</li>
</ul>
</li>
</ul>
<p><strong>总损失：</strong></p>
<p>最终的MLM任务的总损失是所有被预测词的交叉熵损失的<strong>平均值</strong>（或求和，然后除以被预测词的数量）。</p>
<p>$Total Loss = (Loss_{quick} + Loss_{fox} + Loss_{lazy}) / 3$ (假设只有这三个词被预测)</p>
</blockquote>
<br>
<p><strong>NSP</strong></p>
<p>BERT的第一个预训练任务（MLM）主要关注的是<strong>词汇层面的理解和上下文表示</strong>。然而，许多重要的自然语言处理（NLP）下游任务，比如：</p>
<ul>
<li><strong>问答（Question Answering, QA）</strong>：模型需要判断一个句子（问题）和另一个句子（段落中的答案句）之间的关系。</li>
<li><strong>自然语言推理（Natural Language Inference, NLI）</strong>：也称为蕴含识别，模型需要判断一个前提（premise）和一个假设（hypothesis）之间的关系（蕴含、矛盾、中立）。</li>
</ul>
<p>这些任务都要求模型能够理解<strong>两个句子之间的关系</strong>，而不仅仅是单个句子内部的词语关系。传统的语言模型（无论是从左到右还是MLM）并没有直接捕捉这种跨句子的关系。</p>
<p>NSP任务的训练数据生成方式如下：<br>
对于每个训练样本，模型会输入两个句子，我们称之为句子A和句子B。</p>
<ol>
<li><strong>50% 的情况：IsNext (是下一句)</strong>
<ul>
<li>句子B确实是语料库中紧跟在句子A后面的那句话。</li>
<li>这个样本会被标记为 IsNext。</li>
</ul>
</li>
<li><strong>50% 的情况：NotNext (不是下一句)</strong>
<ul>
<li>句子B是语料库中随机选择的另一句话，与句子A没有直接的连续关系。</li>
<li>这个样本会被标记为 NotNext。</li>
</ul>
</li>
</ol>
<p>结构图中的“C”指的是BERT模型在处理多句输入时，用于表示整个输入序列的特殊标记 [CLS] 的最终隐藏向量。在BERT的输入格式中，两个句子A和B会被连接起来，中间用 [SEP] 分隔，开头有 [CLS] 标记。</p>
<p>输入格式大致是：[CLS] Sentence A [SEP] Sentence B [SEP]</p>
<ul>
<li>[CLS] 标记的最终隐藏向量（通常称为 C 或 T_CLS）被认为是包含了整个输入序列（包括两个句子及其关系）的综合信息。</li>
<li>这个 [CLS] 标记对应的隐藏向量会被送入一个简单的分类层（例如一个线性层接Softmax），进行二分类预测：IsNext 或 NotNext。</li>
</ul>
<blockquote>
<p><strong>例子：</strong><br>
对于输入 [CLS] The cat sat on the mat. [SEP] It was a fluffy cat. [SEP]</p>
<ol>
<li>BERT处理整个输入序列。</li>
<li>提取 [CLS] 标记对应的输出向量。</li>
<li>将该向量送入一个分类器。</li>
<li>分类器输出概率：P(IsNext)=0.98, P(NotNext)=0.02。模型预测 IsNext。</li>
</ol>
</blockquote>
<br>
<p><strong>Fine-tuning</strong></p>
<p>BERT直接把两段文本（例如，问题和文章）<strong>拼接</strong>在一起（用特殊分隔符[SEP]隔开），然后一起输入给BERT。BERT的<strong>自注意力机制</strong>神奇之处在于，当它处理这个拼接后的长文本时，它会自动地在问题和文章的词语之间建立起“双向交叉注意力”。也就是说，一个词在问题中，它也会关注文章中的所有词；一个词在文章中，它也会关注问题中的所有词。这一个步骤就完成了传统的“独立编码”和“交叉注意力”两个阶段的工作，简化了模型结构。</p>
<p>对于每个不同的任务，我们只需要把任务对应的输入格式（如单句、句子对）和任务对应的输出要求（如分类结果、词级别标签）接入到预训练好的BERT模型上，然后从头到尾（end-to-end）地微调整个模型的所有参数。</p>
<p>BERT在微调时，根据任务类型利用其输出的不同部分。</p>
<ul>
<li><strong>Token-level tasks (词级别任务)：</strong>
<ul>
<li><strong>用途：</strong> 对于需要对输入文本中的每个词进行判断或标记的任务。</li>
<li><strong>取用输出：</strong> 使用BERT输出的<strong>每个词（token）对应的向量表示</strong>。</li>
<li><strong>举例：</strong>
<ul>
<li><strong>序列标注 (Named Entity Recognition, NER)：</strong> 输入 “张三在纽约工作”。BERT会为&quot;张三&quot;、“在”、“纽约”、&quot;工作&quot;等每个词生成一个向量。然后，将这些向量输入一个线性层，分别预测&quot;张三&quot;是人名（PER）、&quot;纽约&quot;是地名（LOC）。</li>
<li><strong>问答 (Question Answering, QA)：</strong> BERT为文章中的每个词生成向量。这些向量被用于预测答案的起始词和结束词。</li>
</ul>
</li>
</ul>
</li>
<li><strong>Classification tasks (分类任务)：</strong>
<ul>
<li><strong>用途：</strong> 对于需要对整个输入文本（或文本对）进行分类的任务。</li>
<li><strong>取用输出：</strong> 使用BERT输出的**[CLS] token对应的向量表示**。[CLS] token是一个特殊的标记，它的最终向量表示被设计成能够汇聚整个输入序列的语义信息。</li>
<li><strong>举例：</strong>
<ul>
<li><strong>语义蕴含 (Entailment)：</strong> 输入[CLS] 狗在跑 [SEP] 动物在运动 [SEP]。取[CLS]的向量，输入一个分类层，判断属于“蕴含”、“矛盾”还是“中性”。</li>
<li><strong>情感分析 (Sentiment Analysis)：</strong> 输入[CLS] 这部电影太棒了！ [SEP]。取[CLS]的向量，输入一个分类层，判断属于“正面”、“负面”或“中性”。</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>原始BERT论文中提到的问答任务，特指<strong>问答抽取（Extractive QA）</strong>，而不是问答生成。</p>
</blockquote>
<br>
<p><strong>Training Objectives</strong></p>
<ul>
<li><strong>预训练阶段：</strong> BERT在预训练时学习了两种任务：MLM（Masked Language Model）和NSP（Next Sentence Prediction）。NSP任务的输入就是[CLS] Sentence A [SEP] Sentence B [SEP]，它需要判断Sentence B是否是Sentence A的下一句。这里的Sentence A和Sentence B是任意从语料中抽取的。</li>
<li><strong>到下游任务的映射：</strong>
<ol>
<li><strong>Paraphrasing (复述判断)：</strong> 判断两句话是否表达相同的意思。
<ul>
<li>对应：[CLS] 句子A [SEP] 句子B [SEP]</li>
<li>例如：[CLS] 他买了一辆新车 [SEP] 他购入了一台新轿车 [SEP]</li>
</ul>
</li>
<li><strong>Entailment (语义蕴含)：</strong> 判断一个“假设句”是否能从一个“前提句”中推导出来。
<ul>
<li>对应：[CLS] 假设句 [SEP] 前提句 [SEP]</li>
<li>例如：[CLS] 狗在跑 [SEP] 动物在运动 [SEP] (假设句：狗在跑，前提句：动物在运动。狗在跑蕴含动物在运动)</li>
</ul>
</li>
<li><strong>Question Answering (问答)：</strong>
<ul>
<li>对应：[CLS] 问题 [SEP] 段落 [SEP]</li>
<li>例如：[CLS] 谁是美国第一位总统？ [SEP] 乔治·华盛顿是美国的第一位总统… [SEP]</li>
</ul>
</li>
<li><strong>Degenerate text-∅ pair in text classification or sequence tagging (文本分类或序列标注中的“退化”文本-空对)：</strong> 这句话的意思是，对于只涉及单句的任务，可以把“空”视为第二段文本。
<ul>
<li>对应：[CLS] 句子A [SEP] [SEP] (注意，这里Sentence B实际上是空的，只用了一个[SEP]标记)</li>
<li>例如（情感分析）：[CLS] 这部电影太棒了！ [SEP] (严格来说是[CLS] 这部电影太棒了！ [SEP] [SEP])</li>
<li>例如（命名实体识别）：[CLS] 张三在纽约工作 [SEP]</li>
</ul>
</li>
</ol>
</li>
</ul>
<br>
<p><strong>Inference</strong></p>
<p>BERT的输出一般为<strong>分类任务 (情感分析、语义蕴含)</strong>，**序列标注任务 (命名实体识别)**等</p>
<p>假设我们有一个输入序列：<code>&quot;The dog [MASK] at the cat.&quot;</code><br>
BERT的目标是预测 <code>[MASK]</code> 位置应该填入什么词。</p>
<p><strong>步骤分解：</strong></p>
<ol>
<li>
<p><strong>输入预处理与嵌入：</strong></p>
<ul>
<li>输入序列被标记化为：<code>[&quot;The&quot;, &quot;dog&quot;, &quot;[MASK]&quot;, &quot;at&quot;, &quot;the&quot;, &quot;cat&quot;, &quot;[SEP]&quot;]</code> (通常还会加上 <code>[CLS]</code> 在开头，这里为了简化关注点暂不提)。</li>
<li>每个token（包括 <code>[MASK]</code> token本身）被转换为其对应的词嵌入和位置编码，并相加。</li>
<li>得到一个输入张量，维度为 <code>(1, L, D)</code> (batch_size=1, L=7, D=隐藏层维度，例如768)。</li>
</ul>
</li>
<li>
<p><strong>通过Transformer Encoder层：</strong></p>
<ul>
<li>这个 <code>(1, L, D)</code> 的张量会通过BERT的多个Transformer Encoder层（例如BERT-base有12层，BERT-large有24层）。</li>
<li><strong>关键点在于：</strong> BERT的Encoder层中的自注意力是<strong>双向的</strong>。这意味着当模型处理 <code>[MASK]</code> token时，它<strong>可以同时关注到 <code>[MASK]</code> 前后的所有词</strong>（“The”, “dog”, “at”, “the”, “cat”, “[SEP]”）。</li>
<li>每一层都会对token的表示进行细化和更新，融入更深层次的上下文信息。</li>
</ul>
</li>
<li>
<p><strong>获取 <code>[MASK]</code> token 的最终隐藏状态：</strong></p>
<ul>
<li>经过所有Encoder层后，我们得到一个最终的隐藏状态张量，维度仍然是 <code>(1, L, D)</code>。</li>
<li>我们现在需要从这个张量中，<strong>提取对应 <code>[MASK]</code> token 位置的隐藏状态</strong>。</li>
<li>假设 <code>[MASK]</code> token 在序列中的索引是2。那么我们取出 <code>output_tensor[0, 2, :]</code>，这将得到一个维度为 <code>(D,)</code> 的向量（例如 <code>(768,)</code>）。</li>
<li>这个向量是 <code>[MASK]</code> token 在**充分利用了整个序列（包括其上下文）**信息后所形成的上下文感知表示。</li>
</ul>
</li>
<li>
<p><strong>语言模型头（MLM Head / Decoder Head）：</strong></p>
<ul>
<li>这个 <code>(D,)</code> 向量被送入一个专门用于预测被遮盖词的“语言模型头”。这个头通常包含：
<ul>
<li><strong>一个线性层：</strong> 将 <code>(D,)</code> 向量投影到一个中间维度，例如 <code>(D,) -&gt; (D,)</code>。这层通常是一个非线性激活函数（如 GELU）和一个 Layer Normalization。</li>
<li><strong>另一个线性层：</strong> 将中间维度投影到词汇表大小的维度：<code>(D,) -&gt; (V,)</code>。这个输出就是<strong>逻辑值（logits）</strong>。</li>
</ul>
</li>
<li>这个线性层的权重矩阵维度是 <code>(D, V)</code>。</li>
</ul>
</li>
<li>
<p><strong>Softmax 与预测：</strong></p>
<ul>
<li>对 <code>(V,)</code> 维度的 logits 应用 Softmax 激活函数。这将得到一个概率分布，表示词汇表中每个词是 <code>[MASK]</code> token 的概率。</li>
<li><code>P(word | context) = softmax(logits)</code></li>
<li>模型会选择概率最高的词作为预测结果。</li>
</ul>
</li>
</ol>
<p><strong>举例说明：</strong></p>
<p>输入：<code>&quot;The dog [MASK] at the cat.&quot;</code></p>
<ol>
<li>
<p><strong>输入和嵌入：</strong><br>
<code>[CLS] The dog [MASK] at the cat [SEP]</code><br>
假设 <code>[MASK]</code> 的索引是 3。<br>
<code>E_CLS</code>, <code>E_The</code>, <code>E_dog</code>, <code>E_MASK</code>, <code>E_at</code>, <code>E_the</code>, <code>E_cat</code>, <code>E_SEP</code> (初始嵌入)</p>
</li>
<li>
<p><strong>通过12层BERT Encoder：</strong><br>
每个 token 的表示都会通过多层 Transformer Layer 进行更新。<br>
例如，在第一层，<code>E_MASK</code> 就可以通过双向注意力看到 <code>E_The</code>, <code>E_dog</code>, <code>E_at</code>, <code>E_the</code>, <code>E_cat</code> 等所有其他词的信息。<br>
经过所有层后，我们得到最终的隐藏状态 <code>H</code>：<br>
<code>H = [h_CLS, h_The, h_dog, h_MASK, h_at, h_the, h_cat, h_SEP]</code><br>
其中 <code>h_MASK</code> 此时是一个非常丰富的向量，它已经吸收了 <code>The dog</code> 和 <code>at the cat</code> 的所有上下文信息。</p>
</li>
<li>
<p><strong>提取 <code>h_MASK</code>：</strong><br>
我们取出 <code>h_MASK</code>，其维度是 <code>(D,)</code> (例如 768)。</p>
</li>
<li>
<p><strong>语言模型头：</strong><br>
<code>h_MASK</code> 经过一个 <code>(D, D)</code> 的线性层和一个 <code>(D, V)</code> 的线性层。<br>
<code>logits = h_MASK @ W_linear</code> (这里的 <code>W_linear</code> 是两个线性层的组合权重，或者分步计算)<br>
<code>logits</code> 的维度是 <code>(V,)</code> (例如 50257)。</p>
</li>
<li>
<p><strong>Softmax 预测：</strong><br>
<code>softmax(logits)</code> 得到一个概率分布。<br>
在这个概率分布中，<code>barked</code>、<code>looked</code>、<code>ran</code> 等词的概率会很高，而 <code>car</code>、<code>apple</code> 等词的概率会很低。<br>
模型最终选择概率最高的词，例如 <code>barked</code>。</p>
</li>
</ol>
<p><strong>为什么与GPT不同？</strong></p>
<ul>
<li><strong>BERT的 <code>[MASK]</code> 位置：</strong> BERT能够直接在 <code>[MASK]</code> 位置进行预测，因为它的注意力是双向的。当它计算 <code>[MASK]</code> 的表示时，它能看到整个句子的上下文，包括 <code>[MASK]</code> 之后的词。这意味着它能从两边推断出 <code>[MASK]</code> 应该是什么。</li>
<li><strong>GPT的下一个词预测：</strong> GPT的注意力是单向的。如果它要预测 <code>[MASK]</code>，它必须假装 <code>[MASK]</code> 是序列的最后一个词，并且不能看到 <code>[MASK]</code> 之后的任何词。这使得它在训练时无法像BERT那样充分利用左右上下文来理解一个词的含义。GPT擅长的是给定前缀，自回归地续写。</li>
</ul>
<p>因此，BERT的MLM任务和双向注意力使其能够有效地预测被遮盖的词，这是它在理解句子内部关系方面表现出色的关键。</p>
<br>
<h3 id="文本">文本</h3>
<h4 id="GPT2">GPT2</h4>
<p><a target="_blank" rel="noopener" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a></p>
<blockquote>
<p>卖点：更大的数据集，更大的模型，zero shot(一次训练，四处使用</p>
</blockquote>
<p>对比GPT pretrain + fine-tuning</p>
<ul>
<li>
<p>需要针对下游任务进行微调</p>
</li>
<li>
<p>微调训练集需要人工引入新符号</p>
</li>
</ul>
<br>
<h4 id="GPT3">GPT3</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></p>
<blockquote>
<p>卖点：无需微调数据集 + 克服微调泛化性问题 + 可迁移</p>
</blockquote>
<p>GPT3在推理时不更新梯度，通过prompt实现one-shot 或 few-shot</p>
<br>
<h4 id="InstructGPT">InstructGPT</h4>
<p>Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback&quot; (InstructGPT, 2022)</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.05862">https://arxiv.org/abs/2204.05862</a></p>
<br>
<h4 id="LLaMa-1">LLaMa-1</h4>
<p>LLaMA: Open and Efficient Foundation Language Models</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.13971">https://arxiv.org/abs/2302.13971</a></p>
<br>
<h4 id="LLaMA-2">LLaMA-2</h4>
<p>Llama 2: Open Foundation and Fine-Tuned Chat Models</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a></p>
<br>
<h4 id="DeepSeek-V1">DeepSeek-V1</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.02954">https://arxiv.org/abs/2401.02954</a></p>
<br>
<h4 id="DeepSeek-V2">DeepSeek-V2</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.04434">https://arxiv.org/abs/2405.04434</a></p>
<br>
<h4 id="DeepSeek-V3">DeepSeek-V3</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.19437">https://arxiv.org/abs/2412.19437</a></p>
<br>
<h4 id="DeepSeek-R1-Zero-DeepSeek-R1">DeepSeek-R1-Zero&amp;DeepSeek-R1</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2501.12948">https://arxiv.org/abs/2501.12948</a></p>
<br>
<h4 id="Qwen3">Qwen3</h4>
<p><a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf">https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf</a></p>
<p>使用技术：GQA SwiGLU RoPE RMSNorm QK-Norm BBPE embedding</p>
<p><strong>概述</strong></p>
<p>训练数据通过多模型获取</p>
<ul>
<li>Qwen2.5-VL 提取PDF文件内容</li>
<li>Qwen2.5-Math 生成数学内容</li>
<li>Qwen2.5-Coder 生成代码内容</li>
</ul>
<p>三阶段 pre-training</p>
<ul>
<li>通用知识 train on 30 trillion tokens</li>
<li>STEM&amp;coding 知识</li>
<li>长上下文数据</li>
</ul>
<p>多阶段 post-training: fisrt two stages</p>
<ul>
<li>CoT cold-start finetuning</li>
<li>RL on math&amp;coding</li>
</ul>
<p>多阶段 post-training: final two stages</p>
<ul>
<li>混合 reasoning path data finetuning</li>
<li>general-domain RL</li>
</ul>
<br>
<p><strong>架构</strong></p>
<p><strong>embedding</strong></p>
<p><strong>layer</strong></p>
<p>begin</p>
<ul>
<li>
<p>输入 -&gt; RMSNorm</p>
</li>
<li>
<p>transformer</p>
<ul>
<li>GQA 头 &amp; 移除QKV-bias</li>
<li>对 Q，K 进行RMSNorm</li>
<li>RoPE 添加于norm后 Q，K</li>
<li>注意力计算</li>
</ul>
</li>
<li>
<p>残差</p>
</li>
</ul>
<p>end</p>
<p>begin</p>
<ul>
<li>输入 -&gt; RMSNorm</li>
<li>MLP</li>
<li>残差</li>
</ul>
<p>end</p>
<br>
<p><strong>MoE</strong></p>
<ul>
<li>sparse MoE，没有shared MoE</li>
<li>load balancing loss 专家被选的越多惩罚项越大</li>
</ul>
<br>
<p><strong>pretrain</strong></p>
<br>
<h3 id="CV">CV</h3>
<h4 id="Stable-Diffusion">Stable Diffusion</h4>
<p>2112.10752 High-Resolution Image Synthesis with Latent Diffusion Models (<a target="_blank" rel="noopener" href="http://arxiv.org">arxiv.org</a>)</p>
<br>
<h4 id="ViT">ViT</h4>
<p><strong>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale(ViT)</strong></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></p>
<img src="/2025/04/02/Paper-Reading-Record/image-20250530145000198.png" class="" title="image-20250530145000198">
<p><strong>核心思想：</strong> ViT将图像视为一系列的图像块（patches），然后将这些图像块的线性嵌入加上位置编码，送入标准的Transformer Encoder。</p>
<br>
<p><strong>ViT 在预训练阶段（或作为特征提取器）的输出</strong></p>
<p>在图像分类任务中，ViT通常会模仿BERT，引入一个特殊的**<code>[CLS]</code> token**（分类标记）。</p>
<ul>
<li>
<p><strong>输入处理：</strong></p>
<ol>
<li>图像被分割成固定大小的图像块（e.g., 16x16 像素）。</li>
<li>每个图像块被线性投影（embedding）成一个向量。</li>
<li>一个可学习的 <code>[CLS]</code> token 嵌入（与图像块嵌入维度相同）被添加到序列的<strong>开头</strong>。</li>
<li>位置编码被添加到这些嵌入中，以保留空间信息。</li>
<li>所有这些嵌入（<code>[CLS]</code> token嵌入 + 图像块嵌入 + 位置编码）形成一个序列，作为Transformer Encoder的输入。</li>
</ol>
</li>
<li>
<p><strong>Transformer Encoder 处理：</strong></p>
<ul>
<li>这个序列（例如 <code>[CLS]</code> + 256个图像块，如果图像是 224x224 且块大小是 16x16）会通过多个标准的Transformer Encoder层。</li>
<li><strong>关键点：</strong> Transformer Encoder中的自注意力是<strong>双向的</strong>。这意味着 <code>[CLS]</code> token可以关注到所有图像块的信息，反之亦然，所有图像块也可以关注到彼此以及 <code>[CLS]</code> token。</li>
<li>经过所有Encoder层后，每个token（包括 <code>[CLS]</code> token和所有图像块）都会得到一个上下文感知的最终隐藏向量。</li>
</ul>
</li>
<li>
<p><strong>ViT 最终输出的张量维度 (作为特征提取器)：</strong></p>
<ol>
<li>
<p><strong><code>[CLS]</code> token 的最终隐藏向量：</strong></p>
<ul>
<li>这个是<strong>最常被用来表示整个图像信息</strong>的输出。</li>
<li>维度：<code>(batch_size, d_model)</code></li>
<li>例如：如果 <code>batch_size = 3</code>，<code>d_model = 768</code> (ViT-Base)，那么输出是 <code>(3, 768)</code>。</li>
<li>这个向量包含了整个图像的全局特征，因为它通过双向注意力整合了所有图像块的信息。</li>
</ul>
</li>
<li>
<p><strong>所有 token 的最终隐藏向量序列：</strong></p>
<ul>
<li>如果你需要每个图像块的细粒度特征，或者用于其他下游任务（如目标检测、语义分割，但通常需要额外的解码器），你可以使用所有token的输出。</li>
<li>维度：<code>(batch_size, num_tokens, d_model)</code></li>
<li><code>num_tokens</code> = <code>1</code> (for <code>[CLS]</code>) + <code>num_patches</code>。</li>
<li>例如：如果 <code>batch_size = 3</code>，<code>num_patches = 256</code>，<code>d_model = 768</code>，那么输出是 <code>(3, 257, 768)</code>。</li>
</ul>
</li>
</ol>
</li>
</ul>
<br>
<p><strong>ViT 用于图像分类任务时的最终输出</strong></p>
<p>当ViT被用于图像分类任务时，它会在上述 <code>[CLS]</code> token 的输出之上，添加一个分类头。</p>
<ul>
<li>
<p><strong>分类头：</strong></p>
<ul>
<li>通常是一个简单的<strong>线性层（Linear Layer）</strong>，它接收 <code>[CLS]</code> token 的最终隐藏向量 <code>(batch_size, d_model)</code> 作为输入。</li>
<li>线性层将这个向量投影到**类别数量（num_classes）**的维度。</li>
<li>线性层的权重矩阵维度是 <code>(d_model, num_classes)</code>。</li>
<li>进行矩阵乘法：<code>(batch_size, d_model) @ (d_model, num_classes) = (batch_size, num_classes)</code>。</li>
</ul>
</li>
<li>
<p><strong>最终输出维度 (用于分类任务的 logits)：</strong></p>
<ul>
<li><code>(batch_size, num_classes)</code></li>
<li>这些是每个图像在所有类别上的<strong>逻辑值（logits）</strong>。</li>
<li>在推理时，通常会再对这些 logits 应用 Softmax 激活函数，以得到每个类别的概率分布。</li>
<li><code>P(class | image) = softmax(logits)</code></li>
<li>然后选择概率最高的类别作为最终的分类结果。</li>
</ul>
</li>
</ul>
<p><strong>举例说明 (ViT-Base 用于 ImageNet 分类)：</strong></p>
<ul>
<li><code>batch_size = 4</code></li>
<li><code>d_model = 768</code></li>
<li><code>num_classes = 1000</code> (ImageNet 的类别数)</li>
</ul>
<ol>
<li>
<p><strong>ViT Encoder 的输出 (特征提取):</strong></p>
<ul>
<li><code>[CLS]</code> token 的隐藏向量: <code>(4, 768)</code></li>
<li>所有 token 的隐藏向量: <code>(4, 257, 768)</code> (假设图像块数量是 256)</li>
</ul>
</li>
<li>
<p><strong>分类任务的最终输出 (logits):</strong></p>
<ul>
<li><code>(4, 1000)</code></li>
</ul>
</li>
</ol>
<p>ViT的输出是其核心Transformer Encoder层处理后的<strong>上下文感知的特征表示</strong>。对于图像分类，最直接的输出通常是**<code>[CLS]</code> token对应的隐藏向量**，因为它被设计用来聚合整个图像的全局信息。这个 <code>[CLS]</code> token的向量随后会经过一个线性分类头，最终输出<strong>每个类别的logits</strong>，其维度为 <code>(batch_size, num_classes)</code>。</p>
<br>
<h3 id="图像-文本">图像-文本</h3>
<h4 id="UNITER">UNITER</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.11740">https://arxiv.org/abs/1909.11740</a></p>
<img src="/2025/04/02/Paper-Reading-Record/image-20250603133711303.png" class="" title="image-20250603133711303">
<p>four main tasks to pre-train our model:</p>
<ul>
<li>Masked Language Modeling conditioned on image regions (MLM),</li>
<li>Masked Region Modeling conditioned on input text (with three variants) (MRM),</li>
<li>Image-Text Matching (ITM),</li>
<li>Word-Region Alignment (WRA).</li>
</ul>
<p>方法：</p>
<ul>
<li>对于MLM和MRM，randomly mask some words or regions from the input and learn to recover the words or regions as the output of Transformer. Specifically, word masking is realized by replacing the token with a special token [MASK], and region masking is implemented by replacing the visual feature vector with all zeros. 仅mask单模态，自监督学习</li>
<li>对于ITM，sample both positive and negative imagesentence pairs and learn their matching scores. 模型输出 true or false判定图片文本是否匹配（监督学习）并计算交叉熵优化</li>
<li>对于WAR，通过训练模型旨在最小化 $\sum_{i,j} T_{ij}.c(w_i, v_j)$, where $c(w_i, v_j)$ 为衡量 $w_i$ 和 $v_j$ 相关性的成本函数（文中为cosine distance）
<ul>
<li><strong>Optimal Transport (OT) 和传输计划 T 本身不是 UNITER 模型的参数。</strong></li>
<li>最小化的$\sum_{i,j} T_{ij}.c(w_i, v_j)$为损失函数的一部分；当该值最小化时，当前模型中的 词向量 和 图向量的对齐程度最高</li>
</ul>
</li>
</ul>
<br>
<h4 id="ViLT">ViLT</h4>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v139/kim21k/kim21k.pdf">http://proceedings.mlr.press/v139/kim21k/kim21k.pdf</a></p>
<p>参考：<a target="_blank" rel="noopener" href="https://www.gnn.club/?p=2910">https://www.gnn.club/?p=2910</a></p>
<p>对比其他架构</p>
<img src="/2025/04/02/Paper-Reading-Record/20250410155409605.png" class="" title="img">
<p>文章架构</p>
<img src="/2025/04/02/Paper-Reading-Record/20250410214653107.png" class="" title="img">
<p>We train ViLT with two objectives commonly used to train VLP models: image text matching (ITM) and masked language modeling (MLM).</p>
<br>
<h4 id="ALBEF">ALBEF</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.07651">https://arxiv.org/abs/2107.07651</a></p>
<p>参考：<a target="_blank" rel="noopener" href="https://gnn.club/?p=2989">https://gnn.club/?p=2989</a></p>
<img src="/2025/04/02/Paper-Reading-Record/20250421211315671.png" class="" title="img">
<p><strong>训练</strong></p>
<ul>
<li>Image－Text Contrastive Loss，ITC</li>
<li>Image－Text Matching Loss，ITM</li>
<li>Masked Language Modeling Loss，MLM</li>
</ul>
<br>
<h3 id="MoE">MoE</h3>
<h4 id="A-survey-on-MoE">A survey on MoE</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.06204">https://arxiv.org/pdf/2407.06204</a></p>
<img src="/2025/04/02/Paper-Reading-Record/image-20250613211803671.png" class="" title="image-20250613211803671">
<br>
<h4 id="Switch-Transformers">Switch Transformers</h4>
<p>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.03961">https://arxiv.org/abs/2101.03961</a></p>
<img src="/2025/04/02/Paper-Reading-Record/image-20250604100042926.png" class="" title="image-20250604100042926">
<br>
<h4 id="MoE-in-DS-v3">MoE in DS-v3</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/html/2412.19437v1">https://arxiv.org/html/2412.19437v1</a></p>
<img src="/2025/04/02/Paper-Reading-Record/image-20250613211948220.png" class="" title="image-20250613211948220">
<blockquote>
<p>混合 sparse &amp; dense MoE</p>
</blockquote>
<br>
<h3 id="多模态">多模态</h3>
<h4 id="CLIP">CLIP</h4>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a></p>
<p>参考：<a target="_blank" rel="noopener" href="https://gnn.club/?p=2921">https://gnn.club/?p=2921</a></p>
<p>仅预测哪段文本整体与哪张图像配对，而无需预测文本的具体词汇。</p>
<p>Given a batch of N (image, text) pairs, CLIP is trained to predict which of the N × N possible (image, text) pairings across a batch actually occurred.</p>
<p>To do this, CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the $N$ real pairs in the batch while minimizing the cosine similarity of the embeddings of the $N^2 − N$ incorrect pairings. We optimize a symmetric cross entropy loss over these similarity scores.</p>
<img src="/2025/04/02/Paper-Reading-Record/image-20250601135610999.png" class="" title="image-20250601135610999">
<p>模型结构</p>
<p>We consider two different architectures for the image encoder. For the first, we use ResNet-50 (He et al., 2016a) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make several modifications to the original version using the ResNetD improvements from He et al. (2019) and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a single layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image. For the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al., 2020). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme.</p>
<p>The text encoder is a Transformer (Vaswani et al., 2017) with the architecture modifications described in Radford et al. (2019). As a base size we use a 63M-parameter 12- layer 512-wide model with 8 attention heads. The transformer operates on a lower-cased byte pair encoding (BPE) representation of the text with a 49,152 vocab size (Sennrich et al., 2015). For computational efficiency, the max sequence length was capped at 76. The text sequence is bracketed with [SOS] and [EOS] tokens and the activations of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space. Masked self-attention was used in the text encoder to preserve the ability to initialize with a pre-trained language model or add language modeling as an auxiliary objective, though exploration of this is left as future work.</p>
<br>
<h4 id="Flamingo">Flamingo</h4>
<p>Flamingo: a Visual Language Model for Few-Shot Learning</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.14198">https://arxiv.org/abs/2204.14198</a></p>
<img src="/2025/04/02/Paper-Reading-Record/image-20250605114032032.png" class="" title="image-20250605114032032">
<img src="/2025/04/02/Paper-Reading-Record/image-20250605114141206.png" class="" title="image-20250605114141206">
<br>
<h4 id="BLIP-2">BLIP-2</h4>
<p>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.12597">https://arxiv.org/abs/2301.12597</a></p>
<br>
<h2 id="参考">参考</h2>
<br>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/03/28/Netty/" rel="prev" title="Netty">
      <i class="fa fa-chevron-left"></i> Netty
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/04/13/Python/" rel="next" title="Python">
      Python <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">Paper-Reading-Record</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Corpus"><span class="nav-text">Corpus</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Datasets"><span class="nav-text">Datasets</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Paper"><span class="nav-text">Paper</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Distribution-Base"><span class="nav-text">Distribution-Base</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CAP"><span class="nav-text">CAP</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LLM-Base"><span class="nav-text">LLM-Base</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Attention"><span class="nav-text">Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GPT"><span class="nav-text">GPT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BERT"><span class="nav-text">BERT</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E6%9C%AC"><span class="nav-text">文本</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GPT2"><span class="nav-text">GPT2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GPT3"><span class="nav-text">GPT3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#InstructGPT"><span class="nav-text">InstructGPT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LLaMa-1"><span class="nav-text">LLaMa-1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LLaMA-2"><span class="nav-text">LLaMA-2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DeepSeek-V1"><span class="nav-text">DeepSeek-V1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DeepSeek-V2"><span class="nav-text">DeepSeek-V2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DeepSeek-V3"><span class="nav-text">DeepSeek-V3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DeepSeek-R1-Zero-DeepSeek-R1"><span class="nav-text">DeepSeek-R1-Zero&amp;DeepSeek-R1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Qwen3"><span class="nav-text">Qwen3</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CV"><span class="nav-text">CV</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Stable-Diffusion"><span class="nav-text">Stable Diffusion</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ViT"><span class="nav-text">ViT</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F-%E6%96%87%E6%9C%AC"><span class="nav-text">图像-文本</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#UNITER"><span class="nav-text">UNITER</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ViLT"><span class="nav-text">ViLT</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ALBEF"><span class="nav-text">ALBEF</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MoE"><span class="nav-text">MoE</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#A-survey-on-MoE"><span class="nav-text">A survey on MoE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Switch-Transformers"><span class="nav-text">Switch Transformers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MoE-in-DS-v3"><span class="nav-text">MoE in DS-v3</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81"><span class="nav-text">多模态</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CLIP"><span class="nav-text">CLIP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Flamingo"><span class="nav-text">Flamingo</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BLIP-2"><span class="nav-text">BLIP-2</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-text">参考</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="marigo1d"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">marigo1d</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/marigo1d" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;marigo1d" rel="noopener" target="_blank">GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">marigo1d</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">523k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">15:50</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
